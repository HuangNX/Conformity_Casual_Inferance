{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e6bd12-b52a-43a1-82f9-1beacea5277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from data_process import get_dataset_splits\n",
    "from utils.evaluation_utils import load_data_from_file, write_results_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564a8c75-4e37-41df-a4af-59de00ec51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 18:43:29.144489: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 18:43:29.183479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-05 18:43:29.889165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import *\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5521c5-8c9e-4c3e-9ab3-ffd0f4b7805d",
   "metadata": {},
   "source": [
    "# Data Loading and basic setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc0cbf8-fdff-4134-a507-bb6c1aedb2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(3000, 160, 25)\n",
      "float32\n",
      "previous_treatments\n",
      "(3000, 160, 3)\n",
      "float32\n",
      "covariates\n",
      "(3000, 161, 25)\n",
      "float32\n",
      "treatments\n",
      "(3000, 161, 3)\n",
      "float32\n",
      "sequence_length\n",
      "(3000,)\n",
      "int64\n",
      "outcomes\n",
      "(3000, 161, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_from_file(\"../fullfeature_fillmean_1000.txt\")\n",
    "# 数据类型转换\n",
    "for key in dataset.keys():\n",
    "    if key!='sequence_length':\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "    print(key)\n",
    "    print(dataset[key].shape)\n",
    "    print(dataset[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec331f-47e2-4e0f-b9d4-f6d0a256e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.1, random_state=10)\n",
    "train_index, test_index = next(shuffle_split.split(dataset['covariates'][:, :, 0]))\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.11, random_state=10)\n",
    "train_index, val_index = next(shuffle_split.split(dataset['covariates'][train_index, :, 0]))\n",
    "dataset_map = get_dataset_splits(dataset, train_index, val_index, test_index, use_predicted_confounders=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9322ba-62ba-449e-b719-e041efb8bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT = 'results/rmsn_result_test_use_confounders_False'\n",
    "# rnn_fit参数设置\n",
    "networks_to_train='propensity_networks'\n",
    "# networks_to_train='encoder'\n",
    "b_use_predicted_confounders=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6261019d-9815-46ad-b0c4-49f4db244370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with memory growth\n"
     ]
    }
   ],
   "source": [
    "# Setup gpus\n",
    "# 检测 GPU 设备\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # set TensorFlow to use all GPU\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        for gpu in gpus:\n",
    "            # set GPU memery growth\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Using GPU with memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        # Changing device settings after the program is running may cause errors\n",
    "        print(e)\n",
    "else:\n",
    "    # if no GPU，using CPU\n",
    "    print(\"No GPU found, using CPU\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53019bdb-53d7-45d1-9492-d156d74cf9bb",
   "metadata": {},
   "source": [
    "# Data Process Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c1cc5d9-4f7a-4c99-8b68-a0684bb149cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data(raw_sim_data,\n",
    "                       b_predict_actions,\n",
    "                       b_use_actions_only,\n",
    "                       b_use_predicted_confounders,\n",
    "                       b_use_oracle_confounders,\n",
    "                       b_remove_x1,\n",
    "                       keep_first_point=False):\n",
    "    \"\"\"\n",
    "    Create formatted data to train both propensity networks and seq2seq architecture\n",
    "\n",
    "    :param raw_sim_data: Data from simulation\n",
    "    :param scaling_params: means/standard deviations to normalise the data to\n",
    "    :param b_predict_actions: flag to package data for propensity network to forecast actions\n",
    "    :param b_use_actions_only:  flag to package data with only action inputs and not covariates\n",
    "    :param b_predict_censoring: flag to package data to predict censoring locations\n",
    "    :return: processed data to train specific network\n",
    "    \"\"\"\n",
    "    horizon = 1\n",
    "    offset = 1\n",
    "\n",
    "    # Continuous values\n",
    "\n",
    "    # Binary application\n",
    "    treatments = raw_sim_data['treatments']\n",
    "    covariates = raw_sim_data['covariates']\n",
    "    dataset_outputs = raw_sim_data['outcomes']\n",
    "    sequence_lengths = raw_sim_data['sequence_length']\n",
    "    \n",
    "    if b_use_predicted_confounders:\n",
    "        predicted_confounders = raw_sim_data['predicted_confounders']\n",
    "\n",
    "    if b_use_oracle_confounders:\n",
    "        predicted_confounders = raw_sim_data['confounders']\n",
    "\n",
    "    num_treatments = treatments.shape[-1]\n",
    "\n",
    "    # Parcelling INPUTS\n",
    "    if b_predict_actions:\n",
    "        if b_use_actions_only:\n",
    "            inputs = treatments\n",
    "            inputs = inputs[:, :-offset, :]\n",
    "\n",
    "            actions = inputs.copy()\n",
    "\n",
    "        else:\n",
    "            # Uses current covariate, to remove confounding effects between action and current value\n",
    "            if (b_use_predicted_confounders):\n",
    "                print (\"Using predicted confounders\")\n",
    "                inputs = np.concatenate([covariates[:, 1:, ], predicted_confounders[:, 1:, ], treatments[:, :-1, ]],\n",
    "                                        axis=2)\n",
    "            else:\n",
    "                inputs = np.concatenate([covariates[:, 1:,], treatments[:, :-1, ]], axis=2)\n",
    "\n",
    "            actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    else:\n",
    "        if (b_use_predicted_confounders):\n",
    "            inputs = np.concatenate([covariates, predicted_confounders, treatments], axis=2)\n",
    "        else:\n",
    "            inputs = np.concatenate([covariates, treatments], axis=2)\n",
    "        \n",
    "        if not keep_first_point:\n",
    "            inputs = inputs[:, 1:, :]\n",
    "\n",
    "        actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    # Parcelling OUTPUTS\n",
    "    if b_predict_actions:\n",
    "        outputs = treatments\n",
    "        outputs = outputs[:, 1:, :]\n",
    "\n",
    "    else:\n",
    "        if keep_first_point:\n",
    "            outputs = dataset_outputs\n",
    "        else:\n",
    "            outputs = dataset_outputs[:, 1:, :]\n",
    "\n",
    "\n",
    "    # Set array alignment\n",
    "    sequence_lengths = np.array([i - 1 for i in sequence_lengths]) # everything shortens by 1\n",
    "\n",
    "    # Remove any trajectories that are too short\n",
    "    inputs = inputs[sequence_lengths > 0, :, :]\n",
    "    outputs = outputs[sequence_lengths > 0, :, :]\n",
    "    sequence_lengths = sequence_lengths[sequence_lengths > 0]\n",
    "    actions = actions[sequence_lengths > 0, :, :]\n",
    "\n",
    "    # Add active entires\n",
    "    active_entries = np.zeros(outputs.shape, dtype=np.float32)\n",
    "\n",
    "    for i in range(sequence_lengths.shape[0]):\n",
    "        sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "        if not b_predict_actions:\n",
    "            for k in range(horizon):\n",
    "                #include the censoring point too, but ignore future shifts that don't exist\n",
    "                active_entries[i, :sequence_length-k, k] = 1\n",
    "        else:\n",
    "            active_entries[i, :sequence_length, :] = 1\n",
    "\n",
    "    return {'outputs': outputs,  # already scaled\n",
    "            'scaled_inputs': inputs,\n",
    "            'scaled_outputs': outputs,\n",
    "            'actions': actions,\n",
    "            'sequence_lengths': sequence_lengths,\n",
    "            'active_entries': active_entries\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d167e41a-d2e8-4763-a8d6-44c1d5d39e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset(dataset_map, minibatch_size):\n",
    "    key_map = {'inputs': dataset_map['scaled_inputs'],\n",
    "               'outputs': dataset_map['scaled_outputs'],\n",
    "               'active_entries': dataset_map['active_entries'],\n",
    "               'sequence_lengths': dataset_map['sequence_lengths']}\n",
    "\n",
    "    if 'propensity_weights' in dataset_map:\n",
    "        key_map['propensity_weights'] = dataset_map['propensity_weights']\n",
    "\n",
    "    if 'initial_states' in dataset_map:\n",
    "        key_map['initial_states'] = dataset_map['initial_states']\n",
    "\n",
    "    #from_tensor_slices:切片; shuffle:随机打乱; batch:批次组合; prefetch:提前准备（预取）数据\n",
    "    # buffer_size = key_map['inputs'].shape[0]\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(key_map)\\\n",
    "                .shuffle(buffer_size=1000).batch(minibatch_size) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c3155-4258-432d-879d-539562c99e8e",
   "metadata": {},
   "source": [
    "# Mirrored Trainging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6a4516-9b67-4e37-b1a8-79ff556d3e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 18:43:37.734261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21626 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:44:00.0, compute capability: 8.9\n",
      "2024-03-05 18:43:37.735013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21894 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:81:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a3a9d-3862-4bef-b140-ade92c6b3620",
   "metadata": {},
   "source": [
    "## Model Defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a46a2648-72ed-4b16-93cb-95baff49209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \n",
    "    # Data params\n",
    "    #training_data = None if 'training_dataset' not in params else params['training_dataset']\n",
    "    #validation_data = None if 'validation_dataset' not in params else params['validation_dataset']\n",
    "    #test_data = None if 'test_dataset' not in params else params['test_dataset']\n",
    "    input_size = params['input_size']\n",
    "    output_size = params['output_size']\n",
    "\n",
    "    # Network params\n",
    "    net_name = params['net_name']\n",
    "    softmax_size = params['softmax_size']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    hidden_layer_size = params['hidden_layer_size']\n",
    "    memory_activation_type = params['hidden_activation']\n",
    "    output_activation_type = params['output_activation']\n",
    "    #initial_states = None\n",
    "    # input layer\n",
    "    inputs = layers.Input(shape=(None,input_size), dtype=tf.float32)\n",
    "    # define initial states \n",
    "    initial_h =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_h')\n",
    "    initial_c =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_c')\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm, state_h, state_c = layers.LSTM(hidden_layer_size, activation=memory_activation_type, \n",
    "                       return_sequences=True, return_state=True, dropout=dropout_rate)(inputs, initial_state=[initial_h, initial_c])\n",
    "\n",
    "    # flattened_lstm = layers.Flatten()(lstm)\n",
    "\n",
    "    # Seq2Seq(if need)\n",
    "    use_seq2seq_feedback = False\n",
    "    if use_seq2seq_feedback:\n",
    "        logits = lstm\n",
    "    else:\n",
    "        # linear output layer\n",
    "        logits = layers.Dense(output_size)(lstm)\n",
    "\n",
    "    # Softmax\n",
    "    if softmax_size != 0:\n",
    "        logits_reshaped = layers.Reshape((-1, output_size))(logits)\n",
    "        core_outputs, softmax_outputs = tf.split(logits_reshaped, [output_size - softmax_size, softmax_size], axis=-1)\n",
    "        core_activated = layers.Activation(output_activation_type)(core_outputs)\n",
    "        softmax_activated = layers.Softmax(axis=-1)(softmax_outputs)\n",
    "        outputs = layers.Concatenate(axis=-1)([core_activated, softmax_activated])\n",
    "    else:\n",
    "        outputs = layers.Activation(output_activation_type)(logits)\n",
    "\n",
    "    # construct model\n",
    "    model = models.Model(inputs=[inputs, initial_h, initial_c], outputs=[outputs, state_h, state_c], name=net_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14eb66-995c-4aa6-9667-edb75a5c1fea",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4347691e-39b6-495f-9efc-5ea279b5d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # test loss function ###################################\n",
    "    mse_loss_object = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def compute_mse_loss(labels, predictions):\n",
    "        global_batch_size = 256\n",
    "        # 计算每个样本的MSE损失\n",
    "        per_example_loss = mse_loss_object(labels, predictions)\n",
    "        # 计算所有样本的平均MSE损失，并根据全局批量大小进行调整\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=global_batch_size)\n",
    "    \n",
    "    # custom loss function ##################################\n",
    "    class CustomLoss(losses.Loss):\n",
    "        def __init__(self, performance_metric, num_gpus, global_batch_size, name=\"custom_loss\"):\n",
    "            super().__init__(name=name) #reduction=losses.Reduction.NONE\n",
    "            self.performance_metric = performance_metric\n",
    "            self.num_gpus = num_gpus\n",
    "            self.global_batch_size = global_batch_size\n",
    "            # self.weights = params['weights']\n",
    "            # self.active_entries = params['active_entries']\n",
    "\n",
    "        def train_call(self, y_true, y_pred, active_entries, weights):\n",
    "            if self.performance_metric == \"mse\":\n",
    "                loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                       / tf.reduce_sum(active_entries)\n",
    "                # per_example_loss = (tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                #                     / tf.reduce_sum(active_entries)\n",
    "            elif self.performance_metric == \"xentropy\":\n",
    "                loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                       (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                       * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "                # per_example_loss = ((y_true * -tf.math.log(y_pred + 1e-8) + \\\n",
    "                #                    (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8)) * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "\n",
    "            # 将总和除以gpu数，获得全局平均损失\n",
    "            return loss * (1./self.num_gpus)\n",
    "            # return tf.nn.compute_average_loss(per_example_loss, global_batch_size=self.global_batch_size)\n",
    "\n",
    "        def valid_call(self, y_true, y_pred):\n",
    "            if self.performance_metric == \"mse\":\n",
    "               #loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries ) \\\n",
    "               #        / tf.reduce_sum(active_entries)\n",
    "                loss = tf.square(y_true - y_pred)\n",
    "\n",
    "            elif self.performance_metric == \"xentropy\":\n",
    "                loss = (y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                       (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\"performance_metric\": self.performance_metric, \"global_batch_size\": self.global_batch_size})\n",
    "            return config\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe8d73-6887-400f-a882-32d7ef7b55f4",
   "metadata": {},
   "source": [
    "## core routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90518db9-7062-4101-89b9-fbff99462036",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 18:43:54.164477: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2403\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def core_routine(params):\n",
    "    \n",
    "# 设置输入流水线\n",
    "training_processed = params['training_dataset']\n",
    "global_batch_size = params['minibatch_size'] * strategy.num_replicas_in_sync\n",
    "tf_data_train = convert_to_tf_dataset(training_processed, global_batch_size)\n",
    "\n",
    "# distribute them\n",
    "dist_tf_data_train = strategy.experimental_distribute_dataset(tf_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c43acf4-683d-46d8-bed2-b4a68bffe289",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 3)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (50, 160, 3)\n",
      "outputs: (50, 160, 3)\n",
      "active_entries: (50, 160, 3)\n",
      "sequence_lengths: (50,)\n",
      "inputs: (49, 160, 3)\n",
      "outputs: (49, 160, 3)\n",
      "active_entries: (49, 160, 3)\n",
      "sequence_lengths: (49,)\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(batch):\n",
    "    # 这里的内容根据你的数据集结构调整\n",
    "    # 例如，打印出批次的形状或一些关键数据\n",
    "    for key, value in batch.items():\n",
    "        # 打印出每个键对应的值的形状\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "\n",
    "for dist_batch in dist_tf_data_train:\n",
    "    strategy.run(inspect_dataset, args=(dist_batch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaf9afc1-b74d-4535-b576-c1d1071b4098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"treatment_rnn_action_inputs_only\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 3)]            0         []                            \n",
      "                                                                                                  \n",
      " initial_h (InputLayer)      [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " initial_c (InputLayer)      [(None, 16)]                 0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, None, 16),           1280      ['input_1[0][0]',             \n",
      "                              (None, 16),                            'initial_h[0][0]',           \n",
      "                              (None, 16)]                            'initial_c[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 3)              51        ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None, 3)              0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1331 (5.20 KB)\n",
      "Trainable params: 1331 (5.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义损失函数\n",
    "# loss_func = CustomLoss(params['performance_metric'], strategy.num_replicas_in_sync, global_batch_size)\n",
    "\n",
    "# 定义衡量指标\n",
    "with strategy.scope():\n",
    "    train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "    valid_loss = metrics.Mean(name='valid_loss')\n",
    "    valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "\n",
    "# 构架模型和优化器\n",
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = create_model(params)\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5227b29-0069-4d2f-8f00-b14daecbf795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数 ###################################################################################\n",
    "def train_step(data): #, chunk_sizes\n",
    "    inputs = data['inputs']\n",
    "    outputs = data['outputs']\n",
    "    active_entries = data['active_entries']\n",
    "    weights = data['propensity_weights'] if 'propensity_weights' in data else tf.constant(1.0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        initial_state = tf.zeros([batch_size, hidden_layer_size], dtype=tf.float32)\n",
    "        predictions,_,_ = model([inputs,initial_state, initial_state], training=True)\n",
    "        # Compute loss\n",
    "        # loss = loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "        loss = compute_mse_loss(outputs, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clip_norm = max_norm)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    #self.train_loss.update_state(loss)\n",
    "    train_metric.update_state(outputs, predictions)\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data): #, chunk_sizes\n",
    "    per_replica_losses = strategy.run(train_step, args=(data,)) #, chunk_sizes\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "# ###########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9056b4-8807-4131-8aaf-ef9d237e072e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-03-05 18:44:20.942445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-03-05 18:44:20.959377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-05 18:44:20.966005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_size = params['hidden_layer_size']\n",
    "max_norm = params['max_norm']\n",
    "for epoch in range(params['num_epochs']):\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in tf_data_train:\n",
    "        total_loss += distributed_train_step(x)\n",
    "        num_batches += 1\n",
    "        train_loss = total_loss / num_batches\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "    print(template.format(epoch + 1, train_loss,\n",
    "                         train_metric.result() * 100, valid_loss.result(),\n",
    "                         valid_metric.result() * 100))\n",
    "\n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c6d2c-1cbc-42d4-875a-ba78ee28faf0",
   "metadata": {},
   "source": [
    "## rnn fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8036011f-0132-41ea-a36b-172856fc8cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Training propensity networks\n",
      "INFO:Running hyperparameter optimisation\n",
      "INFO:Using GPU with memory growth\n",
      "INFO:Using specifications for treatment_rnn_action_inputs_only: (0.1, 4, 100, 64, 0.005, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment_rnn_action_inputs_only\n"
     ]
    }
   ],
   "source": [
    "specifications = {\n",
    "     'rnn_propensity_weighted': (0.1, 4, 100, 64, 0.005, 1.0),\n",
    "     'treatment_rnn_action_inputs_only': (0.1, 4, 100, 64, 0.005, 1.0),\n",
    "     'treatment_rnn': (0.1, 4, 100, 64, 0.005, 1.0),\n",
    "} # decrease learning rate from 0.01 to 0.005 \n",
    "####################################################################################################################\n",
    "\n",
    "\n",
    "# def rnn_fit(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders,\n",
    "#             b_use_oracle_confounders=False, b_remove_x1=False):\n",
    "\n",
    "b_use_oracle_confounders=False; b_remove_x1=False\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "# Get the correct networks to train\n",
    "if networks_to_train == \"propensity_networks\":\n",
    "    logging.info(\"Training propensity networks\")\n",
    "    net_names = ['treatment_rnn_action_inputs_only']\n",
    "    # net_names = ['treatment_rnn']\n",
    "\n",
    "elif networks_to_train == \"encoder\":\n",
    "    logging.info(\"Training R-MSN encoder\")\n",
    "    net_names = [\"rnn_propensity_weighted\"]\n",
    "\n",
    "elif networks_to_train == \"user_defined\":\n",
    "    logging.info(\"Training user defined network\")\n",
    "    raise NotImplementedError(\"Specify network to use!\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognised network type\")\n",
    "\n",
    "logging.info(\"Running hyperparameter optimisation\")\n",
    "\n",
    "# Experiment name\n",
    "expt_name = \"treatment_effects\"\n",
    "\n",
    "# Possible networks to use along with their activation functions\n",
    "# change hidden layer of rnn_propensity_weighted to tanh\n",
    "activation_map = {'rnn_propensity_weighted': (\"tanh\", 'linear'),\n",
    "                  'rnn_propensity_weighted_logistic': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid')\n",
    "                  }\n",
    "\n",
    "# Setup tensorflow\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # set TensorFlow to use all GPU\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        for gpu in gpus:\n",
    "            # set GPU memery growth\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logging.info(\"Using GPU with memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        # Changing device settings after the program is running may cause errors\n",
    "        logging.info(e)\n",
    "else:\n",
    "    # if no GPU，using CPU\n",
    "    logging.info(\"No GPU found, using CPU\")\n",
    "\n",
    "## Create a distribution strategy\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#print('Number of devices: %d' % strategy.num_replicas_in_sync)\n",
    "\n",
    "training_data = dataset_map['training_data']\n",
    "validation_data = dataset_map['validation_data']\n",
    "test_data = dataset_map['test_data']\n",
    "\n",
    "# Start Running hyperparam opt\n",
    "#opt_params = {}\n",
    "mse_dict = {}\n",
    "for net_name in net_names:\n",
    "    print(net_name)\n",
    "    # Re-run hyperparameter optimisation if parameters are not specified, otherwise train with defined params\n",
    "    max_hyperparam_runs = 3 if net_name not in specifications else 1\n",
    "\n",
    "    # Pull datasets\n",
    "    b_predict_actions = \"treatment_rnn\" in net_name\n",
    "    use_truncated_bptt = net_name != \"rnn_model_bptt\" # whether to train with truncated backpropagation through time\n",
    "    b_propensity_weight = \"rnn_propensity_weighted\" in net_name\n",
    "    b_use_actions_only = \"rnn_action_inputs_only\" in net_name\n",
    "\n",
    "\n",
    "   # Extract only relevant trajs and shift data\n",
    "    training_processed = get_processed_data(training_data, b_predict_actions,\n",
    "                                                 b_use_actions_only, b_use_predicted_confounders,\n",
    "                                                 b_use_oracle_confounders, b_remove_x1)\n",
    "    validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                                   b_use_actions_only, b_use_predicted_confounders,\n",
    "                                                   b_use_oracle_confounders, b_remove_x1)\n",
    "    test_processed = get_processed_data(test_data, b_predict_actions,\n",
    "                                             b_use_actions_only, b_use_predicted_confounders,\n",
    "                                             b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "\n",
    "    num_features = training_processed['scaled_inputs'].shape[-1]\n",
    "    # num_features = 28\n",
    "    num_outputs = training_processed['scaled_outputs'].shape[-1]\n",
    "\n",
    "    # Load propensity weights if they exist\n",
    "    if b_propensity_weight:\n",
    "\n",
    "        if net_name == 'rnn_propensity_weighted_den_only':\n",
    "            # use un-stabilised IPTWs generated by propensity networks\n",
    "            propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_den_only.npy\"))\n",
    "        elif net_name == \"rnn_propensity_weighted_logistic\":\n",
    "            # Use logistic regression weights\n",
    "            propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "            tmp = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_logistic.npy\"))\n",
    "            propensity_weights = tmp[:propensity_weights.shape[0], :, :]\n",
    "        else:\n",
    "            # use stabilised IPTWs generated by propensity networks\n",
    "            propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "\n",
    "        logging.info(\"Net name = {}. Mean-adjusting!\".format(net_name))\n",
    "\n",
    "        propensity_weights /= propensity_weights.mean()\n",
    "\n",
    "        training_processed['propensity_weights'] = np.array(propensity_weights, dtype='float32')\n",
    "\n",
    "    # Start hyperparamter optimisation (training model directly)\n",
    "    hyperparam_count = 0\n",
    "    # while True:\n",
    "\n",
    "    if net_name not in specifications:\n",
    "\n",
    "        dropout_rate = np.random.choice([0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "        memory_multiplier = np.random.choice([0.5, 1, 2, 3, 4])\n",
    "        num_epochs = 100\n",
    "        minibatch_size = np.random.choice([64, 128, 256])\n",
    "        learning_rate = np.random.choice([0.01, 0.005, 0.001])  #([0.01, 0.001, 0.0001])\n",
    "        max_norm = np.random.choice([0.5, 1.0, 2.0, 4.0])\n",
    "        hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "    else:\n",
    "        spec = specifications[net_name]\n",
    "        logging.info(\"Using specifications for {}: {}\".format(net_name, spec))\n",
    "        dropout_rate = spec[0]\n",
    "        memory_multiplier = spec[1]\n",
    "        num_epochs = spec[2]\n",
    "        minibatch_size = spec[3]\n",
    "        learning_rate = spec[4]\n",
    "        max_norm = spec[5]\n",
    "        hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "    model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "\n",
    "    # transform data to tf format\n",
    "\n",
    "\n",
    "    # construct model parameters\n",
    "    # hidden_layer_size = int(memory_multiplier * num_features)\n",
    "    hidden_layer_size = int(memory_multiplier * 25)\n",
    "    params = {'net_name': net_name,\n",
    "            'experiment_name': expt_name,\n",
    "            'training_dataset': training_processed,\n",
    "            'validation_dataset': validation_processed,\n",
    "            'test_dataset':  test_processed,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'input_size': num_features,\n",
    "            'output_size': num_outputs,\n",
    "            'hidden_layer_size': hidden_layer_size,\n",
    "            'num_epochs': num_epochs,\n",
    "            'minibatch_size': minibatch_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'max_norm': max_norm,\n",
    "            'model_folder': model_folder,\n",
    "            'hidden_activation': hidden_activation,\n",
    "            'output_activation': output_activation,\n",
    "            'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "            'softmax_size': 0, #not used in this paper, but allows for categorical actions\n",
    "            'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse'}\n",
    "\n",
    "    # core_routine(model_parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84114de5-610b-403d-b61a-ac7f3da98f8f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Training propensity networks\n",
      "INFO:Running hyperparameter optimisation\n",
      "INFO:Using GPU with memory growth\n",
      "INFO:Using specifications for treatment_rnn: (0.1, 4, 100, 64, 0.005, 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treatment_rnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 14:36:09.344243: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2403\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\024TensorSliceDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-03-05 14:36:11.819374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-03-05 14:36:11.844503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-03-05 14:36:11.845333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-05 14:36:12.290942: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fcb87e4280 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-05 14:36:12.290991: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-03-05 14:36:12.291001: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-03-05 14:36:12.296597: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-05 14:36:12.474575: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 26.423446655273438, Accuracy: 33.42791748046875, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 20.1297607421875, Accuracy: 25.465864181518555, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 3, Loss: 19.281408309936523, Accuracy: 24.392627716064453, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 4, Loss: 19.014102935791016, Accuracy: 24.05445671081543, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 5, Loss: 18.92331314086914, Accuracy: 23.93960189819336, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 6, Loss: 18.84199333190918, Accuracy: 23.83672523498535, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 7, Loss: 18.776779174804688, Accuracy: 23.754222869873047, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 8, Loss: 18.7154541015625, Accuracy: 23.676645278930664, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 9, Loss: 18.688953399658203, Accuracy: 23.643117904663086, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 10, Loss: 18.630285263061523, Accuracy: 23.568899154663086, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 11, Loss: 18.58951759338379, Accuracy: 23.5173282623291, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 12, Loss: 18.589981079101562, Accuracy: 23.517911911010742, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 13, Loss: 18.566993713378906, Accuracy: 23.48883056640625, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 14, Loss: 18.525121688842773, Accuracy: 23.435861587524414, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 15, Loss: 18.508533477783203, Accuracy: 23.414873123168945, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 16, Loss: 18.464481353759766, Accuracy: 23.35914421081543, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 17, Loss: 18.448261260986328, Accuracy: 23.338624954223633, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 18, Loss: 18.419231414794922, Accuracy: 23.301895141601562, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 19, Loss: 18.403728485107422, Accuracy: 23.282278060913086, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 20, Loss: 18.37464141845703, Accuracy: 23.245487213134766, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 21, Loss: 18.372140884399414, Accuracy: 23.242326736450195, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 22, Loss: 18.337600708007812, Accuracy: 23.19862937927246, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 23, Loss: 18.335927963256836, Accuracy: 23.196510314941406, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 24, Loss: 18.312957763671875, Accuracy: 23.16745376586914, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 25, Loss: 18.28569793701172, Accuracy: 23.132965087890625, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 26, Loss: 18.302839279174805, Accuracy: 23.154645919799805, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 27, Loss: 18.26531219482422, Accuracy: 23.107175827026367, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 28, Loss: 18.25404930114746, Accuracy: 23.092931747436523, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 29, Loss: 18.231430053710938, Accuracy: 23.064313888549805, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 30, Loss: 18.221054077148438, Accuracy: 23.051189422607422, Test Loss: 0.0, Test Accuracy: 0.0\n",
      "Epoch 31, Loss: 18.198413848876953, Accuracy: 23.02254867553711, Test Loss: 0.0, Test Accuracy: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrnn_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetworks_to_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODEL_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_use_predicted_confounders\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [16], line 175\u001b[0m, in \u001b[0;36mrnn_fit\u001b[0;34m(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\u001b[0m\n\u001b[1;32m    154\u001b[0m hidden_layer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(memory_multiplier \u001b[38;5;241m*\u001b[39m num_features)\n\u001b[1;32m    155\u001b[0m model_parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet_name\u001b[39m\u001b[38;5;124m'\u001b[39m: net_name,\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: expt_name,\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m: training_processed,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;66;03m#not used in this paper, but allows for categorical actions\u001b[39;00m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperformance_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxentropy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_activation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m--> 175\u001b[0m \u001b[43mcore_routine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [12], line 66\u001b[0m, in \u001b[0;36mcore_routine\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     64\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf_data_train:\n\u001b[0;32m---> 66\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdistributed_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m num_batches\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_fit(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0048e140-f37e-47a7-862d-d0f761502014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_multiplier * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e7c1b-f726-43d7-a895-c9eef62499c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_causal",
   "language": "python",
   "name": "new_causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
