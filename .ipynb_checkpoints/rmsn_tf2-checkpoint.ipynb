{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25376294-cae6-4b0e-be47-b7fc29cacc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from data_process import get_dataset_splits\n",
    "from utils.evaluation_utils import load_data_from_file, write_results_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c6bdaf-5126-4b02-b964-fbdac392c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 22:16:43.580147: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-17 22:16:43.647984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-17 22:16:44.705566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dfb6dd-b0a5-49d2-994b-a5a1e0880065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1508f-ddeb-47b3-8af9-c08f36e8e6d6",
   "metadata": {},
   "source": [
    "将所有代码转化为单线程，而非调用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afe277-4d5c-4645-a1ca-502132501860",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f597cc7-5a43-41a9-80d6-a7c00c5943a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(24000, 160, 25)\n",
      "float32\n",
      "previous_treatments\n",
      "(24000, 160, 3)\n",
      "float32\n",
      "covariates\n",
      "(24000, 161, 25)\n",
      "float32\n",
      "treatments\n",
      "(24000, 161, 3)\n",
      "float32\n",
      "sequence_length\n",
      "(24000,)\n",
      "int64\n",
      "outcomes\n",
      "(24000, 161, 1)\n",
      "float32\n",
      "predicted_confounders\n",
      "(24000, 161, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_from_file(\"../new_sample_8000_v2_dataset_with_substitute_confounders.txt\")\n",
    "# 数据类型转换\n",
    "for key in dataset.keys():\n",
    "    if key!='sequence_length':\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "    print(key)\n",
    "    print(dataset[key].shape)\n",
    "    print(dataset[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65195d8c-aea7-4c80-b79b-b2449734b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.1, random_state=10)\n",
    "train_index, test_index = next(shuffle_split.split(dataset['covariates'][:, :, 0]))\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.11, random_state=10)\n",
    "train_index, val_index = next(shuffle_split.split(dataset['covariates'][train_index, :, 0]))\n",
    "dataset_map = get_dataset_splits(dataset, train_index, val_index, test_index, use_predicted_confounders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b510c-4248-4d62-9b04-2a415e3cc6bb",
   "metadata": {},
   "source": [
    "# RMSN port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f203b4-7487-4219-b300-ddb1da2642fd",
   "metadata": {},
   "source": [
    "train_rmsn → rnn_fit → train (RNNModel-->training:get_training_graph-->validation:get_prediction_graph) → model_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8d133-a8e8-469b-8860-b399d5c4eb0e",
   "metadata": {},
   "source": [
    "## train_rmsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb08e94e-ff18-4c87-83c5-4895c9b80125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_rmsn(dataset_map, model_name, b_use_predicted_confounders):###########################\n",
    "# model_name = 'tf2_try'\n",
    "# MODEL_ROOT = os.path.join('results', model_name)\n",
    "MODEL_ROOT = 'results/rmsn_result_test_use_confounders_True'\n",
    "\n",
    "# if not os.path.exists(MODEL_ROOT):\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "# else:\n",
    "#     # Need to delete previously saved model.\n",
    "#     shutil.rmtree(MODEL_ROOT)\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "\n",
    "# rnn_fit参数设置\n",
    "# networks_to_train='propensity_networks'\n",
    "networks_to_train='encoder'\n",
    "b_use_predicted_confounders=True\n",
    "\n",
    "    # rnn_fit(dataset_map=dataset_map, networks_to_train='propensity_networks', MODEL_ROOT=MODEL_ROOT,\n",
    "    #         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     propensity_generation(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                           b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rnn_fit(networks_to_train='encoder', dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#             b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmsn_mse = rnn_test(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmse = np.sqrt(np.mean(rmsn_mse)) * 100\n",
    "    # return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9a319-a931-4bb4-8e50-abf614034137",
   "metadata": {},
   "source": [
    "## rnn_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529f430-9c63-40a1-9216-09d7e2e40b44",
   "metadata": {},
   "source": [
    "### 1.1 基础参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47dc0bb-17c8-45e5-a95e-0427a826e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "specifications = {\n",
    "     'rnn_propensity_weighted': (0.1, 4, 100, 64, 0.01, 1.0),\n",
    "     'treatment_rnn_action_inputs_only': (0.1, 3, 100, 128, 0.01, 2.0),\n",
    "     'treatment_rnn': (0.1, 4, 100, 64, 0.01, 1.0),}\n",
    "# #####################################################################################\n",
    "# def rnn_fit(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders,\n",
    "#             b_use_oracle_confounders=False, b_remove_x1=False):\n",
    "    \n",
    "# Get the correct networks to train\n",
    "if networks_to_train == \"propensity_networks\":\n",
    "    logging.info(\"Training propensity networks\")\n",
    "    # net_names = ['treatment_rnn_action_inputs_only', 'treatment_rnn']\n",
    "    net_names = ['treatment_rnn']\n",
    "\n",
    "elif networks_to_train == \"encoder\":\n",
    "    logging.info(\"Training R-MSN encoder\")\n",
    "    net_names = [\"rnn_propensity_weighted\"]\n",
    "\n",
    "elif networks_to_train == \"user_defined\":\n",
    "    logging.info(\"Training user defined network\")\n",
    "    raise NotImplementedError(\"Specify network to use!\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognised network type\")\n",
    "\n",
    "    logging.info(\"Running hyperparameter optimisation\")\n",
    "\n",
    "# Experiment name\n",
    "expt_name = \"treatment_effects\"\n",
    "\n",
    "# Possible networks to use along with their activation functions\n",
    "# change hidden layer of rnn_propensity_weighted to tanh\n",
    "activation_map = {'rnn_propensity_weighted': (\"tanh\", 'linear'),\n",
    "                  'rnn_propensity_weighted_logistic': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid')\n",
    "                  }\n",
    "\n",
    "    \n",
    "# Start Running hyperparam opt\n",
    "opt_params = {}\n",
    "# for net_name in net_names:\n",
    "net_name = net_names[0]\n",
    "# Re-run hyperparameter optimisation if parameters are not specified, otherwise train with defined params（如果需要超参优化则跑3次，否则跑一次就行）\n",
    "max_hyperparam_runs = 3 if net_name not in specifications else 1\n",
    "\n",
    "# Pull datasets\n",
    "b_predict_actions = \"treatment_rnn\" in net_name\n",
    "use_truncated_bptt = net_name != \"rnn_model_bptt\" # whether to train with truncated backpropagation through time\n",
    "b_propensity_weight = \"rnn_propensity_weighted\" in net_name\n",
    "b_use_actions_only = \"rnn_action_inputs_only\" in net_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ca44-6b19-4bc0-9691-44d81b5d99b7",
   "metadata": {},
   "source": [
    "### 1.2 gpu设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d30138a-0d8a-4aab-b965-746f99e26960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with memory growth\n"
     ]
    }
   ],
   "source": [
    "# Setup tensorflow\n",
    "# 检测 GPU 设备\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # 配置 TensorFlow 使用第一个 GPU\n",
    "        gpu0 = gpus[0]\n",
    "        tf.config.set_visible_devices([gpu0], 'GPU')\n",
    "        # 设置 GPU 内存增长\n",
    "        tf.config.experimental.set_memory_growth(gpu0, True)\n",
    "        print(\"Using GPU with memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        # 在程序运行后修改设备设置可能会引发错误\n",
    "        print(e)\n",
    "else:\n",
    "    # 如果没有检测到 GPU，使用 CPU\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992e187-499a-4b46-9953-f70361e681d9",
   "metadata": {},
   "source": [
    "### 1.3 模型参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a9e2cbb-0c0a-409f-a3c6-fdda857fd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start hyperparamter optimisation\n",
    "hyperparam_count = 0\n",
    "# 删掉超参数优化（随机选取超参数）部分\n",
    "spec = specifications[net_name]\n",
    "logging.info(\"Using specifications for {}: {}\".format(net_name, spec))\n",
    "dropout_rate = spec[0]\n",
    "memory_multiplier = spec[1]\n",
    "num_epochs = spec[2]\n",
    "minibatch_size = spec[3]\n",
    "learning_rate = spec[4]\n",
    "max_norm = spec[5]\n",
    "hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "            \n",
    "# hyperparam_opt = train(net_name, expt_name,\n",
    "#                       training_processed, validation_processed, test_processed,\n",
    "#                       dropout_rate, memory_multiplier, num_epochs,\n",
    "#                       minibatch_size, learning_rate, max_norm,\n",
    "#                       use_truncated_bptt,\n",
    "#                       num_features, num_outputs, model_folder,\n",
    "#                       hidden_activation, output_activation,\n",
    "#                       config,\n",
    "#                       \"hyperparam opt: {} of {}\".format(hyperparam_count,\n",
    "#                                                         max_hyperparam_runs))\n",
    "\n",
    "#     hyperparam_count = len(hyperparam_opt.columns)\n",
    "#     if hyperparam_count >= max_hyperparam_runs:\n",
    "#         opt_params[net_name] = hyperparam_opt.T\n",
    "#         break\n",
    "\n",
    "# logging.info(\"Done\")\n",
    "# logging.info(hyperparam_opt.T)\n",
    "\n",
    "# # Flag optimal params\n",
    "# logging.info(opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ec6e-32f9-4274-9932-c2b37e0d1e71",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd38d16-3735-489d-ae89-261de81a91a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_processed_data(raw_sim_data,\n",
    "                       b_predict_actions,\n",
    "                       b_use_actions_only,\n",
    "                       b_use_predicted_confounders,\n",
    "                       b_use_oracle_confounders,\n",
    "                       b_remove_x1,\n",
    "                       keep_first_point=False):\n",
    "    \"\"\"\n",
    "    Create formatted data to train both propensity networks and seq2seq architecture\n",
    "\n",
    "    :param raw_sim_data: Data from simulation\n",
    "    :param scaling_params: means/standard deviations to normalise the data to\n",
    "    :param b_predict_actions: flag to package data for propensity network to forecast actions\n",
    "    :param b_use_actions_only:  flag to package data with only action inputs and not covariates\n",
    "    :param b_predict_censoring: flag to package data to predict censoring locations\n",
    "    :return: processed data to train specific network\n",
    "    \"\"\"\n",
    "    horizon = 1\n",
    "    offset = 1\n",
    "\n",
    "    # Continuous values\n",
    "\n",
    "    # Binary application\n",
    "    treatments = raw_sim_data['treatments']\n",
    "    covariates = raw_sim_data['covariates']\n",
    "    dataset_outputs = raw_sim_data['outcomes']\n",
    "    sequence_lengths = raw_sim_data['sequence_length']\n",
    "    \n",
    "    if b_use_predicted_confounders:\n",
    "        predicted_confounders = raw_sim_data['predicted_confounders']\n",
    "\n",
    "    if b_use_oracle_confounders:\n",
    "        predicted_confounders = raw_sim_data['confounders']\n",
    "\n",
    "    num_treatments = treatments.shape[-1]\n",
    "\n",
    "    # Parcelling INPUTS\n",
    "    if b_predict_actions:\n",
    "        if b_use_actions_only:\n",
    "            inputs = treatments\n",
    "            inputs = inputs[:, :-offset, :]\n",
    "\n",
    "            actions = inputs.copy()\n",
    "\n",
    "        else:\n",
    "            # Uses current covariate, to remove confounding effects between action and current value\n",
    "            if (b_use_predicted_confounders):\n",
    "                print (\"Using predicted confounders\")\n",
    "                inputs = np.concatenate([covariates[:, 1:, ], predicted_confounders[:, 1:, ], treatments[:, :-1, ]],\n",
    "                                        axis=2)\n",
    "            else:\n",
    "                inputs = np.concatenate([covariates[:, 1:,], treatments[:, :-1, ]], axis=2)\n",
    "\n",
    "            actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    else:\n",
    "        if (b_use_predicted_confounders):\n",
    "            inputs = np.concatenate([covariates, predicted_confounders, treatments], axis=2)\n",
    "        else:\n",
    "            inputs = np.concatenate([covariates, treatments], axis=2)\n",
    "        \n",
    "        if not keep_first_point:\n",
    "            inputs = inputs[:, 1:, :]\n",
    "\n",
    "        actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    # Parcelling OUTPUTS\n",
    "    if b_predict_actions:\n",
    "        outputs = treatments\n",
    "        outputs = outputs[:, 1:, :]\n",
    "\n",
    "    else:\n",
    "        if keep_first_point:\n",
    "            outputs = dataset_outputs\n",
    "        else:\n",
    "            outputs = dataset_outputs[:, 1:, :]\n",
    "\n",
    "\n",
    "    # Set array alignment\n",
    "    sequence_lengths = np.array([i - 1 for i in sequence_lengths]) # everything shortens by 1\n",
    "\n",
    "    # Remove any trajectories that are too short\n",
    "    inputs = inputs[sequence_lengths > 0, :, :]\n",
    "    outputs = outputs[sequence_lengths > 0, :, :]\n",
    "    sequence_lengths = sequence_lengths[sequence_lengths > 0]\n",
    "    actions = actions[sequence_lengths > 0, :, :]\n",
    "\n",
    "    # Add active entires\n",
    "    active_entries = np.zeros(outputs.shape, dtype=np.float32)\n",
    "\n",
    "    for i in range(sequence_lengths.shape[0]):\n",
    "        sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "        if not b_predict_actions:\n",
    "            for k in range(horizon):\n",
    "                #include the censoring point too, but ignore future shifts that don't exist\n",
    "                active_entries[i, :sequence_length-k, k] = 1\n",
    "        else:\n",
    "            active_entries[i, :sequence_length, :] = 1\n",
    "\n",
    "    return {'outputs': outputs,  # already scaled\n",
    "            'scaled_inputs': inputs,\n",
    "            'scaled_outputs': outputs,\n",
    "            'actions': actions,\n",
    "            'sequence_lengths': sequence_lengths,\n",
    "            'active_entries': active_entries\n",
    "            }\n",
    "\n",
    "\n",
    "def convert_to_tf_dataset(dataset_map, minibatch_size):\n",
    "    key_map = {'inputs': dataset_map['scaled_inputs'],\n",
    "               'outputs': dataset_map['scaled_outputs'],\n",
    "               'active_entries': dataset_map['active_entries'],\n",
    "               'sequence_lengths': dataset_map['sequence_lengths']}\n",
    "\n",
    "    if 'propensity_weights' in dataset_map:\n",
    "        key_map['propensity_weights'] = dataset_map['propensity_weights']\n",
    "\n",
    "    if 'initial_states' in dataset_map:\n",
    "        key_map['initial_states'] = dataset_map['initial_states']\n",
    "\n",
    "    #from_tensor_slices:切片; shuffle:随机打乱; batch:批次组合; prefetch:提前准备（预取）数据\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(key_map)\\\n",
    "                .shuffle(buffer_size = 1000).batch(minibatch_size) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8354ab9-1da0-44bd-8f3d-e5781b698bb3",
   "metadata": {},
   "source": [
    "### 1.4 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e895cc5-0d22-4d39-bb94-da72f4631cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset_map['training_data']\n",
    "validation_data = dataset_map['validation_data']\n",
    "test_data = dataset_map['test_data']\n",
    "\n",
    " # Extract only relevant trajs and shift data\n",
    "b_use_oracle_confounders = False; b_remove_x1 = False\n",
    "training_processed = get_processed_data(training_data, b_predict_actions,\n",
    "                                             b_use_actions_only, b_use_predicted_confounders,\n",
    "                                             b_use_oracle_confounders, b_remove_x1)\n",
    "validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                               b_use_actions_only, b_use_predicted_confounders,\n",
    "                                               b_use_oracle_confounders, b_remove_x1)\n",
    "test_processed = get_processed_data(test_data, b_predict_actions,\n",
    "                                         b_use_actions_only, b_use_predicted_confounders,\n",
    "                                         b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "num_features = training_processed['scaled_inputs'].shape[-1]\n",
    "num_outputs = training_processed['scaled_outputs'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e986ae9d-9e7c-4aaf-8a21-36e28d733ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2876/4000134917.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  propensity_weights /= propensity_weights.mean()\n"
     ]
    }
   ],
   "source": [
    "# Load propensity weights if they exist\n",
    "if b_propensity_weight:\n",
    "    if net_name == 'rnn_propensity_weighted_den_only':\n",
    "        # use un-stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_den_only.npy\"))\n",
    "    elif net_name == \"rnn_propensity_weighted_logistic\":\n",
    "        # Use logistic regression weights\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "        tmp = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_logistic.npy\"))\n",
    "        propensity_weights = tmp[:propensity_weights.shape[0], :, :]\n",
    "    else:\n",
    "        # use stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "\n",
    "    logging.info(\"Net name = {}. Mean-adjusting!\".format(net_name))\n",
    "\n",
    "    propensity_weights /= propensity_weights.mean()\n",
    "\n",
    "    training_processed['propensity_weights'] = np.array(propensity_weights, dtype='float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05465609-d94f-460a-937e-aa3df342380b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19224, 160, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "propensity_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92766f4d-e63d-428b-9317-d0d941be50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    2,     2,     2, ..., 19216, 19216, 19216]),\n",
       " array([ 37,  38,  39, ..., 122, 123, 127]),\n",
       " array([0, 0, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(propensity_weights==np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78464a8b-0263-440a-88f3-3c61d0e9c2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffbddf8b-03ef-4bff-b872-e771df271067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensorflow format\n",
    "tf_data_train = convert_to_tf_dataset(training_processed, minibatch_size)\n",
    "tf_data_valid = convert_to_tf_dataset(validation_processed, minibatch_size)\n",
    "tf_data_test = convert_to_tf_dataset(test_processed, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5226159-cc7b-49e6-979a-cf5edc0cbc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'inputs': TensorSpec(shape=(None, 160, 28), dtype=tf.float32, name=None), 'outputs': TensorSpec(shape=(None, 160, 1), dtype=tf.float32, name=None), 'active_entries': TensorSpec(shape=(None, 160, 1), dtype=tf.float32, name=None), 'sequence_lengths': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'propensity_weights': TensorSpec(shape=(None, 160, 1), dtype=tf.float32, name=None)}>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc09d-c919-4a2b-b5ff-96fd4fc8a4fd",
   "metadata": {},
   "source": [
    "## core routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c7346-384d-4d49-982f-f378b06198da",
   "metadata": {},
   "source": [
    "### def_train修改\n",
    "1. 去除session会话\n",
    "2. 各步骤拆分与重构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af5357-82fc-4ff8-ad1c-c754c2b57efd",
   "metadata": {},
   "source": [
    "#### 2.1 参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ec4a1-be8c-484a-aa71-7a8ba11660c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(net_name,\n",
    "#           expt_name,\n",
    "#           training_dataset, validation_dataset, test_dataset,\n",
    "#           dropout_rate,\n",
    "#           memory_multiplier,\n",
    "#           num_epochs,\n",
    "#           minibatch_size,\n",
    "#           learning_rate,\n",
    "#           max_norm,\n",
    "#           use_truncated_bptt,\n",
    "#           num_features,\n",
    "#           num_outputs,\n",
    "#           model_folder,\n",
    "#           hidden_activation,\n",
    "#           output_activation,\n",
    "#           tf_config,\n",
    "#           additonal_info=\"\",\n",
    "#           b_use_state_initialisation=False,\n",
    "#           b_use_seq2seq_feedback=False,\n",
    "#           b_use_seq2seq_training_mode=False,\n",
    "#           adapter_multiplier=0,\n",
    "#           b_use_memory_adapter=False,\n",
    "#           verbose=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a0ab33a-749c-45e7-af22-ffaeac0425de",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs = 1\n",
    "hidden_layer_size = int(memory_multiplier * num_features)\n",
    "\n",
    "b_use_state_initialisation = False\n",
    "if b_use_state_initialisation:\n",
    "    full_state_size = int(training_dataset['initial_states'].shape[-1])\n",
    "    adapter_size = adapter_multiplier * full_state_size\n",
    "else:\n",
    "    adapter_size = 0\n",
    "    \n",
    "model_parameters = {'net_name': net_name,\n",
    "                    'experiment_name': expt_name,\n",
    "                    'training_dataset': tf_data_train,\n",
    "                    'validation_dataset': tf_data_valid,\n",
    "                    'test_dataset': tf_data_test,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'input_size': num_features,\n",
    "                    'output_size': num_outputs,\n",
    "                    'hidden_layer_size': hidden_layer_size,\n",
    "                    'num_epochs': 10, # for test\n",
    "                    'minibatch_size': minibatch_size,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_norm': max_norm,\n",
    "                    'model_folder': model_folder,\n",
    "                    'hidden_activation': hidden_activation,\n",
    "                    'output_activation': output_activation,\n",
    "                    'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "                    'softmax_size': 0, #not used in this paper, but allows for categorical actions\n",
    "                    'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72be6-75cb-4797-bd27-a556bbb21222",
   "metadata": {},
   "source": [
    "#### 2.2 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8994abb-e797-4f8e-ac96-d75e9a5a67d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_model(input_shape, hidden_layer_size, output_size, softmax_size, dropout_rate, \n",
    "#                  memory_activation_type, output_activation_type, use_seq2seq_training_mode, use_seq2seq_feedback, use_truncated_bptt, states=None):\n",
    "def create_model(params, net_name='Default'):\n",
    "    \n",
    "    # Data params\n",
    "    training_data = None if 'training_dataset' not in params else params['training_dataset']\n",
    "    validation_data = None if 'validation_dataset' not in params else params['validation_dataset']\n",
    "    test_data = None if 'test_dataset' not in params else params['test_dataset']\n",
    "    input_size = params['input_size']\n",
    "    output_size = params['output_size']\n",
    "\n",
    "    # Network params\n",
    "    softmax_size = params['softmax_size']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    hidden_layer_size = params['hidden_layer_size']\n",
    "    memory_activation_type = params['hidden_activation']\n",
    "    output_activation_type = params['output_activation']\n",
    "        \n",
    "    initial_states = None\n",
    "    # 输入层\n",
    "    # lstm_additional_size = output_size \\\n",
    "    #                         if not use_seq2seq_training_mode and use_seq2seq_feedback\\\n",
    "    #                         else 0\n",
    "    # input_size=input_minibatch.shape[2] + lstm_additional_size\n",
    "    # input_size=input_shape + lstm_additional_size\n",
    "    inputs = layers.Input(shape=(None,input_size), dtype=tf.float32)\n",
    "\n",
    "    # LSTM 层，设置初始状态和反向传播\n",
    "    # if states is not None and use_truncated_bptt:\n",
    "    #     lstm = layers.LSTM(hidden_layer_size, activation=memory_activation_type, \n",
    "    #                        return_sequences=True, dropout=dropout_rate)(inputs, initial_state=initial_state)\n",
    "    # else:\n",
    "    lstm = layers.LSTM(hidden_layer_size, activation=memory_activation_type, \n",
    "                       return_sequences=True, dropout=dropout_rate)(inputs)\n",
    "\n",
    "    # 展平 LSTM 输出以用于后续的密集层（在tf2.x，不需要进行展平，dense层会自动处理）\n",
    "    # flattened_lstm = layers.Flatten()(lstm)\n",
    "\n",
    "    # 可选的 Seq2Seq 反馈逻辑\n",
    "    use_seq2seq_feedback = False\n",
    "    if use_seq2seq_feedback:\n",
    "        # 这里可以根据您的需求添加 Seq2Seq 反馈逻辑\n",
    "        logits = lstm\n",
    "    else:\n",
    "        # 线性输出层\n",
    "        logits = layers.Dense(output_size)(lstm)\n",
    "\n",
    "    # Softmax 分量处理\n",
    "    if softmax_size != 0:\n",
    "        logits_reshaped = layers.Reshape((-1, output_size))(logits)\n",
    "        core_outputs, softmax_outputs = tf.split(logits_reshaped, [output_size - softmax_size, softmax_size], axis=-1)\n",
    "        core_activated = layers.Activation(output_activation_type)(core_outputs)\n",
    "        softmax_activated = layers.Softmax(axis=-1)(softmax_outputs)\n",
    "        outputs = layers.Concatenate(axis=-1)([core_activated, softmax_activated])\n",
    "    else:\n",
    "        outputs = layers.Activation(output_activation_type)(logits)\n",
    "\n",
    "    # 创建模型\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=net_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f49d3e37-ebdf-44b6-8913-a06edcc81512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Default\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 28)]        0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 112)         63168     \n",
      "                                                                 \n",
      " dense (Dense)               (None, None, 1)           113       \n",
      "                                                                 \n",
      " activation (Activation)     (None, None, 1)           0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63281 (247.19 KB)\n",
      "Trainable params: 63281 (247.19 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model(model_parameters)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7408a-29b0-4032-852e-d79f58f5ed9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Common training routine to all RNN models_without_confounders - seq2seq + standard\n",
    "    \"\"\"\n",
    "\n",
    "    min_epochs = 1\n",
    "    # Setup default hidden layer size\n",
    "    hidden_layer_size = int(memory_multiplier * num_features)\n",
    "\n",
    "    if b_use_state_initialisation:\n",
    "\n",
    "        full_state_size = int(training_dataset['initial_states'].shape[-1])\n",
    "\n",
    "        adapter_size = adapter_multiplier * full_state_size\n",
    "\n",
    "    else:\n",
    "        adapter_size = 0\n",
    "\n",
    "        # Training simulation\n",
    "    model_parameters = {'net_name': net_name,\n",
    "                        'experiment_name': expt_name,\n",
    "                        'training_dataset': tf_data_train,\n",
    "                        'validation_dataset': tf_data_valid,\n",
    "                        'test_dataset': tf_data_test,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'input_size': num_features,\n",
    "                        'output_size': num_outputs,\n",
    "                        'hidden_layer_size': hidden_layer_size,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'minibatch_size': minibatch_size,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'max_norm': max_norm,\n",
    "                        'model_folder': model_folder,\n",
    "                        'hidden_activation': hidden_activation,\n",
    "                        'output_activation': output_activation,\n",
    "                        'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "                        'softmax_size': 0, #not used in this paper, but allows for categorical actions\n",
    "                        'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse',\n",
    "                        'use_seq2seq_feedback': b_use_seq2seq_feedback,\n",
    "                        'use_seq2seq_training_mode': b_use_seq2seq_training_mode,\n",
    "                        'use_memory_adapter': b_use_memory_adapter,\n",
    "                        'memory_adapter_size': adapter_size}\n",
    "\n",
    "    # Get the right model\n",
    "    model = RnnModel(model_parameters)\n",
    "    serialisation_name = model.serialisation_name\n",
    "\n",
    "    if helpers.hyperparameter_result_exists(model_folder, net_name, serialisation_name):\n",
    "        logging.warning(\"Combination found: skipping {}\".format(serialisation_name))\n",
    "        return helpers.load_hyperparameter_results(model_folder, net_name)\n",
    "\n",
    "    training_handles = model.get_training_graph(use_truncated_bptt=use_truncated_bptt,\n",
    "                                                b_use_state_initialisation=b_use_state_initialisation)\n",
    "    validation_handles = model.get_prediction_graph(use_validation_set=True, with_dropout=False,\n",
    "                                                    b_use_state_initialisation=b_use_state_initialisation)\n",
    "\n",
    "    # Start optimising\n",
    "    num_minibatches = int(np.ceil(training_dataset['scaled_inputs'].shape[0] / model_parameters['minibatch_size']))\n",
    "\n",
    "    i = 1\n",
    "    epoch_count = 1\n",
    "    step_count = 1\n",
    "    min_loss = np.inf\n",
    "    with sess.as_default():\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        optimisation_summary = pd.Series([])\n",
    "\n",
    "        while True:\n",
    "            #for step_count in tqdm(range(num_minibatches), desc=f\"Epoch {epoch_count}\"):\n",
    "            try:\n",
    "                # loss, _ = sess.run([training_handles['loss'],\n",
    "                #                     training_handles['optimiser']])\n",
    "                loss, _, numerator = sess.run([training_handles['loss'],\n",
    "                                    training_handles['optimiser'],\n",
    "                                    training_handles['numerator']])\n",
    "\n",
    "                # rango added - tensorflow debugger 2023.10.20###################\n",
    "                # if sess.should_stop():\n",
    "                #     break  # NaN or Inf occurred.\n",
    "                # ###############################################################\n",
    "\n",
    "                # Flog output\n",
    "                if (verbose == True):\n",
    "                    logging.info(\"Epoch {} | iteration = {} of {}, loss = {} | loss_numerator = {} | net = {} | info = {}\".format(\n",
    "                        epoch_count,\n",
    "                        step_count,\n",
    "                        num_minibatches,\n",
    "                        loss,\n",
    "                        numerator,\n",
    "                        model.net_name,\n",
    "                        additonal_info))\n",
    "\n",
    "                if step_count == num_minibatches:\n",
    "\n",
    "                    # Reinit datasets\n",
    "                    sess.run(validation_handles['initializer'])\n",
    "\n",
    "                    means = []\n",
    "                    UBs = []\n",
    "                    LBs = []\n",
    "                    while True:\n",
    "                        try:\n",
    "                            mean, upper_bound, lower_bound = sess.run([validation_handles['mean'],\n",
    "                                                                       validation_handles['upper_bound'],\n",
    "                                                                       validation_handles['lower_bound']])\n",
    "\n",
    "                            means.append(mean)\n",
    "                            UBs.append(upper_bound)\n",
    "                            LBs.append(lower_bound)\n",
    "                        except tf.errors.OutOfRangeError:\n",
    "                            break\n",
    "\n",
    "                    means = np.concatenate(means, axis=0)\n",
    "\n",
    "                    \"\"\"\n",
    "                    means = np.concatenate(means, axis=0)*training_dataset['output_stds'] \\\n",
    "                            + training_dataset['output_means']\n",
    "                    UBs = np.concatenate(UBs, axis=0)*training_dataset['output_stds'] \\\n",
    "                          + training_dataset['output_means']\n",
    "                    LBs = np.concatenate(LBs, axis=0)*training_dataset['output_stds'] \\\n",
    "                          + training_dataset['output_means']\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "                    active_entries = validation_dataset['active_entries']\n",
    "                    output = validation_dataset['outputs']\n",
    "\n",
    "                    if model_parameters['performance_metric'] == \"mse\":\n",
    "                        validation_loss = np.sum((means - output)**2 * active_entries) / np.sum(active_entries)\n",
    "                        #logging.info(\"Epoch {} Detection| Means= {} | Output = {}\".format(epoch_count, means, output))\n",
    "\n",
    "                    elif model_parameters['performance_metric'] == \"xentropy\":\n",
    "                        _, _,features_size = output.shape\n",
    "                        partition_idx = features_size\n",
    "\n",
    "                        # Do binary first\n",
    "                        validation_loss = np.sum((output[:, :, :partition_idx] * -np.log(means[:, :, :partition_idx] + 1e-8)\n",
    "                                                 + (1 - output[:, :, :partition_idx]) * -np.log(1 - means[:, :, :partition_idx] + 1e-8))\n",
    "                                                 * active_entries[:, :, :partition_idx]) \\\n",
    "                                          / np.sum(active_entries[:, :, :partition_idx])\n",
    "\n",
    "                    optimisation_summary[epoch_count] = validation_loss\n",
    "\n",
    "                    # Compute validation loss\n",
    "                    if (verbose == True):\n",
    "                        logging.info(\"Epoch {} Summary| Validation loss = {} | net = {} | info = {}\".format(\n",
    "                            epoch_count,\n",
    "                            validation_loss,\n",
    "                            model.net_name,\n",
    "                            additonal_info))\n",
    "\n",
    "                    if np.isnan(validation_loss):\n",
    "                        logging.warning(\"NAN Loss found, terminating routine\")\n",
    "                        break\n",
    "\n",
    "                    # Save model and loss trajectories\n",
    "                    if validation_loss < min_loss and epoch_count > min_epochs:\n",
    "                        cp_name = serialisation_name + \"_optimal\"\n",
    "                        helpers.save_network(sess, model_folder, cp_name, optimisation_summary)\n",
    "                        min_loss = validation_loss\n",
    "\n",
    "                    # Update\n",
    "                    epoch_count += 1\n",
    "                    step_count = 0\n",
    "\n",
    "                step_count += 1\n",
    "                i += 1\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        # Save final\n",
    "        cp_name = serialisation_name + \"_final\"\n",
    "        helpers.save_network(sess, model_folder, cp_name, optimisation_summary)\n",
    "        helpers.add_hyperparameter_results(optimisation_summary, model_folder, net_name, serialisation_name)\n",
    "\n",
    "        hyperparam_df = helpers.load_hyperparameter_results(model_folder, net_name)\n",
    "\n",
    "        logging.info(\"Terminated at iteration {}\".format(i))\n",
    "\n",
    "    return hyperparam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715d6f0-ef02-461b-aa11-534df9297b19",
   "metadata": {},
   "source": [
    "### 2.3 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118e472-7b2b-499e-a26f-36e12ab81c60",
   "metadata": {},
   "source": [
    "#### 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4705a289-2e81-4828-8f9c-932f701fcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(losses.Loss):\n",
    "    def __init__(self, params, name=\"custom_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.performance_metric = params['performance_metric']\n",
    "        # self.weights = params['weights']\n",
    "        # self.active_entries = params['active_entries']\n",
    "\n",
    "    def train_call(self, y_true, y_pred, active_entries, weights):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def valid_call(self, y_true, y_pred, active_entries):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries ) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries) / tf.reduce_sum(active_entries)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"performance_metric\": self.performance_metric})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a36a1-7f02-4dbb-aed7-11240388c98f",
   "metadata": {},
   "source": [
    "#### 训练步骤和验证步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f283750-1b64-426d-bc8f-ab1d25f5d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=model_parameters['learning_rate'])\n",
    "loss_func = CustomLoss(model_parameters)\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs, outputs, active_entries, weights):\n",
    "    if weights is None:\n",
    "        weights = tf.constant(1.0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(outputs, predictions)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, inputs, outputs, active_entries):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_func.valid_call(outputs, predictions, active_entries)\n",
    "    valid_loss.update_state(loss)\n",
    "    valid_metric.update_state(outputs, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed826629-9926-4112-b4a2-b3ea259f9be5",
   "metadata": {},
   "source": [
    "#### 训练模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b61247a1-98fd-4fd4-8f87-24f998f82176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    # optimisation_summary = pd.Series([])\n",
    "    for epoch in tf.range(1, epochs+1):\n",
    "        \n",
    "        for data in ds_train:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "        for data in ds_valid:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "        # 同样的日志和状态重置操作\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192bf221-bc75-441a-89e0-e74cde192c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModule(tf.Module):\n",
    "    def __init__(self, params, name=None):\n",
    "        super(TrainModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            self.epochs = params['num_epochs']\n",
    "            self.ds_train = params['training_dataset']\n",
    "            self.ds_valid = params['validation_dataset']\n",
    "            self.ds_test = params['test_dataset']\n",
    "            self.optimizer = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "            self.loss_func = CustomLoss(params)\n",
    "\n",
    "            self.train_loss = metrics.Mean(name='train_loss')\n",
    "            self.train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "            self.valid_loss = metrics.Mean(name='valid_loss')\n",
    "            self.valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, model, inputs, outputs, active_entries, weights):\n",
    "        if weights is None:\n",
    "            weights = tf.constant(1.0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = self.loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        self.train_loss.update_state(loss)\n",
    "        self.train_metric.update_state(outputs, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(self, model, inputs, outputs, active_entries):\n",
    "        predictions = model(inputs, training=False)\n",
    "        loss = self.loss_func.valid_call(outputs, predictions, active_entries)\n",
    "        self.valid_loss.update_state(loss)\n",
    "        self.valid_metric.update_state(outputs, predictions)\n",
    "    \n",
    "    def train_model(self, model):\n",
    "        # optimisation_summary = pd.Series([])\n",
    "        for epoch in tf.range(1, self.epochs+1):\n",
    "\n",
    "            for data in self.ds_train:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "            for data in self.ds_valid:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "            # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "            # 同样的日志和状态重置操作\n",
    "            logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "\n",
    "            if epoch%1 ==0:\n",
    "                printbar()\n",
    "                tf.print(tf.strings.format(logs,\n",
    "                (epoch,self.train_loss.result(),self.train_metric.result(),self.valid_loss.result(),self.valid_metric.result())))\n",
    "                tf.print(\"\")\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.valid_loss.reset_states()\n",
    "            self.train_metric.reset_states()\n",
    "            self.valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4849e3b-b60b-47af-a82a-c493c5018995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================14:04:03\n",
      "Epoch=1,Loss:0.499527872,Accuracy:0.550320148,Valid Loss:0.516167939,Valid Accuracy:0.517898262\n",
      "\n",
      "================================================================================14:04:04\n",
      "Epoch=2,Loss:0.475318611,Accuracy:0.527793229,Valid Loss:0.506939232,Valid Accuracy:0.508185446\n",
      "\n",
      "================================================================================14:04:04\n",
      "Epoch=3,Loss:0.465685338,Accuracy:0.526379406,Valid Loss:0.510615706,Valid Accuracy:0.511348367\n",
      "\n",
      "================================================================================14:04:05\n",
      "Epoch=4,Loss:0.45916155,Accuracy:0.529349089,Valid Loss:0.516534448,Valid Accuracy:0.510599077\n",
      "\n",
      "================================================================================14:04:06\n",
      "Epoch=5,Loss:0.454828203,Accuracy:0.528462529,Valid Loss:0.516691566,Valid Accuracy:0.520721555\n",
      "\n",
      "================================================================================14:04:06\n",
      "Epoch=6,Loss:0.482320428,Accuracy:0.559840798,Valid Loss:0.525589466,Valid Accuracy:0.525901377\n",
      "\n",
      "================================================================================14:04:07\n",
      "Epoch=7,Loss:0.468005776,Accuracy:0.529510081,Valid Loss:0.542681396,Valid Accuracy:0.534254\n",
      "\n",
      "================================================================================14:04:08\n",
      "Epoch=8,Loss:0.457480669,Accuracy:0.533512414,Valid Loss:0.51867938,Valid Accuracy:0.512872338\n",
      "\n",
      "================================================================================14:04:09\n",
      "Epoch=9,Loss:0.450703472,Accuracy:0.534296751,Valid Loss:0.513790131,Valid Accuracy:0.515044034\n",
      "\n",
      "================================================================================14:04:09\n",
      "Epoch=10,Loss:0.443599194,Accuracy:0.533094049,Valid Loss:0.527673125,Valid Accuracy:0.528042316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Train = TrainModule(model_parameters)\n",
    "Train.train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9032fff3-d7d8-42f4-b6ef-5d1f2114c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================12:15:12\n",
      "Epoch=1,Loss:-1.04218924,Accuracy:0.315503478,Valid Loss:-1.6271013,Valid Accuracy:0.256319433\n",
      "\n",
      "================================================================================12:15:13\n",
      "Epoch=2,Loss:-1.8213017,Accuracy:0.25058189,Valid Loss:-1.92948687,Valid Accuracy:0.243127644\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=3,Loss:-1.99545646,Accuracy:0.245146066,Valid Loss:-2.00789499,Valid Accuracy:0.237991899\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=4,Loss:-2.0445447,Accuracy:0.242646903,Valid Loss:-2.07682967,Valid Accuracy:0.236074388\n",
      "\n",
      "================================================================================12:15:15\n",
      "Epoch=5,Loss:-2.07054353,Accuracy:0.241571859,Valid Loss:-2.07036352,Valid Accuracy:0.23433511\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=6,Loss:-2.08658314,Accuracy:0.24051398,Valid Loss:-2.02451658,Valid Accuracy:0.234643\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=7,Loss:-2.08822966,Accuracy:0.239788935,Valid Loss:-2.04530382,Valid Accuracy:0.233269155\n",
      "\n",
      "================================================================================12:15:17\n",
      "Epoch=8,Loss:-2.10269213,Accuracy:0.239260048,Valid Loss:-2.09449148,Valid Accuracy:0.231957987\n",
      "\n",
      "================================================================================12:15:18\n",
      "Epoch=9,Loss:-2.10704803,Accuracy:0.238550946,Valid Loss:-2.07969904,Valid Accuracy:0.231935427\n",
      "\n",
      "================================================================================12:15:19\n",
      "Epoch=10,Loss:-2.12130713,Accuracy:0.238038853,Valid Loss:-2.11661029,Valid Accuracy:0.231317282\n",
      "\n",
      "================================================================================12:15:24\n",
      "Epoch=1,Loss:-1.21397805,Accuracy:0.299676478,Valid Loss:-1.7329247,Valid Accuracy:0.245012134\n",
      "\n",
      "================================================================================12:15:25\n",
      "Epoch=2,Loss:-1.89918232,Accuracy:0.247787476,Valid Loss:-2.00681877,Valid Accuracy:0.239056319\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=3,Loss:-2.02381587,Accuracy:0.244440153,Valid Loss:-2.01860476,Valid Accuracy:0.236832559\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=4,Loss:-2.0467248,Accuracy:0.242398903,Valid Loss:-1.99007571,Valid Accuracy:0.235698864\n",
      "\n",
      "================================================================================12:15:27\n",
      "Epoch=5,Loss:-2.06792665,Accuracy:0.240782276,Valid Loss:-2.07615876,Valid Accuracy:0.23492761\n",
      "\n",
      "================================================================================12:15:28\n",
      "Epoch=6,Loss:-2.08954358,Accuracy:0.240094557,Valid Loss:-2.09414053,Valid Accuracy:0.233243883\n",
      "\n",
      "================================================================================12:15:29\n",
      "Epoch=7,Loss:-2.10589767,Accuracy:0.239126161,Valid Loss:-2.08491945,Valid Accuracy:0.233909249\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=8,Loss:-2.10808825,Accuracy:0.238345563,Valid Loss:-2.10724974,Valid Accuracy:0.23198238\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=9,Loss:-2.11758161,Accuracy:0.238061085,Valid Loss:-2.09254766,Valid Accuracy:0.231905296\n",
      "\n",
      "================================================================================12:15:31\n",
      "Epoch=10,Loss:-2.12772536,Accuracy:0.237541556,Valid Loss:-2.09236026,Valid Accuracy:0.231176525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_names = ['model1','model2']\n",
    "for name in net_names:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(model_parameters, name)\n",
    "    # train_model(model, tf_data_train, tf_data_valid, 10)\n",
    "    Train = TrainModule(model_parameters)\n",
    "    Train.train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660dcda-5d51-47ae-a57e-3570fe4c5eca",
   "metadata": {},
   "source": [
    "### 2.4 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca9b3e36-cb8b-4208-8e4d-a33d28fbc990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/savedmodel_try/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/savedmodel_try/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export saved model.\n"
     ]
    }
   ],
   "source": [
    "path = 'results/tf2_try/savedmodel_try'\n",
    "model.save(path, save_format = 'tf')\n",
    "print('export saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12fb4e51-2e73-43bb-add4-fae0b01b772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = models.load_model(path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d54a8-ba90-4260-9ffb-9b12532697b5",
   "metadata": {},
   "source": [
    "### 2.5 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f9a8e93-6b0f-440f-b46d-94de062a6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型评估函数\n",
    "def evaluate_model(model, ds_test):\n",
    "    total_loss = 0\n",
    "    total_metric = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 遍历数据集中的每个批次\n",
    "    for data in ds_test:\n",
    "        valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        total_loss += valid_loss.result().numpy()\n",
    "        total_metric += valid_metric.result().numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 重置状态\n",
    "        valid_loss.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "    # 计算整个数据集的平均损失和指标\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metric = total_metric / num_batches\n",
    "    return avg_loss, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6989a47-7ac5-4e66-9faf-ad79db5a5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -2.254194164276123, Test Accuracy: 0.22184113562107086\n",
      "5/5 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = evaluate_model(model, tf_data_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "input_data_for_prediction = tf_data_test.map(lambda x: x['inputs'])\n",
    "predictions = model_loaded.predict(input_data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee42e19-e360-49bb-9fda-3c35c119e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 160, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a3425-92cd-4780-96a5-d3ab5bc8e876",
   "metadata": {},
   "source": [
    "## Propensitry Generation\n",
    "def propensity_generation(dataset_map, MODEL_ROOT, b_use_predicted_confounders, b_use_all_data=False,\n",
    "                          b_use_oracle_confounders=False, b_remove_x1=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0435e9e2-8e77-4b4a-8b6c-f734143bc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmsn.libs.model_process as model_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8fb8d8-42a6-4321-b7a8-6fe4a0293f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/rmsn_result_test_use_confounders_True/treatment_rnn_action_inputs_only/treatment_rnn_action_inputs_only.csv\n",
      "results/rmsn_result_test_use_confounders_True/treatment_rnn/treatment_rnn.csv\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "action_inputs_only = model_process.load_optimal_parameters(net_name='treatment_rnn_action_inputs_only', MODEL_ROOT=MODEL_ROOT)\n",
    "action_w_trajectory_inputs = model_process.load_optimal_parameters(net_name='treatment_rnn', MODEL_ROOT=MODEL_ROOT)\n",
    "\n",
    "# Generate propensity weights for validation data as well - used for MSM which is calibrated on train + valid data\n",
    "b_with_validation = False\n",
    "# Generate non-stabilised IPTWs (default false)\n",
    "b_denominator_only = False\n",
    "b_use_predicted_confounders = True\n",
    "b_use_all_data=False\n",
    "b_use_oracle_confounders=False\n",
    "b_remove_x1=False\n",
    "\n",
    "# Config + activation functions\n",
    "activation_map = {'rnn_propensity_weighted': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'rnn_model_bptt': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  }\n",
    "\n",
    "configs = {'action_num': action_inputs_only,\n",
    "           'action_den': action_w_trajectory_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d86ece3-4f82-4587-bd20-003ca45d3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if b_use_all_data:\n",
    "    training_data = dataset_map\n",
    "    validation_data = dataset_map\n",
    "    test_data = None\n",
    "else:\n",
    "    training_data = dataset_map['training_data']\n",
    "    validation_data = dataset_map['validation_data']\n",
    "    test_data = dataset_map['test_data']\n",
    "    \n",
    "if b_with_validation:\n",
    "    for k in training_data:\n",
    "        training_data[k] = np.concatenate([training_data[k], validation_data[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51801536-64ee-4238-a803-694c3569d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_predictions(config):\n",
    "\n",
    "    net_name = config[0]\n",
    "    serialisation_name = config[-1]\n",
    "\n",
    "    hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "    # Pull datasets\n",
    "    b_predict_actions = \"treatment_rnn\" in net_name\n",
    "    b_use_actions_only = \"rnn_action_inputs_only\" in net_name\n",
    "\n",
    "    # Extract only relevant trajs and shift data\n",
    "    training_processed = get_processed_data(training_data, b_predict_actions, b_use_actions_only,\n",
    "                                                 b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                                   b_use_actions_only,\n",
    "                                                   b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    # rango added 23.10.24\n",
    "    # if b_with_test:\n",
    "    #     test_processed = core.get_processed_data(test_data, b_predict_actions, b_use_actions_only,\n",
    "    #                                              b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "    num_features = training_processed['scaled_inputs'].shape[-1]  # 4 if not b_use_actions_only else 3\n",
    "    num_outputs = training_processed['scaled_outputs'].shape[-1]  # 1 if not b_predict_actions else 3  # 5\n",
    "\n",
    "\n",
    "    # Unpack remaining variables\n",
    "    dropout_rate = config[1]\n",
    "    memory_multiplier = config[2] / num_features\n",
    "    num_epochs = config[3]\n",
    "    minibatch_size = config[4]\n",
    "    learning_rate = config[5]\n",
    "    max_norm = config[6]\n",
    "    tf_data_train = convert_to_tf_dataset(training_processed, minibatch_size)\n",
    "    tf_data_valid = convert_to_tf_dataset(validation_processed, minibatch_size)\n",
    "\n",
    "    model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "    model = model_process.load_model(model_folder, serialisation_name)\n",
    "\n",
    "    # predictition\n",
    "    outputs = training_processed['scaled_outputs']\n",
    "    results = model_predict(model, tf_data_train)\n",
    "    predictions = results['mean_pred']\n",
    "    #means, outputs, _, _ = test(training_processed, validation_processed, training_processed, tf_config,\n",
    "    #                            net_name, expt_name, dropout_rate, num_features, num_outputs,\n",
    "    #                            memory_multiplier, num_epochs, minibatch_size, learning_rate, max_norm,\n",
    "    #                            hidden_activation, output_activation, model_folder)\n",
    "\n",
    "    return predictions, outputs\n",
    "\n",
    "def get_weights(probs, targets):\n",
    "    w = probs*targets + (1-probs) * (1-targets)\n",
    "    return w.prod(axis=2)\n",
    "\n",
    "\n",
    "def get_weights_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    probs, targets = get_predictions(config)\n",
    "\n",
    "    return get_weights(probs, targets)\n",
    "\n",
    "def get_probabilities_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    probs, targets = get_predictions(config)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe7c88d4-c3d3-4871-a8c3-6df5d70fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def model_predict(model, dataset, pred_times=100):\n",
    "    # Initialize lists to store final statistics for all chunks\n",
    "    all_means = []\n",
    "    all_upper_bounds = []\n",
    "    all_lower_bounds = []\n",
    "    logs = 'Predicting ' + model.name\n",
    "\n",
    "    for data_chunk in tqdm(dataset, desc=logs):\n",
    "        chunk_predictions = []\n",
    "\n",
    "        # Predict the current chunk multiple times\n",
    "        for _ in range(pred_times):\n",
    "            prediction = model.predict(data_chunk['inputs'], verbose=0)\n",
    "            chunk_predictions.append(prediction)\n",
    "\n",
    "        # Convert list of predictions to a numpy array for statistical computation\n",
    "        chunk_predictions = np.array(chunk_predictions)\n",
    "\n",
    "        # Calculate mean, upper bound, and lower bound for the current chunk\n",
    "        mean_estimate = np.mean(chunk_predictions, axis=0)\n",
    "        upper_bound = np.percentile(chunk_predictions, 95, axis=0)\n",
    "        lower_bound = np.percentile(chunk_predictions, 5, axis=0)\n",
    "\n",
    "        # Append the statistics of the current chunk to their respective lists\n",
    "        all_means.append(mean_estimate)\n",
    "        all_upper_bounds.append(upper_bound)\n",
    "        all_lower_bounds.append(lower_bound)\n",
    "\n",
    "    # Optional: Convert lists to numpy arrays if further processing is needed\n",
    "    all_means = np.concatenate(all_means, axis=0) if all_means else np.array([])\n",
    "    all_upper_bounds = np.concatenate(all_upper_bounds, axis=0) if all_upper_bounds else np.array([])\n",
    "    all_lower_bounds = np.concatenate(all_lower_bounds, axis=0) if all_lower_bounds else np.array([])\n",
    "\n",
    "    # At this point, you can either return the raw statistics for each chunk,\n",
    "    # or aggregate them in some way depending on your application's needs.\n",
    "    # The following returns the list of statistics for all chunks.\n",
    "    return {\n",
    "        'mean_pred': all_means,\n",
    "        'upper_bound': all_upper_bounds,\n",
    "        'lower_bound': all_lower_bounds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "719e1d4f-a171-4cf0-b8b4-564d81441371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 151/151 [00:41<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predicted confounders\n",
      "Using predicted confounders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn: 100%|██████████| 301/301 [01:06<00:00,  4.51it/s]\n",
      "/tmp/ipykernel_47549/1390237117.py:7: RuntimeWarning: divide by zero encountered in divide\n",
      "  propensity_weights = 1.0/den if b_denominator_only else num/den\n"
     ]
    }
   ],
   "source": [
    "# Action with trajs\n",
    "weights = {k: get_weights_from_config(configs[k]) for k in configs}\n",
    "\n",
    "den = weights['action_den']\n",
    "num = weights['action_num']\n",
    "\n",
    "propensity_weights = 1.0/den if b_denominator_only else num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57f89f64-7876-4344-92c3-8091eade4441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truncation @ 95th and 5th percentiles\n",
    "UB = np.percentile(propensity_weights, 99)\n",
    "LB = np.percentile(propensity_weights, 1)\n",
    "\n",
    "propensity_weights[propensity_weights > UB] = UB\n",
    "propensity_weights[propensity_weights < LB] = LB\n",
    "\n",
    "# Adjust so for 3 trajectories here\n",
    "horizon = 1\n",
    "(num_patients, num_time_steps) = propensity_weights.shape\n",
    "output = np.ones((num_patients, num_time_steps, horizon))\n",
    "\n",
    "tmp = np.ones((num_patients, num_time_steps))\n",
    "tmp[:, 1:] = propensity_weights[:, :-1]\n",
    "propensity_weights = tmp\n",
    "\n",
    "for i in range(horizon):\n",
    "    output[:, :num_time_steps-i, i] = propensity_weights[:, i:]\n",
    "\n",
    "propensity_weights = output.cumprod(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3f4cb85-96b2-4bbd-a792-e0ba600fb962",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m all_upper_bounds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m all_lower_bounds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtf_data_train\u001b[49m:\n\u001b[1;32m      6\u001b[0m    chunk_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m    \u001b[38;5;66;03m# Predict the current chunk multiple times\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_data_train' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "all_means = []\n",
    "all_upper_bounds = []\n",
    "all_lower_bounds = []\n",
    "\n",
    "for data_chunk in tf_data_train:\n",
    "    chunk_predictions = []\n",
    "\n",
    "    # Predict the current chunk multiple times\n",
    "    for _ in range(pred_times):\n",
    "        prediction = model.predict(data_chunk['inputs'], verbose=0)\n",
    "        chunk_predictions.append(prediction)\n",
    "\n",
    "    # Convert list of predictions to a numpy array for statistical computation\n",
    "    chunk_predictions = np.array(chunk_predictions)\n",
    "\n",
    "    # Calculate mean, upper bound, and lower bound for the current chunk\n",
    "    mean_estimate = np.mean(chunk_predictions, axis=0)\n",
    "    upper_bound = np.percentile(chunk_predictions, 95, axis=0)\n",
    "    lower_bound = np.percentile(chunk_predictions, 5, axis=0)\n",
    "\n",
    "    # Optional: Convert lists to numpy arrays if further processing is needed\n",
    "    all_means = np.concatenate(mean_estimate, axis=0)\n",
    "    all_upper_bounds = np.concatenate(upper_bound, axis=0)\n",
    "    all_lower_bounds = np.concatenate(lower_bound, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a8c30-8eba-4d47-a20b-ef6c4d10e008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_causal",
   "language": "python",
   "name": "new_causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
