{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a666667-11e2-467c-b89c-55b3cbfeed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ee2816-7992-437b-bd8a-10df5202ecc0",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb48e40a-53e0-4e31-8be6-c8f174ba2b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39979422 entries, 0 to 39979421\n",
      "Data columns (total 16 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   pid                 int64  \n",
      " 1   date                int64  \n",
      " 2   move_frequency      float64\n",
      " 3   move_distance       float64\n",
      " 4   trip_time           float64\n",
      " 5   home_time           float64\n",
      " 6   norm_sum            float64\n",
      " 7   norm_mean           float64\n",
      " 8   norm_max            float64\n",
      " 9   norm_home_activity  float64\n",
      " 10  norm_home_base      float64\n",
      " 11  group_year          int64  \n",
      " 12  voluntary           int64  \n",
      " 13  gender              int64  \n",
      " 14  age                 int64  \n",
      " 15  income              float64\n",
      "dtypes: float64(10), int64(6)\n",
      "memory usage: 4.8 GB\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "data = pd.read_csv(\"data/demo12W_30.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60fc620f-bad9-460a-8ee8-07d88e4b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation\n",
    "# 选择日期在20191201到20191231之间的行，并删除重复的行\n",
    "data_comp = data[data['date'].between(20191201, 20191231)].drop_duplicates(subset=['pid', 'date'])\n",
    "data = data[~data['date'].between(20191201, 20191231)]\n",
    "data_comp['group_year'] = 1\n",
    "\n",
    "# 合并数据\n",
    "data = pd.concat([data, data_comp], axis=0).sort_values(by=['pid', 'date'])\n",
    "\n",
    "# NOTE: `na_seadec`是R中的特定函数，需要Python实现或找到相应的Python替代。\n",
    "# 例如，下面使用pandas的interpolate函数作为替代。\n",
    "data['norm_mean'] = data.groupby(['pid', 'group_year'])['norm_mean'].transform(lambda x: x.interpolate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5821ff2f-97f3-4292-b8e8-f2647426765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>precip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20190101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20190102</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20190103</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20190104</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20190105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       date  precip\n",
       "0  20190101     0.0\n",
       "1  20190102     0.1\n",
       "2  20190103     0.0\n",
       "3  20190104     0.1\n",
       "4  20190105     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weather data\n",
    "precip = pd.read_csv(\"data/precipitation.csv\")\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "184706ce-d2f9-44b8-b809-5ddffc66c4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding factors\n",
    "# Using the query function to filter data\n",
    "data = data.query('date < 20191201 or date > 20191231')\n",
    "\n",
    "# Convert date to string then to datetime object and create weekday column\n",
    "data['datestamp'] = pd.to_datetime(data['date'].astype(str), format=\"%Y%m%d\")\n",
    "data['weekday'] = data['datestamp'].dt.weekday\n",
    "data['weekfactor'] = data['weekday'].astype(str)\n",
    "\n",
    "# Timeline calculations\n",
    "data['timeline'] = np.where(data['group_year'] == 1, \n",
    "                           (data['datestamp'] - pd.Timestamp('2020-01-01')).dt.days, \n",
    "                           (data['datestamp'] - pd.Timestamp('2019-01-01')).dt.days)\n",
    "\n",
    "# Define event and festival\n",
    "date_conditions = [\n",
    "    (data['date'].between(20200120, 20200123)),\n",
    "    (data['date'].between(20200124, 20200223)),\n",
    "    (data['date'].between(20200224, 20200508)),\n",
    "    (data['date'] >= 20200509),\n",
    "    (data['date'].between(20200210, 20200216)),\n",
    "    (data['date'].between(20200217, 20200223)),\n",
    "    (data['date'].between(20200321, 20200328)),\n",
    "    (data['date'].between(20200329, 20200508))\n",
    "]\n",
    "\n",
    "event_columns = ['Tran', 'E1', 'E2', 'E3', 'R1', 'R2', 'Low', 'Open']\n",
    "# Tran：人传人；E1：一级响应；E2：二级响应；E3：三级响应；R1：第一次复工；R2：第二次复工；Low：低风险；Open：公共空间开放\n",
    "\n",
    "for col, cond in zip(event_columns, date_conditions):\n",
    "    data[col] = np.where(cond, 1, 0)\n",
    "\n",
    "# Define tp0 - tp7 columns（代表上述的8个不同时期，不重合）\n",
    "data['tp0'] = np.where(data['date'].between(20200120, 20200123), 1, 0)\n",
    "data['tp1'] = np.where(data['date'].between(20200124, 20200209), 1, 0)\n",
    "data['tp2'] = np.where(data['date'].between(20200210, 20200216), 1, 0)\n",
    "data['tp3'] = np.where(data['date'].between(20200217, 20200223), 1, 0)\n",
    "data['tp4'] = np.where(data['date'].between(20200224, 20200320), 1, 0)\n",
    "data['tp5'] = np.where(data['date'].between(20200321, 20200328), 1, 0)\n",
    "data['tp6'] = np.where(data['date'].between(20200329, 20200508), 1, 0)\n",
    "data['tp7'] = np.where(data['date'] >= 20200509, 1, 0)\n",
    "\n",
    "# Define holidays columns\n",
    "holidays_conditions = [\n",
    "    (data['date'].isin([20190101, 20200101])),\n",
    "    (data['date'].between(20190128, 20190204) | data['date'].between(20200117, 20200124)),\n",
    "    (data['date'].between(20190204, 20190210) | data['date'].between(20200124, 20200202)),\n",
    "    (data['date'].between(20190405, 20190407) | data['date'].between(20200404, 20200406)),\n",
    "    (data['date'].between(20190501, 20190504) | data['date'].between(20200501, 20200505))\n",
    "]\n",
    "\n",
    "holidays_columns = ['newyear', 'sprtransp', 'spring', 'tomb', 'labor']\n",
    "for col, cond in zip(holidays_columns, holidays_conditions):\n",
    "    data[col] = np.where(cond, 1, 0)\n",
    "    \n",
    "# Joining the data and precip DataFrames\n",
    "data = pd.merge(data, precip, on=\"date\", how=\"inner\")\n",
    "\n",
    "# Calculating the quantiles for the income column\n",
    "income_thre = data['income'].quantile(q=[0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "# Updating and creating new columns\n",
    "data['age'] = pd.cut(data['age'], bins=[0, 5, 13, 16, float('inf')], labels=['Young', 'Adult', 'Elder', 'Elder'], right=False,  ordered=False)\n",
    "data['gender'] = data['gender'].map({1: 'Male', 2: 'Female'})\n",
    "data['income'] = pd.cut(data['income'], bins=[0] + list(income_thre)[1:], labels=['Bottom', 'Lower', 'Middle', 'Upper', 'Superior'], right=False, include_lowest=True)\n",
    "\n",
    "data['income3'] = data['income'].map({\n",
    "    'Bottom': 'Bottom',\n",
    "    'Superior': 'Upper',\n",
    "    'Lower': 'Middle',\n",
    "    'Middle': 'Middle',\n",
    "    'Upper': 'Middle'\n",
    "})\n",
    "\n",
    "# Converting columns to category type with specific levels\n",
    "data['age'] = data['age'].astype('category')\n",
    "data['gender'] = data['gender'].astype('category')\n",
    "data['income'] = pd.Categorical(data['income'], categories=[\"Bottom\", \"Lower\", \"Middle\", \"Upper\", \"Superior\"], ordered=True)\n",
    "data['income3'] = pd.Categorical(data['income3'], categories=[\"Middle\", \"Bottom\", \"Upper\"], ordered=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99370496-1210-4b90-862e-5e70f967fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/demo12W_30_processed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5e68a5-62a4-448a-baf8-afa61e7f5d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid                         0\n",
      "date                        0\n",
      "move_frequency              0\n",
      "move_distance               0\n",
      "trip_time                   0\n",
      "home_time                   0\n",
      "norm_sum              1848403\n",
      "norm_mean               34600\n",
      "norm_max              1848403\n",
      "norm_home_activity    6086872\n",
      "norm_home_base        2249310\n",
      "group_year                  0\n",
      "voluntary                   0\n",
      "gender                      0\n",
      "age                         0\n",
      "income                   6060\n",
      "datestamp                   0\n",
      "weekday                     0\n",
      "weekfactor                  0\n",
      "timeline                    0\n",
      "Tran                        0\n",
      "E1                          0\n",
      "E2                          0\n",
      "E3                          0\n",
      "R1                          0\n",
      "R2                          0\n",
      "Low                         0\n",
      "Open                        0\n",
      "tp0                         0\n",
      "tp1                         0\n",
      "tp2                         0\n",
      "tp3                         0\n",
      "tp4                         0\n",
      "tp5                         0\n",
      "tp6                         0\n",
      "tp7                         0\n",
      "newyear                     0\n",
      "sprtransp                   0\n",
      "spring                      0\n",
      "tomb                        0\n",
      "labor                       0\n",
      "precip                      0\n",
      "income3                  6060\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b5cc9a-5848-4218-99d6-e85717b37b90",
   "metadata": {},
   "source": [
    "# data sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96536da9-f0b4-46d2-82d9-5015da7f4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/demo12W_30_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8914955-5c9f-4cd9-aa97-7ee5e61cc136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sampling: select 5,000 individuals randomly\n",
    "sample_it = data['pid'].drop_duplicates().sample(n=10000)\n",
    "sample_data = data[data['pid'].isin(sample_it)]\n",
    "#print(sample_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7210a35d-fcf9-4ae5-943a-f38d9047ac9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bde33f-d217-42b1-922c-8ba2b1e394f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_data.sort_values(by=['pid', 'date']).reset_index(drop=True)# use yesterday norm_mean as conformity factor\n",
    "sample_data['conformity'] = sample_data.groupby('pid')['norm_mean'].shift(1)\n",
    "# fill null in conformity\n",
    "sample_data['conformity'] = sample_data['conformity'].fillna(0)\n",
    "sample_data['log_conformity'] = np.log(sample_data['conformity'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e60d703e-e974-4b2d-9cdb-994c4aacd6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0.000000\n",
       "1          7.188280\n",
       "2          7.779820\n",
       "3          7.698167\n",
       "4          7.782097\n",
       "             ...   \n",
       "3029995    6.017498\n",
       "3029996    6.016901\n",
       "3029997    6.872145\n",
       "3029998    6.027145\n",
       "3029999    5.981465\n",
       "Name: log_conformity, Length: 3030000, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data['log_conformity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42397f75-970d-42c0-ae8e-c5652678b332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain treatment intervals (conformity) using PAM clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "# built KMeans class and fit data\n",
    "X = sample_data['log_conformity'].values.reshape(-1, 1)\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(X)\n",
    "\n",
    "# get cluster labels\n",
    "labels = kmeans.labels_\n",
    "sample_data['conformity_class'] = labels\n",
    "\n",
    "# 获取每个聚类的中心值\n",
    "cluster_centers = kmeans.cluster_centers_.flatten()\n",
    "\n",
    "# 根据中心值对标签进行排序\n",
    "sorted_label_idx = cluster_centers.argsort()\n",
    "\n",
    "# 创建一个映射\n",
    "label_mapping = {original_label: new_label for new_label, original_label in enumerate(sorted_label_idx)}\n",
    "\n",
    "# 更新数据帧的标签\n",
    "sample_data['conformity_class'] = sample_data['conformity_class'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7082987-e35a-4cc5-ac4c-a5b73c651192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14943 0.0 13.803\n",
      "439492 13.806 285.811\n",
      "1057498 285.812 826.595\n",
      "1072992 826.596 2388.876\n",
      "445075 2388.887 38101.0\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_data[sample_data['conformity_class']==0]['conformity']), min(sample_data[sample_data['conformity_class']==0]['conformity']), \n",
    "      max(sample_data[sample_data['conformity_class']==0]['conformity']))\n",
    "print(len(sample_data[sample_data['conformity_class']==1]['conformity']), min(sample_data[sample_data['conformity_class']==1]['conformity']), \n",
    "      max(sample_data[sample_data['conformity_class']==1]['conformity']))\n",
    "print(len(sample_data[sample_data['conformity_class']==2]['conformity']), min(sample_data[sample_data['conformity_class']==2]['conformity']), max(sample_data[sample_data['conformity_class']==2]['conformity']))\n",
    "print(len(sample_data[sample_data['conformity_class']==3]['conformity']), min(sample_data[sample_data['conformity_class']==3]['conformity']), max(sample_data[sample_data['conformity_class']==3]['conformity']))\n",
    "print(len(sample_data[sample_data['conformity_class']==4]['conformity']), min(sample_data[sample_data['conformity_class']==4]['conformity']), max(sample_data[sample_data['conformity_class']==4]['conformity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a30d7c8-536e-4c12-98ad-a7becb8a99cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 2.694829862931355\n",
      "2.6950325040209067 5.658823462243795\n",
      "5.6588269488543395 6.718523904287164\n",
      "6.718525112606929 7.778996760734406\n",
      "7.779001363473078 10.54802205317538\n"
     ]
    }
   ],
   "source": [
    "print(min(sample_data[sample_data['conformity_class']==0]['log_conformity']), max(sample_data[sample_data['conformity_class']==0]['log_conformity']))\n",
    "print(min(sample_data[sample_data['conformity_class']==1]['log_conformity']), max(sample_data[sample_data['conformity_class']==1]['log_conformity']))\n",
    "print(min(sample_data[sample_data['conformity_class']==2]['log_conformity']), max(sample_data[sample_data['conformity_class']==2]['log_conformity']))\n",
    "print(min(sample_data[sample_data['conformity_class']==3]['log_conformity']), max(sample_data[sample_data['conformity_class']==3]['log_conformity']))\n",
    "print(min(sample_data[sample_data['conformity_class']==4]['log_conformity']), max(sample_data[sample_data['conformity_class']==4]['log_conformity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8021cb32-f7bc-4639-8c97-11430ad7b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build conformity class\n",
    "# 定义边界\n",
    "bins = [0.0, 1173.652, 2942.316, 5842.279, 10852.704, float('inf')]\n",
    "\n",
    "# 使用cut函数\n",
    "sample_data['conformity_class'] = pd.cut(sample_data['conformity'], bins=bins, labels=[0, 1, 2, 3, 4], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e33bf309-b81e-4501-b387-924c0dd22a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 直方图\n",
    "sns.histplot(sample_data['conformity'], kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63903af2-7042-40e4-9e25-6e7869502c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 14943\n",
      "1: 439492\n",
      "2: 1057498\n",
      "3: 1072992\n",
      "4: 445075\n"
     ]
    }
   ],
   "source": [
    "unique_values, counts = np.unique(sample_data['conformity_class'].values, return_counts=True)\n",
    "\n",
    "# 将结果打印为类似于 dataframe 的 value_counts 的格式\n",
    "for value, count in zip(unique_values, counts):\n",
    "    print(f\"{value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "133444ed-b1ff-4aef-ad87-841b812976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['week'] =  pd.to_datetime(sample_data['date'].astype(str), format=\"%Y%m%d\").dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b40ef8aa-3084-4c8f-a01a-afc743e96359",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>date</th>\n",
       "      <th>move_frequency</th>\n",
       "      <th>move_distance</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>home_time</th>\n",
       "      <th>norm_sum</th>\n",
       "      <th>norm_mean</th>\n",
       "      <th>norm_max</th>\n",
       "      <th>norm_home_activity</th>\n",
       "      <th>...</th>\n",
       "      <th>newyear</th>\n",
       "      <th>sprtransp</th>\n",
       "      <th>spring</th>\n",
       "      <th>tomb</th>\n",
       "      <th>labor</th>\n",
       "      <th>precip</th>\n",
       "      <th>income3</th>\n",
       "      <th>conformity</th>\n",
       "      <th>conformity_class</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1930</td>\n",
       "      <td>20190101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.000</td>\n",
       "      <td>10083.266667</td>\n",
       "      <td>713.439</td>\n",
       "      <td>881.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Middle</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1930</td>\n",
       "      <td>20190102</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16453.0</td>\n",
       "      <td>8910.0</td>\n",
       "      <td>12.206</td>\n",
       "      <td>16659.166667</td>\n",
       "      <td>577.107</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>15642.733</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Middle</td>\n",
       "      <td>713.439</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1930</td>\n",
       "      <td>20190103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18527.0</td>\n",
       "      <td>2494.0</td>\n",
       "      <td>13.189</td>\n",
       "      <td>22684.433333</td>\n",
       "      <td>610.891</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>15965.033</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Middle</td>\n",
       "      <td>577.107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1930</td>\n",
       "      <td>20190104</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26393.0</td>\n",
       "      <td>6624.0</td>\n",
       "      <td>5.016</td>\n",
       "      <td>15067.466667</td>\n",
       "      <td>766.142</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>10567.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Middle</td>\n",
       "      <td>610.891</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1930</td>\n",
       "      <td>20190105</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4656.0</td>\n",
       "      <td>0.268</td>\n",
       "      <td>9763.033333</td>\n",
       "      <td>999.628</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>475.200</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Middle</td>\n",
       "      <td>766.142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pid      date  move_frequency  move_distance  trip_time  home_time  \\\n",
       "0  1930  20190101             0.0            0.0        0.0     24.000   \n",
       "1  1930  20190102             5.0        16453.0     8910.0     12.206   \n",
       "2  1930  20190103             4.0        18527.0     2494.0     13.189   \n",
       "3  1930  20190104             5.0        26393.0     6624.0      5.016   \n",
       "4  1930  20190105             5.0            0.0     4656.0      0.268   \n",
       "\n",
       "       norm_sum  norm_mean  norm_max  norm_home_activity  ...  newyear  \\\n",
       "0  10083.266667    713.439     881.0                 NaN  ...        1   \n",
       "1  16659.166667    577.107    1001.0           15642.733  ...        0   \n",
       "2  22684.433333    610.891    1091.0           15965.033  ...        0   \n",
       "3  15067.466667    766.142    1173.0           10567.900  ...        0   \n",
       "4   9763.033333    999.628    1099.0             475.200  ...        0   \n",
       "\n",
       "   sprtransp  spring tomb labor precip income3  conformity  conformity_class  \\\n",
       "0          0       0    0     0    0.0  Middle       0.000                 0   \n",
       "1          0       0    0     0    0.1  Middle     713.439                 0   \n",
       "2          0       0    0     0    0.0  Middle     577.107                 0   \n",
       "3          0       0    0     0    0.1  Middle     610.891                 0   \n",
       "4          0       0    0     0    0.0  Middle     766.142                 0   \n",
       "\n",
       "   week  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3c9b8fa-997c-4de9-85a1-309d41ef17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_generation(data):\n",
    "\n",
    "    # encoding categories\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    ind_attr_cols = ['gender', 'age', 'income3']\n",
    "    for col in ind_attr_cols:\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "    data['income3'] = data['income3'].fillna(0)\n",
    "\n",
    "    # weekday one-hot encoding\n",
    "    one_hot_encoded = pd.get_dummies(data['weekday'], prefix='weekday')\n",
    "    data = data.drop('weekday', axis=1)\n",
    "    data= pd.concat([data, one_hot_encoded], axis=1)\n",
    "    # sample_data2020.head()\n",
    "\n",
    "    # use is_workday\n",
    "    # from chinese_calendar import is_workday\n",
    "    # sample_data2020['datestamp'] = pd.to_datetime(sample_data2020['date'].astype(str), format=\"%Y%m%d\")\n",
    "    # sample_data2020['workday'] = sample_data2020['datestamp'].apply(lambda x: int(is_workday(x)))\n",
    "    \n",
    "    # construct policy features\n",
    "    data['restrict'] = np.where(data['date'].between(20200124, 20200223), 1, 0)\n",
    "    data['open'] = np.where(data['date'] > 20200223, 1, 0)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f061c13a-c7e8-49fa-9058-695154678450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use data in 2019\n",
    "sample_data2019 = sample_data.query('date<20200101').reset_index(drop=True)\n",
    "sample_data2019 = features_generation(sample_data2019)\n",
    "# use data in 2020\n",
    "sample_data2020 = sample_data.query('date>=20200101').reset_index(drop=True)\n",
    "sample_data2020 = features_generation(sample_data2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56fea616-504f-4fc4-a600-29c868a27b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week\n",
      "5    4\n",
      "Name: conformity_class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 分组并计算每组中的唯一标签数\n",
    "unique_labels_per_week = sample_data2020.groupby('week')['conformity_class'].nunique()\n",
    "\n",
    "# 检查每周的标签数是否与所有可能的标签数相匹配\n",
    "all_labels_count = sample_data2020['conformity_class'].nunique()\n",
    "weeks_without_all_labels = unique_labels_per_week[unique_labels_per_week < all_labels_count]\n",
    "print(weeks_without_all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9123e3b8-8e18-412b-9128-5f222c72a26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "print(int(len(sample_data2020)/len(sample_data2020['pid'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29b1fa8c-184f-4a2e-a08d-807b5eff5149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid                       0\n",
      "date                      0\n",
      "move_frequency            0\n",
      "move_distance             0\n",
      "trip_time                 0\n",
      "home_time                 0\n",
      "norm_sum              29743\n",
      "norm_mean               192\n",
      "norm_max              29743\n",
      "norm_home_activity    91587\n",
      "norm_home_base        35706\n",
      "group_year                0\n",
      "voluntary                 0\n",
      "gender                    0\n",
      "age                       0\n",
      "income                    0\n",
      "datestamp                 0\n",
      "weekfactor                0\n",
      "timeline                  0\n",
      "Tran                      0\n",
      "E1                        0\n",
      "E2                        0\n",
      "E3                        0\n",
      "R1                        0\n",
      "R2                        0\n",
      "Low                       0\n",
      "Open                      0\n",
      "tp0                       0\n",
      "tp1                       0\n",
      "tp2                       0\n",
      "tp3                       0\n",
      "tp4                       0\n",
      "tp5                       0\n",
      "tp6                       0\n",
      "tp7                       0\n",
      "newyear                   0\n",
      "sprtransp                 0\n",
      "spring                    0\n",
      "tomb                      0\n",
      "labor                     0\n",
      "precip                    0\n",
      "income3                   0\n",
      "conformity                0\n",
      "conformity_class          0\n",
      "weekday_0                 0\n",
      "weekday_1                 0\n",
      "weekday_2                 0\n",
      "weekday_3                 0\n",
      "weekday_4                 0\n",
      "weekday_5                 0\n",
      "weekday_6                 0\n",
      "restrict                  0\n",
      "open                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sample_data2020.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f811d7-f61a-49d8-94c5-b0facd5cfe09",
   "metadata": {},
   "source": [
    "# data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e0efc-8d7a-433f-8a04-f0db05028856",
   "metadata": {},
   "source": [
    "covariates：gender, age, income, festivals(newyear, sprtransp, spring, tomb, labor), events(Tran), weekday  \n",
    "treatments: norm_mean, policy(如何编码？), voluntary(=case)  \n",
    "outcome: move_distance  \n",
    "Tran：人传人；E1：一级响应；E2：二级响应；E3：三级响应；R1：第一次复工；R2：第二次复工；Low：低风险；Open：公共空间开放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6433e60c-f019-4191-bb2d-0c502fb4f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset, num_covariates, num_treatments):\n",
    "\n",
    "        for covariate_id in range(num_covariates):\n",
    "            covariate_mean = np.mean(dataset['previous_covariates'][:, :, covariate_id])\n",
    "            covariate_std = np.std(dataset['previous_covariates'][:, :, covariate_id])\n",
    "            dataset['previous_covariates'][:, :, covariate_id] = (dataset['previous_covariates'][:, :,\n",
    "                                                      covariate_id] - covariate_mean) / covariate_std\n",
    "            # covariate_max = np.max(dataset['previous_covariates'][:, :, covariate_id])\n",
    "            # covariate_min = np.min(dataset['previous_covariates'][:, :, covariate_id])\n",
    "            # dataset['previous_covariates'][:, :, covariate_id] = (dataset['previous_covariates'][:, :,\n",
    "            #                                           covariate_id] - covariate_min) / (covariate_max - covariate_min)\n",
    "\n",
    "        for covariate_id in range(num_covariates):\n",
    "            covariate_mean = np.mean(dataset['covariates'][:, :, covariate_id])\n",
    "            covariate_std = np.std(dataset['covariates'][:, :, covariate_id])\n",
    "            dataset['covariates'][:, :, covariate_id] = (dataset['covariates'][:, :,\n",
    "                                                      covariate_id] - covariate_mean) / covariate_std\n",
    "            # covariate_max = np.max(dataset['covariates'][:, :, covariate_id])\n",
    "            # covariate_min = np.min(dataset['covariates'][:, :, covariate_id])\n",
    "            # dataset['covariates'][:, :, covariate_id] = (dataset['covariates'][:, :,\n",
    "            #                                           covariate_id] - covariate_min) / (covariate_max - covariate_min)\n",
    "        \n",
    "        for treatment_id in range(num_treatments):\n",
    "            treatment_mean = np.mean(dataset['previous_treatments'][:, :, treatment_id])\n",
    "            treatment_std = np.std(dataset['previous_treatments'][:, :, treatment_id])\n",
    "            dataset['previous_treatments'][:, :, treatment_id] = (dataset['previous_treatments'][:, :,\n",
    "                                                      treatment_id] - treatment_mean) / treatment_std\n",
    "            # use Min-Max scaling\n",
    "            # treatment_max = np.max(dataset['previous_treatments'][:, :, treatment_id])\n",
    "            # treatment_min = np.min(dataset['previous_treatments'][:, :, treatment_id])\n",
    "            # dataset['previous_treatments'][:, :, treatment_id] = (dataset['previous_treatments'][:, :,\n",
    "            #                                           treatment_id] - treatment_min) / (treatment_max - treatment_min)\n",
    "\n",
    "        for treatment_id in range(num_treatments):\n",
    "            treatment_mean = np.mean(dataset['treatments'][:, :, treatment_id])\n",
    "            treatment_std = np.std(dataset['treatments'][:, :, treatment_id])\n",
    "            dataset['treatments'][:, :, treatment_id] = (dataset['treatments'][:, :,\n",
    "                                                      treatment_id] - treatment_mean) / treatment_std\n",
    "            # use Min-Max scaling\n",
    "            # treatment_max = np.max(dataset['treatments'][:, :, treatment_id])\n",
    "            # treatment_min = np.min(dataset['treatments'][:, :, treatment_id])\n",
    "            # dataset['treatments'][:, :, treatment_id] = (dataset['treatments'][:, :,\n",
    "            #                                           treatment_id] - treatment_min) / (treatment_max - treatment_min)\n",
    "\n",
    "        outcome_mean = np.mean(dataset['outcomes'])\n",
    "        outcome_std= np.std(dataset['outcomes'])\n",
    "        dataset['outcomes'] = (dataset['outcomes'] - outcome_mean) /outcome_std\n",
    "\n",
    "        return dataset\n",
    "\n",
    "def generate_dataset(data, max_timesteps, covariate_cols, treatment_cols, outcome_col, log_transform):\n",
    "            dataset = dict()\n",
    "\n",
    "            dataset['previous_covariates'] = []\n",
    "            dataset['previous_treatments'] = []\n",
    "            dataset['covariates'] = []\n",
    "            dataset['treatments'] = []\n",
    "            dataset['sequence_length'] = []\n",
    "            dataset['outcomes'] = []\n",
    "            \n",
    "            timesteps = int(len(data)/len(data['pid'].unique()))\n",
    "            \n",
    "            for pid in data['pid'].unique():\n",
    "                single_data = data[data['pid']==pid].reset_index(drop=True)\n",
    "                covariates_history = single_data[covariate_cols].values\n",
    "                treatments_history = single_data[treatment_cols].values\n",
    "                \n",
    "                previous_covariates = np.vstack((np.array(covariates_history[1:timesteps - 1]), np.zeros(shape=(max_timesteps-timesteps, len(covariate_cols)))))\n",
    "                previous_treatments = np.vstack((np.array(treatments_history[1:timesteps - 1]),np.zeros(shape=(max_timesteps-timesteps, len(treatment_cols)))))\n",
    "                \n",
    "                covariates = np.vstack((np.array(covariates_history[1:timesteps]),np.zeros(shape=(max_timesteps-timesteps, len(covariate_cols)))))\n",
    "                treatments = np.vstack((np.array(treatments_history[1:timesteps]),np.zeros(shape=(max_timesteps-timesteps, len(treatment_cols)))))\n",
    "                \n",
    "                outcomes = single_data[outcome_col].values[1:timesteps]\n",
    "                if log_transform:\n",
    "                    outcomes = np.log(outcomes + 1)\n",
    "                outcomes = outcomes[:, np.newaxis]\n",
    "                outcomes = np.vstack((np.array(outcomes), np.zeros(shape=(max_timesteps-timesteps, 1))))\n",
    "                \n",
    "                dataset['previous_covariates'].append(np.array(previous_covariates))\n",
    "                dataset['previous_treatments'].append(np.array(previous_treatments))\n",
    "                dataset['covariates'].append(np.array(covariates))\n",
    "                dataset['treatments'].append(np.array(treatments))\n",
    "                dataset['sequence_length'].append(np.array(timesteps))\n",
    "                dataset['outcomes'].append(np.array(outcomes))\n",
    "            \n",
    "            for key in dataset.keys():\n",
    "                dataset[key] = np.array(dataset[key])\n",
    "\n",
    "            return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4877dbb3-858c-4ac6-8602-3302f86051e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mean_std(column):\n",
    "    if np.all((column == 0) | (column == 1)):\n",
    "        mean = 0\n",
    "        std = 1\n",
    "    else:\n",
    "        mean = np.mean(column)\n",
    "        std = np.std(column)\n",
    "    return mean, std\n",
    "\n",
    "def get_normalize_params(dataset, num_covariates, num_treatments):\n",
    "    scale_params = dict()\n",
    "\n",
    "    for key in ['previous_covariates', 'previous_treatments', 'covariates', 'treatments', 'outcomes']:\n",
    "        scale_params[key] = []\n",
    "\n",
    "    for covariate_id in range(num_covariates):\n",
    "\n",
    "        column = dataset['previous_covariates'][:, :, covariate_id]\n",
    "        # if feature is [0,1], then mean = 0, std = 1\n",
    "        pre_covariate_mean, pre_covariate_std = compute_mean_std(column)\n",
    "        scale_params['previous_covariates'].append(np.array([pre_covariate_mean, pre_covariate_std]))\n",
    "\n",
    "        column = dataset['covariates'][:, :, covariate_id]\n",
    "        covariate_mean, covariate_std = compute_mean_std(column)\n",
    "        scale_params['covariates'].append(np.array([covariate_mean, covariate_std]))\n",
    "\n",
    "    for treatment_id in range(num_treatments):\n",
    "        column = dataset['previous_treatments'][:, :, treatment_id]\n",
    "        pre_treatment_mean, pre_treatment_std = compute_mean_std(column)\n",
    "        scale_params['previous_treatments'].append(np.array([pre_treatment_mean, pre_treatment_std]))\n",
    "\n",
    "        column = dataset['treatments'][:, :, treatment_id]\n",
    "        treatment_mean, treatment_std = compute_mean_std(column)\n",
    "        scale_params['treatments'].append(np.array([treatment_mean, treatment_std]))\n",
    "\n",
    "    scale_params['outcomes'].append(np.array([np.mean(dataset['outcomes']), np.std(dataset['outcomes'])]))\n",
    "\n",
    "    for key in scale_params.keys():\n",
    "        scale_params[key] = np.array(scale_params[key])\n",
    "\n",
    "    return scale_params\n",
    "\n",
    "def get_dataset_normalize(dataset, scale_params, num_covariates, num_treatments):\n",
    "    for covariate_id in range(num_covariates):\n",
    "        dataset['previous_covariates'][:, :, covariate_id] = \\\n",
    "            (dataset['previous_covariates'][:, :, covariate_id] - scale_params['previous_covariates'][covariate_id, 0]) / \\\n",
    "            scale_params['previous_covariates'][covariate_id, 1]\n",
    "\n",
    "        dataset['covariates'][:, :, covariate_id] = \\\n",
    "            (dataset['covariates'][:, :, covariate_id] - scale_params['covariates'][covariate_id, 0]) / \\\n",
    "            scale_params['covariates'][covariate_id, 1]\n",
    "\n",
    "    for treatment_id in range(num_treatments):\n",
    "        dataset['previous_treatments'][:, :, treatment_id] = \\\n",
    "            (dataset['previous_treatments'][:, :, treatment_id] - scale_params['previous_treatments'][treatment_id, 0]) / \\\n",
    "            scale_params['previous_treatments'][treatment_id, 1]\n",
    "\n",
    "        dataset['treatments'][:, :, treatment_id] = \\\n",
    "            (dataset['treatments'][:, :, treatment_id] - scale_params['treatments'][treatment_id, 0]) / \\\n",
    "            scale_params['treatments'][treatment_id, 1]\n",
    "\n",
    "    dataset['outcomes'] = (dataset['outcomes'] - scale_params['outcomes'][0,0]) /scale_params['outcomes'][0,1]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d9d3a19-6605-44c6-bf40-6e9f4ba86eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps = int(len(sample_data2020)/len(sample_data2020['pid'].unique()))\n",
    "covariate_cols = ['gender','age','income3','weekday_0','weekday_1','weekday_2','weekday_3','weekday_4', 'weekday_5', 'weekday_6',\n",
    "                  'sprtransp','spring','precip', 'voluntary']\n",
    "treatment_cols = ['conformity_class','restrict','open']\n",
    "# treatment_cols = ['restrict','open']\n",
    "outcome_col = 'move_distance'\n",
    "dataset2020 = generate_dataset(sample_data2020, max_timesteps, covariate_cols, treatment_cols, outcome_col)\n",
    "#dataset2019 = generate_dataset(sample_data2019, max_timesteps, covariate_cols, treatment_cols, outcome_col)\n",
    "\n",
    "# merge data2020 and data2019\n",
    "# dataset = dict()\n",
    "# for key in dataset2020.keys():\n",
    "#     # if key == 'sequence_length':\n",
    "#     #     dataset[key] = dataset2019[key].append(dataset2020[key])\n",
    "#     # else:\n",
    "#     dataset[key] = np.concatenate((dataset2019[key], dataset2020[key]), axis=0)\n",
    "#norm_dataset = normalize_dataset(dataset, len(covariate_cols), len(treatment_cols)-2) # treatment仅正则化conformity和voluntary两项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16c5a0a-524b-4012-96f1-90144e090bfc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'previous_covariates': array([[[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ]],\n",
      "\n",
      "       [[ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. , 48.5,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. , 48.5,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ]]]), 'previous_treatments': array([[[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       [[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.]],\n",
      "\n",
      "       [[1., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]]]), 'covariates': array([[[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ]],\n",
      "\n",
      "       [[ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. , 48.5,  0. ],\n",
      "        [ 0. ,  0. ,  2. , ...,  0. ,  0.2,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. , 48.5,  0. ],\n",
      "        [ 1. ,  0. ,  0. , ...,  0. ,  0.2,  0. ]],\n",
      "\n",
      "       [[ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        ...,\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0. ,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. , 48.5,  0. ],\n",
      "        [ 1. ,  0. ,  1. , ...,  0. ,  0.2,  0. ]]]), 'treatments': array([[[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 1.]],\n",
      "\n",
      "       [[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.],\n",
      "        [1., 0., 1.]],\n",
      "\n",
      "       [[1., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.]]]), 'sequence_length': array([152, 152, 152, ..., 152, 152, 152]), 'outcomes': array([[[50564.        ],\n",
      "        [ 4147.        ],\n",
      "        [39519.        ],\n",
      "        ...,\n",
      "        [22903.        ],\n",
      "        [    0.        ],\n",
      "        [10516.1514314 ]],\n",
      "\n",
      "       [[ 1178.        ],\n",
      "        [ 2356.        ],\n",
      "        [ 1178.        ],\n",
      "        ...,\n",
      "        [ 7780.        ],\n",
      "        [    0.        ],\n",
      "        [    0.        ]],\n",
      "\n",
      "       [[    0.        ],\n",
      "        [ 4272.        ],\n",
      "        [    0.        ],\n",
      "        ...,\n",
      "        [ 1874.        ],\n",
      "        [    0.        ],\n",
      "        [ 1447.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[16299.52091821],\n",
      "        [16667.74203644],\n",
      "        [13985.07904666],\n",
      "        ...,\n",
      "        [    0.        ],\n",
      "        [    0.        ],\n",
      "        [    0.        ]],\n",
      "\n",
      "       [[60806.        ],\n",
      "        [33968.        ],\n",
      "        [47799.        ],\n",
      "        ...,\n",
      "        [33560.        ],\n",
      "        [28708.        ],\n",
      "        [ 2906.        ]],\n",
      "\n",
      "       [[    0.        ],\n",
      "        [12400.        ],\n",
      "        [50877.        ],\n",
      "        ...,\n",
      "        [    0.        ],\n",
      "        [    0.        ],\n",
      "        [    0.        ]]])}\n"
     ]
    }
   ],
   "source": [
    "print(dataset2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20ed43fe-b9dd-4551-93c7-473e4442ea0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(5000, 150, 14)\n",
      "previous_treatments\n",
      "(5000, 150, 3)\n",
      "covariates\n",
      "(5000, 151, 14)\n",
      "treatments\n",
      "(5000, 151, 3)\n",
      "sequence_length\n",
      "(5000,)\n",
      "outcomes\n",
      "(5000, 151, 1)\n"
     ]
    }
   ],
   "source": [
    "for key in list(dataset2020.keys()):\n",
    "    print(key)\n",
    "    print(dataset2020[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5e89e32a-3409-4567-8392-eb5912c1697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(20000, 150, 14)\n",
      "previous_treatments\n",
      "(20000, 150, 3)\n",
      "covariates\n",
      "(20000, 151, 14)\n",
      "treatments\n",
      "(20000, 151, 3)\n",
      "sequence_length\n",
      "(20000,)\n",
      "outcomes\n",
      "(20000, 151, 1)\n"
     ]
    }
   ],
   "source": [
    "for key in list(dataset.keys()):\n",
    "    print(key)\n",
    "    print(dataset[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "870ea93e-e5c8-4ad3-bfad-8b9f93802580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data\n",
    "import pickle\n",
    "\n",
    "def write_results_to_file(filename, data):\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=2)\n",
    "\n",
    "write_results_to_file('data/sample5000_for_predict_v2.txt', dataset2020)\n",
    "#write_results_to_file('data/real_data_sample1000.txt', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37fabf5e-738e-4ef5-9cf3-a4309833ae0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender           0\n",
      "age              0\n",
      "income3          0\n",
      "workday          0\n",
      "sprtransp        0\n",
      "spring           0\n",
      "precip           0\n",
      "voluntary        0\n",
      "conformity       0\n",
      "restrict         0\n",
      "open             0\n",
      "move_distance    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "features = sample_data2020[covariate_cols+treatment_cols+[outcome_col]]\n",
    "print(features.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df99dada-f7ac-4e13-b19e-14f00c2d2184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8010, 150, 1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iptw = np.load(\"results/rmsn_sample_10000_gamma_0.6_use_confounders_True/propensity_scores.npy\")\n",
    "iptw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b89744a-f8a7-4773-b71a-8c5727a24727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.27508382828776695\n"
     ]
    }
   ],
   "source": [
    "print(iptw.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fa93e45-0a88-4fd1-8bbb-1c4c1f49414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = norm_dataset['outcomes']\n",
    "sequence_lengths = norm_dataset['sequence_length']\n",
    "horizon = 1\n",
    "b_predict_actions = False\n",
    "active_entries = np.zeros(outputs.shape)\n",
    "for i in range(sequence_lengths.shape[0]):\n",
    "    sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "    if not b_predict_actions:\n",
    "        for k in range(horizon):\n",
    "            #include the censoring point too, but ignore future shifts that don't exist\n",
    "            active_entries[i, :sequence_length-k, k] = 1\n",
    "    else:\n",
    "        active_entries[i, :sequence_length, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "207b316d-3201-4959-b204-0b84fe9dc6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1510000.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(active_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b2ccdbe-4113-4461-b100-339bd4602cd1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'previous_covariates': array([[[ 0.4836432 ,  0.14829019, -0.27194955],\n",
      "        [ 0.21966952,  0.159632  , -0.29473256],\n",
      "        [ 0.31210715,  0.23716383, -0.18455728],\n",
      "        ...,\n",
      "        [-0.01889956, -0.01106907,  0.01809696],\n",
      "        [-0.02130581,  0.00441948,  0.01511471],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.10181909, -0.16746599,  0.65813375],\n",
      "        [-0.27828921, -0.08667827,  0.841197  ],\n",
      "        [-0.37494351,  0.03567515,  0.61385476],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.34463483, -0.25486716, -0.46962224],\n",
      "        [ 0.10949454, -0.14256839, -0.42181919],\n",
      "        [ 0.17931144, -0.05347157, -0.10505994],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.28026265,  0.50328932,  0.47389911],\n",
      "        [-0.22226105,  0.38199745,  0.51104897],\n",
      "        [-0.28832491,  0.3523136 ,  0.2850223 ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.17419137,  0.04186875,  0.17190079],\n",
      "        [ 0.13587907,  0.02717503,  0.19338748],\n",
      "        [ 0.1911088 ,  0.02102746,  0.11307143],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.04927679,  0.18928482,  0.22837258],\n",
      "        [-0.125041  ,  0.15645134,  0.38648094],\n",
      "        [-0.17634608,  0.24811516,  0.349121  ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]]]), 'previous_treatments': array([[[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]]), 'covariates': array([[[ 0.4836432 ,  0.14829019, -0.27194955],\n",
      "        [ 0.21966952,  0.159632  , -0.29473256],\n",
      "        [ 0.31210715,  0.23716383, -0.18455728],\n",
      "        ...,\n",
      "        [-0.02130581,  0.00441948,  0.01511471],\n",
      "        [-0.00452109, -0.00587509,  0.02528537],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.10181909, -0.16746599,  0.65813375],\n",
      "        [-0.27828921, -0.08667827,  0.841197  ],\n",
      "        [-0.37494351,  0.03567515,  0.61385476],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.34463483, -0.25486716, -0.46962224],\n",
      "        [ 0.10949454, -0.14256839, -0.42181919],\n",
      "        [ 0.17931144, -0.05347157, -0.10505994],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.28026265,  0.50328932,  0.47389911],\n",
      "        [-0.22226105,  0.38199745,  0.51104897],\n",
      "        [-0.28832491,  0.3523136 ,  0.2850223 ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.17419137,  0.04186875,  0.17190079],\n",
      "        [ 0.13587907,  0.02717503,  0.19338748],\n",
      "        [ 0.1911088 ,  0.02102746,  0.11307143],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.04927679,  0.18928482,  0.22837258],\n",
      "        [-0.125041  ,  0.15645134,  0.38648094],\n",
      "        [-0.17634608,  0.24811516,  0.349121  ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ],\n",
      "        [ 0.        ,  0.        ,  0.        ]]]), 'confounders': array([[[-0.05681389],\n",
      "        [-0.18431217],\n",
      "        [-0.02872134],\n",
      "        ...,\n",
      "        [-0.03037612],\n",
      "        [-0.00103766],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[ 0.24198298],\n",
      "        [ 0.05412057],\n",
      "        [ 0.19738193],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[ 0.26550339],\n",
      "        [ 0.04842322],\n",
      "        [ 0.19908305],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.29878451],\n",
      "        [-0.30744606],\n",
      "        [-0.38675874],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[-0.15902802],\n",
      "        [-0.14515051],\n",
      "        [-0.19600572],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[ 0.03853501],\n",
      "        [-0.14050488],\n",
      "        [-0.03816072],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]]]), 'treatments': array([[[1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]]), 'sequence_length': array([30, 25, 28, ..., 25, 26, 28]), 'outcomes': array([[[-0.09931144],\n",
      "        [ 0.03139569],\n",
      "        [ 0.00190339],\n",
      "        ...,\n",
      "        [ 0.00136263],\n",
      "        [-0.00495462],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[ 0.09596961],\n",
      "        [ 0.15504068],\n",
      "        [-0.14614256],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[-0.03159848],\n",
      "        [ 0.12222049],\n",
      "        [-0.20725152],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-0.09502959],\n",
      "        [-0.18552044],\n",
      "        [ 0.07479009],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[-0.03956476],\n",
      "        [-0.07424241],\n",
      "        [ 0.02870581],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[-0.02858409],\n",
      "        [ 0.03322225],\n",
      "        [-0.12932372],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]]])}\n"
     ]
    }
   ],
   "source": [
    "def load_data_from_file(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        return pickle.load(handle)\n",
    "\n",
    "# 使用函数加载数据\n",
    "filename = \"results/simulated_data/_dataset_with_substitute_confounders.txt\"  # 更改为您的文件名\n",
    "simulated_data = load_data_from_file(filename)\n",
    "print(simulated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "32f47bd7-e9b5-4c7b-83e7-0eef9b1de8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(5000, 29, 3)\n",
      "previous_treatments\n",
      "(5000, 29, 3)\n",
      "covariates\n",
      "(5000, 30, 3)\n",
      "confounders\n",
      "(5000, 30, 1)\n",
      "treatments\n",
      "(5000, 30, 3)\n",
      "sequence_length\n",
      "(5000,)\n",
      "outcomes\n",
      "(5000, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "for key in list(simulated_data.keys()):\n",
    "    print(key)\n",
    "    print(simulated_data[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf0d92c4-17bd-4019-a0ff-adb3a62d9aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['outcomes'][1,:,:]))\n",
    "print(dataset['sequence_length'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c2b1e1-6778-4ae6-802b-6f9d4fdf9b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
