{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25376294-cae6-4b0e-be47-b7fc29cacc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_process import get_dataset_splits\n",
    "from utils.evaluation_utils import load_data_from_file, write_results_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c6bdaf-5126-4b02-b964-fbdac392c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 13:12:43.670484: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 13:12:43.714968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 13:12:44.497955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9dfb6dd-b0a5-49d2-994b-a5a1e0880065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1508f-ddeb-47b3-8af9-c08f36e8e6d6",
   "metadata": {},
   "source": [
    "将所有代码转化为单线程，而非调用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afe277-4d5c-4645-a1ca-502132501860",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f597cc7-5a43-41a9-80d6-a7c00c5943a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(3000, 160, 25)\n",
      "float32\n",
      "previous_treatments\n",
      "(3000, 160, 3)\n",
      "float32\n",
      "covariates\n",
      "(3000, 161, 25)\n",
      "float32\n",
      "treatments\n",
      "(3000, 161, 3)\n",
      "float32\n",
      "sequence_length\n",
      "(3000,)\n",
      "int64\n",
      "outcomes\n",
      "(3000, 161, 1)\n",
      "float32\n",
      "predicted_confounders\n",
      "(3000, 161, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_from_file(\"results/tf2_test_dataset_with_substitute_confounders.txt\")\n",
    "# 数据类型转换\n",
    "for key in dataset.keys():\n",
    "    if key!='sequence_length':\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "    print(key)\n",
    "    print(dataset[key].shape)\n",
    "    print(dataset[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65195d8c-aea7-4c80-b79b-b2449734b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.1, random_state=10)\n",
    "train_index, test_index = next(shuffle_split.split(dataset['covariates'][:, :, 0]))\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.11, random_state=10)\n",
    "train_index, val_index = next(shuffle_split.split(dataset['covariates'][train_index, :, 0]))\n",
    "dataset_map = get_dataset_splits(dataset, train_index, val_index, test_index, use_predicted_confounders=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b510c-4248-4d62-9b04-2a415e3cc6bb",
   "metadata": {},
   "source": [
    "# RMSN port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f203b4-7487-4219-b300-ddb1da2642fd",
   "metadata": {},
   "source": [
    "train_rmsn → rnn_fit → train (RNNModel-->training:get_training_graph-->validation:get_prediction_graph) → model_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8d133-a8e8-469b-8860-b399d5c4eb0e",
   "metadata": {},
   "source": [
    "## train_rmsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb08e94e-ff18-4c87-83c5-4895c9b80125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_rmsn(dataset_map, model_name, b_use_predicted_confounders):###########################\n",
    "# model_name = 'tf2_try'\n",
    "# MODEL_ROOT = os.path.join('results', model_name)\n",
    "MODEL_ROOT = 'results/rmsn_tf2_test_use_confounders_True'\n",
    "\n",
    "# if not os.path.exists(MODEL_ROOT):\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "# else:\n",
    "#     # Need to delete previously saved model.\n",
    "#     shutil.rmtree(MODEL_ROOT)\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "\n",
    "# rnn_fit参数设置\n",
    "networks_to_train='propensity_networks'\n",
    "# networks_to_train='encoder'\n",
    "b_use_predicted_confounders=True\n",
    "\n",
    "    # rnn_fit(dataset_map=dataset_map, networks_to_train='propensity_networks', MODEL_ROOT=MODEL_ROOT,\n",
    "    #         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     propensity_generation(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                           b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rnn_fit(networks_to_train='encoder', dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#             b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmsn_mse = rnn_test(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmse = np.sqrt(np.mean(rmsn_mse)) * 100\n",
    "    # return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9a319-a931-4bb4-8e50-abf614034137",
   "metadata": {},
   "source": [
    "## rnn_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529f430-9c63-40a1-9216-09d7e2e40b44",
   "metadata": {},
   "source": [
    "### 1.1 基础参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a47dc0bb-17c8-45e5-a95e-0427a826e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "specifications = {\n",
    "     'rnn_propensity_weighted': (0.1, 4, 100, 64, 0.01, 1.0),\n",
    "     'treatment_rnn_action_inputs_only': (0.1, 3, 100, 128, 0.01, 2.0),\n",
    "     'treatment_rnn': (0.1, 4, 100, 64, 0.01, 1.0),}\n",
    "# #####################################################################################\n",
    "# def rnn_fit(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders,\n",
    "#             b_use_oracle_confounders=False, b_remove_x1=False):\n",
    "    \n",
    "# Get the correct networks to train\n",
    "if networks_to_train == \"propensity_networks\":\n",
    "    logging.info(\"Training propensity networks\")\n",
    "    net_names = ['treatment_rnn_action_inputs_only']\n",
    "    # net_names = ['treatment_rnn']\n",
    "\n",
    "elif networks_to_train == \"encoder\":\n",
    "    logging.info(\"Training R-MSN encoder\")\n",
    "    net_names = [\"rnn_propensity_weighted\"]\n",
    "\n",
    "elif networks_to_train == \"user_defined\":\n",
    "    logging.info(\"Training user defined network\")\n",
    "    raise NotImplementedError(\"Specify network to use!\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognised network type\")\n",
    "\n",
    "    logging.info(\"Running hyperparameter optimisation\")\n",
    "\n",
    "# Experiment name\n",
    "expt_name = \"treatment_effects\"\n",
    "\n",
    "# Possible networks to use along with their activation functions\n",
    "# change hidden layer of rnn_propensity_weighted to tanh\n",
    "activation_map = {'rnn_propensity_weighted': (\"tanh\", 'linear'),\n",
    "                  'rnn_propensity_weighted_logistic': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid')\n",
    "                  }\n",
    "\n",
    "    \n",
    "# Start Running hyperparam opt\n",
    "opt_params = {}\n",
    "# for net_name in net_names:\n",
    "net_name = net_names[0]\n",
    "# Re-run hyperparameter optimisation if parameters are not specified, otherwise train with defined params（如果需要超参优化则跑3次，否则跑一次就行）\n",
    "max_hyperparam_runs = 3 if net_name not in specifications else 1\n",
    "\n",
    "# Pull datasets\n",
    "b_predict_actions = \"treatment_rnn\" in net_name\n",
    "use_truncated_bptt = net_name != \"rnn_model_bptt\" # whether to train with truncated backpropagation through time\n",
    "b_propensity_weight = \"rnn_propensity_weighted\" in net_name\n",
    "b_use_actions_only = \"rnn_action_inputs_only\" in net_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ca44-6b19-4bc0-9691-44d81b5d99b7",
   "metadata": {},
   "source": [
    "### 1.2 gpu设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d30138a-0d8a-4aab-b965-746f99e26960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with memory growth\n"
     ]
    }
   ],
   "source": [
    "# Setup tensorflow\n",
    "# 检测 GPU 设备\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # set TensorFlow to use all GPU\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        for gpu in gpus:\n",
    "            # set GPU memery growth\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Using GPU with memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        # Changing device settings after the program is running may cause errors\n",
    "        print(e)\n",
    "else:\n",
    "    # if no GPU，using CPU\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b163d5-7cac-4811-9a7b-522085a2de8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992e187-499a-4b46-9953-f70361e681d9",
   "metadata": {},
   "source": [
    "### 1.3 模型参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a9e2cbb-0c0a-409f-a3c6-fdda857fd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start hyperparamter optimisation\n",
    "hyperparam_count = 0\n",
    "# 删掉超参数优化（随机选取超参数）部分\n",
    "spec = specifications[net_name]\n",
    "logging.info(\"Using specifications for {}: {}\".format(net_name, spec))\n",
    "dropout_rate = spec[0]\n",
    "memory_multiplier = spec[1]\n",
    "num_epochs = spec[2]\n",
    "minibatch_size = spec[3]\n",
    "learning_rate = spec[4]\n",
    "max_norm = spec[5]\n",
    "hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "            \n",
    "# hyperparam_opt = train(net_name, expt_name,\n",
    "#                       training_processed, validation_processed, test_processed,\n",
    "#                       dropout_rate, memory_multiplier, num_epochs,\n",
    "#                       minibatch_size, learning_rate, max_norm,\n",
    "#                       use_truncated_bptt,\n",
    "#                       num_features, num_outputs, model_folder,\n",
    "#                       hidden_activation, output_activation,\n",
    "#                       config,\n",
    "#                       \"hyperparam opt: {} of {}\".format(hyperparam_count,\n",
    "#                                                         max_hyperparam_runs))\n",
    "\n",
    "#     hyperparam_count = len(hyperparam_opt.columns)\n",
    "#     if hyperparam_count >= max_hyperparam_runs:\n",
    "#         opt_params[net_name] = hyperparam_opt.T\n",
    "#         break\n",
    "\n",
    "# logging.info(\"Done\")\n",
    "# logging.info(hyperparam_opt.T)\n",
    "\n",
    "# # Flag optimal params\n",
    "# logging.info(opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ec6e-32f9-4274-9932-c2b37e0d1e71",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd38d16-3735-489d-ae89-261de81a91a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_processed_data(raw_sim_data,\n",
    "                       b_predict_actions,\n",
    "                       b_use_actions_only,\n",
    "                       b_use_predicted_confounders,\n",
    "                       b_use_oracle_confounders,\n",
    "                       b_remove_x1,\n",
    "                       keep_first_point=False):\n",
    "    \"\"\"\n",
    "    Create formatted data to train both propensity networks and seq2seq architecture\n",
    "\n",
    "    :param raw_sim_data: Data from simulation\n",
    "    :param scaling_params: means/standard deviations to normalise the data to\n",
    "    :param b_predict_actions: flag to package data for propensity network to forecast actions\n",
    "    :param b_use_actions_only:  flag to package data with only action inputs and not covariates\n",
    "    :param b_predict_censoring: flag to package data to predict censoring locations\n",
    "    :return: processed data to train specific network\n",
    "    \"\"\"\n",
    "    horizon = 1\n",
    "    offset = 1\n",
    "\n",
    "    # Change Continuous values 2 Multiclass values\n",
    "    treatments = raw_sim_data['treatments']\n",
    "    continuous_indices = [index for index in range(treatments.shape[2]) if not np.all(np.isin(treatments[:, :, index], [0, 1]))]\n",
    "    bins = 20\n",
    "    for index in continuous_indices:\n",
    "        # 对每个连续值列进行分箱编码\n",
    "        treatments[:, :, index] = np.digitize(treatments[:, :, index], \n",
    "                                                    bins=np.linspace(treatments[:, :, index].min(), treatments[:, :, index].max(), bins+1)[1:-1], \n",
    "                                                    right=True)\n",
    "    \n",
    "    # Binary application\n",
    "    \n",
    "    covariates = raw_sim_data['covariates']\n",
    "    dataset_outputs = raw_sim_data['outcomes']\n",
    "    sequence_lengths = raw_sim_data['sequence_length']\n",
    "    \n",
    "    if b_use_predicted_confounders:\n",
    "        predicted_confounders = raw_sim_data['predicted_confounders']\n",
    "\n",
    "    if b_use_oracle_confounders:\n",
    "        predicted_confounders = raw_sim_data['confounders']\n",
    "\n",
    "    num_treatments = treatments.shape[-1]\n",
    "\n",
    "    # Parcelling INPUTS\n",
    "    if b_predict_actions:\n",
    "        if b_use_actions_only:\n",
    "            inputs = treatments\n",
    "            inputs = inputs[:, :-offset, :]\n",
    "\n",
    "            actions = inputs.copy()\n",
    "\n",
    "        else:\n",
    "            # Uses current covariate, to remove confounding effects between action and current value\n",
    "            if (b_use_predicted_confounders):\n",
    "                print (\"Using predicted confounders\")\n",
    "                inputs = np.concatenate([covariates[:, 1:, ], predicted_confounders[:, 1:, ], treatments[:, :-1, ]],\n",
    "                                        axis=2)\n",
    "            else:\n",
    "                inputs = np.concatenate([covariates[:, 1:,], treatments[:, :-1, ]], axis=2)\n",
    "\n",
    "            actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    else:\n",
    "        if (b_use_predicted_confounders):\n",
    "            inputs = np.concatenate([covariates, predicted_confounders, treatments], axis=2)\n",
    "        else:\n",
    "            inputs = np.concatenate([covariates, treatments], axis=2)\n",
    "        \n",
    "        if not keep_first_point:\n",
    "            inputs = inputs[:, 1:, :]\n",
    "\n",
    "        actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    # Parcelling OUTPUTS\n",
    "    if b_predict_actions:\n",
    "        outputs = treatments\n",
    "        outputs = outputs[:, 1:, :]\n",
    "\n",
    "    else:\n",
    "        if keep_first_point:\n",
    "            outputs = dataset_outputs\n",
    "        else:\n",
    "            outputs = dataset_outputs[:, 1:, :]\n",
    "\n",
    "\n",
    "    # Set array alignment\n",
    "    sequence_lengths = np.array([i - 1 for i in sequence_lengths]) # everything shortens by 1\n",
    "\n",
    "    # Remove any trajectories that are too short\n",
    "    inputs = inputs[sequence_lengths > 0, :, :]\n",
    "    outputs = outputs[sequence_lengths > 0, :, :]\n",
    "    sequence_lengths = sequence_lengths[sequence_lengths > 0]\n",
    "    actions = actions[sequence_lengths > 0, :, :]\n",
    "\n",
    "    # Add active entires\n",
    "    active_entries = np.zeros(outputs.shape, dtype=np.float32)\n",
    "\n",
    "    for i in range(sequence_lengths.shape[0]):\n",
    "        sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "        if not b_predict_actions:\n",
    "            for k in range(horizon):\n",
    "                #include the censoring point too, but ignore future shifts that don't exist\n",
    "                active_entries[i, :sequence_length-k, k] = 1\n",
    "        else:\n",
    "            active_entries[i, :sequence_length, :] = 1\n",
    "\n",
    "    return {'outputs': outputs,  # already scaled\n",
    "            'scaled_inputs': inputs,\n",
    "            'scaled_outputs': outputs,\n",
    "            'actions': actions,\n",
    "            'sequence_lengths': sequence_lengths,\n",
    "            'active_entries': active_entries\n",
    "            }\n",
    "\n",
    "\n",
    "def convert_to_tf_dataset(dataset_map, minibatch_size):\n",
    "    key_map = {'inputs': dataset_map['scaled_inputs'],\n",
    "               'outputs': dataset_map['scaled_outputs'],\n",
    "               'active_entries': dataset_map['active_entries'],\n",
    "               'sequence_lengths': dataset_map['sequence_lengths']}\n",
    "\n",
    "    if 'propensity_weights' in dataset_map:\n",
    "        key_map['propensity_weights'] = dataset_map['propensity_weights']\n",
    "\n",
    "    if 'initial_states' in dataset_map:\n",
    "        key_map['initial_states'] = dataset_map['initial_states']\n",
    "\n",
    "    #from_tensor_slices:切片; shuffle:随机打乱; batch:批次组合; prefetch:提前准备（预取）数据\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(key_map)\\\n",
    "                .shuffle(buffer_size = 1000).batch(minibatch_size) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8354ab9-1da0-44bd-8f3d-e5781b698bb3",
   "metadata": {},
   "source": [
    "### 1.4 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e895cc5-0d22-4d39-bb94-da72f4631cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset_map['training_data']\n",
    "validation_data = dataset_map['validation_data']\n",
    "test_data = dataset_map['test_data']\n",
    "\n",
    " # Extract only relevant trajs and shift data\n",
    "b_use_oracle_confounders = False; b_remove_x1 = False\n",
    "training_processed = get_processed_data(training_data, b_predict_actions,\n",
    "                                             b_use_actions_only, b_use_predicted_confounders,\n",
    "                                             b_use_oracle_confounders, b_remove_x1)\n",
    "validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                               b_use_actions_only, b_use_predicted_confounders,\n",
    "                                               b_use_oracle_confounders, b_remove_x1)\n",
    "test_processed = get_processed_data(test_data, b_predict_actions,\n",
    "                                         b_use_actions_only, b_use_predicted_confounders,\n",
    "                                         b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "num_features = training_processed['scaled_inputs'].shape[-1]\n",
    "num_outputs = training_processed['scaled_outputs'].shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e986ae9d-9e7c-4aaf-8a21-36e28d733ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load propensity weights if they exist\n",
    "if b_propensity_weight:\n",
    "    if net_name == 'rnn_propensity_weighted_den_only':\n",
    "        # use un-stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_den_only.npy\"))\n",
    "    elif net_name == \"rnn_propensity_weighted_logistic\":\n",
    "        # Use logistic regression weights\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "        tmp = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_logistic.npy\"))\n",
    "        propensity_weights = tmp[:propensity_weights.shape[0], :, :]\n",
    "    else:\n",
    "        # use stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "\n",
    "    logging.info(\"Net name = {}. Mean-adjusting!\".format(net_name))\n",
    "\n",
    "    propensity_weights /= propensity_weights.mean()\n",
    "\n",
    "    training_processed['propensity_weights'] = np.array(propensity_weights, dtype='float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffbddf8b-03ef-4bff-b872-e771df271067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensorflow format\n",
    "tf_data_train = convert_to_tf_dataset(training_processed, minibatch_size)\n",
    "tf_data_valid = convert_to_tf_dataset(validation_processed, minibatch_size)\n",
    "tf_data_test = convert_to_tf_dataset(test_processed, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5226159-cc7b-49e6-979a-cf5edc0cbc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'inputs': TensorSpec(shape=(None, 160, 3), dtype=tf.float32, name=None), 'outputs': TensorSpec(shape=(None, 160, 3), dtype=tf.float32, name=None), 'active_entries': TensorSpec(shape=(None, 160, 3), dtype=tf.float32, name=None), 'sequence_lengths': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc09d-c919-4a2b-b5ff-96fd4fc8a4fd",
   "metadata": {},
   "source": [
    "## core routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c7346-384d-4d49-982f-f378b06198da",
   "metadata": {},
   "source": [
    "### def_train修改\n",
    "1. 去除session会话\n",
    "2. 各步骤拆分与重构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af5357-82fc-4ff8-ad1c-c754c2b57efd",
   "metadata": {},
   "source": [
    "#### 2.1 参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ec4a1-be8c-484a-aa71-7a8ba11660c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(net_name,\n",
    "#           expt_name,\n",
    "#           training_dataset, validation_dataset, test_dataset,\n",
    "#           dropout_rate,\n",
    "#           memory_multiplier,\n",
    "#           num_epochs,\n",
    "#           minibatch_size,\n",
    "#           learning_rate,\n",
    "#           max_norm,\n",
    "#           use_truncated_bptt,\n",
    "#           num_features,\n",
    "#           num_outputs,\n",
    "#           model_folder,\n",
    "#           hidden_activation,\n",
    "#           output_activation,\n",
    "#           tf_config,\n",
    "#           additonal_info=\"\",\n",
    "#           b_use_state_initialisation=False,\n",
    "#           b_use_seq2seq_feedback=False,\n",
    "#           b_use_seq2seq_training_mode=False,\n",
    "#           adapter_multiplier=0,\n",
    "#           b_use_memory_adapter=False,\n",
    "#           verbose=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0ab33a-749c-45e7-af22-ffaeac0425de",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs = 1\n",
    "# hidden_layer_size = int(memory_multiplier * num_features)\n",
    "hidden_layer_size = 128\n",
    "\n",
    "b_use_state_initialisation = False\n",
    "if b_use_state_initialisation:\n",
    "    full_state_size = int(training_dataset['initial_states'].shape[-1])\n",
    "    adapter_size = adapter_multiplier * full_state_size\n",
    "else:\n",
    "    adapter_size = 0\n",
    "    \n",
    "model_parameters = {'net_name': net_name,\n",
    "                    'experiment_name': expt_name,\n",
    "                    'training_dataset': tf_data_train,\n",
    "                    'validation_dataset': tf_data_valid,\n",
    "                    'test_dataset': tf_data_test,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'input_size': num_features,\n",
    "                    'output_size': num_outputs,\n",
    "                    'hidden_layer_size': hidden_layer_size,\n",
    "                    'num_epochs': 10, # for test\n",
    "                    'minibatch_size': minibatch_size,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_norm': max_norm,\n",
    "                    'model_folder': model_folder,\n",
    "                    'hidden_activation': hidden_activation,\n",
    "                    'output_activation': output_activation,\n",
    "                    'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "                    'softmax_size': 1, #not used in this paper, but allows for categorical actions\n",
    "                    'mc_size': 0,\n",
    "                    'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72be6-75cb-4797-bd27-a556bbb21222",
   "metadata": {},
   "source": [
    "#### 2.2 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8994abb-e797-4f8e-ac96-d75e9a5a67d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \n",
    "    # Data params\n",
    "    #training_data = None if 'training_dataset' not in params else params['training_dataset']\n",
    "    #validation_data = None if 'validation_dataset' not in params else params['validation_dataset']\n",
    "    #test_data = None if 'test_dataset' not in params else params['test_dataset']\n",
    "    input_size = params['input_size']\n",
    "    output_size = params['output_size']\n",
    "\n",
    "    # Network params\n",
    "    net_name = params['net_name']\n",
    "    softmax_size = params['softmax_size']\n",
    "    bins = 20\n",
    "    predict_size = output_size - softmax_size + bins\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    hidden_layer_size = params['hidden_layer_size']\n",
    "    memory_activation_type = params['hidden_activation']\n",
    "    output_activation_type = params['output_activation']\n",
    "    #initial_states = None\n",
    "    # input layer\n",
    "    inputs = layers.Input(shape=(None,input_size), dtype=tf.float32)\n",
    "    # define initial states \n",
    "    initial_h =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_h')\n",
    "    initial_c =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_c')\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm, state_h, state_c = layers.LSTM(hidden_layer_size, activation=memory_activation_type, \n",
    "                       return_sequences=True, return_state=True, dropout=dropout_rate)(inputs, initial_state=[initial_h, initial_c])\n",
    "\n",
    "    # flattened_lstm = layers.Flatten()(lstm)\n",
    "\n",
    "    # linear output layer\n",
    "    logits = layers.Dense(predict_size)(lstm)\n",
    "\n",
    "    # Softmax（对应多分类任务）\n",
    "    if softmax_size != 0:\n",
    "        logits_reshaped = layers.Reshape((-1, predict_size))(logits)\n",
    "        softmax_outputs, core_outputs = tf.split(logits_reshaped, [bins, predict_size - bins], axis=-1)\n",
    "        core_activated = layers.Activation(output_activation_type)(core_outputs)\n",
    "        softmax_activated = layers.Softmax(axis=-1)(softmax_outputs)\n",
    "        outputs = layers.Concatenate(axis=-1)([softmax_activated, core_activated])\n",
    "    \n",
    "    # MC dropout（对应连续任务）\n",
    "    elif mc_size != 0:\n",
    "        logits_reshaped = layers.Reshape((-1, output_size))(logits)\n",
    "        mc_outputs, core_outputs = tf.split(logits_reshaped, [mc_size, output_size - mc_size], axis=-1)\n",
    "        core_activated = layers.Activation(output_activation_type)(core_outputs)\n",
    "        mc_activated = layers.Dense(1)(mc_outputs)\n",
    "        mc_activated = layers.Dropout(dropout_rate)(mc_activated, training=True) # 启用dropout，即使在推断时\n",
    "        outputs = layers.Concatenate(axis=-1)([mc_activated, core_activated])\n",
    "    \n",
    "    else:\n",
    "        outputs = layers.Activation(output_activation_type)(logits)\n",
    "\n",
    "    # construct model\n",
    "    model = models.Model(inputs=[inputs, initial_h, initial_c], outputs=[outputs, state_h, state_c], name=net_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f49d3e37-ebdf-44b6-8913-a06edcc81512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"treatment_rnn_action_inputs_only\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 3)]            0         []                            \n",
      "                                                                                                  \n",
      " initial_h (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " initial_c (InputLayer)      [(None, 128)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, None, 128),          67584     ['input_1[0][0]',             \n",
      "                              (None, 128),                           'initial_h[0][0]',           \n",
      "                              (None, 128)]                           'initial_c[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 22)             2838      ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, None, 22)             0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.split (TFOpLambda)       [(None, None, 20),           0         ['reshape[0][0]']             \n",
      "                              (None, None, 2)]                                                    \n",
      "                                                                                                  \n",
      " softmax (Softmax)           (None, None, 20)             0         ['tf.split[0][0]']            \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None, 2)              0         ['tf.split[0][1]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, None, 22)             0         ['softmax[0][0]',             \n",
      "                                                                     'activation[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 70422 (275.09 KB)\n",
      "Trainable params: 70422 (275.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model(model_parameters)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715d6f0-ef02-461b-aa11-534df9297b19",
   "metadata": {},
   "source": [
    "### 2.3 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118e472-7b2b-499e-a26f-36e12ab81c60",
   "metadata": {},
   "source": [
    "#### 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4705a289-2e81-4828-8f9c-932f701fcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(losses.Loss):\n",
    "    def __init__(self, performance_metric, name=\"custom_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.performance_metric = performance_metric\n",
    "        # self.weights = params['weights']\n",
    "        # self.active_entries = params['active_entries']\n",
    "\n",
    "    def train_call(self, y_true, y_pred, active_entries, weights):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"sparse_xentropy\":\n",
    "            scce = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            loss = scce(y_true, y_pred, sample_weight=active_entries * weights)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def valid_call(self, y_true, y_pred, active_entries):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries ) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries) / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"sparse_xentropy\":\n",
    "            scce = losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "            loss = scce(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"performance_metric\": self.performance_metric})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a36a1-7f02-4dbb-aed7-11240388c98f",
   "metadata": {},
   "source": [
    "#### 训练步骤和验证步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f283750-1b64-426d-bc8f-ab1d25f5d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=model_parameters['learning_rate'])\n",
    "# continuous_loss_func = CustomLoss(performance_metric='mse')\n",
    "multiclass_loss_func = CustomLoss(performance_metric='sparse_xentropy')\n",
    "binary_loss_func = CustomLoss(performance_metric='xentropy')\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "\n",
    "mc_size = 1\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs, outputs, active_entries, weights):\n",
    "    if weights is None:\n",
    "        weights = tf.constant(1.0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        continuous_outputs, binary_outputs = tf.split(outputs, [mc_size, output_size - mc_size], axis=-1)\n",
    "        predictions = model(inputs, training=True)\n",
    "        continuous_preds, binary_preds = tf.split(predictions, [bins, predict_size - bins], axis=-1)\n",
    "        \n",
    "        continuous_loss = continuous_loss_func.train_call(continuous_outputs, continuous_predictions, active_entries, weights)\n",
    "        binary_loss = binary_loss_func.train_call(binary_outputs, binary_predictions, active_entries, weights)\n",
    "        total_loss = continuous_loss + binary_loss\n",
    "        \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(total_loss)\n",
    "    train_metric.update_state(continuous_outputs, continuous_preds)\n",
    "    \n",
    "@tf.function\n",
    "def valid_step(model, inputs, outputs, active_entries):\n",
    "    continuous_outputs, binary_outputs = tf.split(outputs, [mc_size, output_size - mc_size], axis=-1)    \n",
    "    predictions = model(inputs, training=False)\n",
    "    continuous_preds, binary_preds = tf.split(predictions, [bins, predict_size - bins], axis=-1)\n",
    "    \n",
    "    continuous_loss = continuous_loss_func.valid_call(continuous_outputs, continuous_predictions, active_entries)\n",
    "    binary_loss = binary_loss_func.valid_call(binary_outputs, binary_predictions, active_entries)\n",
    "    total_loss = continuous_loss + binary_loss\n",
    "    valid_loss.update_state(total_loss)\n",
    "    valid_metric.update_state(continuous_outputs, continuous_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed826629-9926-4112-b4a2-b3ea259f9be5",
   "metadata": {},
   "source": [
    "#### 训练模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b61247a1-98fd-4fd4-8f87-24f998f82176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    # optimisation_summary = pd.Series([])\n",
    "    for epoch in tf.range(1, epochs+1):\n",
    "        \n",
    "        for data in ds_train:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "        for data in ds_valid:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "        # 同样的日志和状态重置操作\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "192bf221-bc75-441a-89e0-e74cde192c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModule(tf.Module):\n",
    "    def __init__(self, params, name=None):\n",
    "        super(TrainModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            # self.epochs = params['num_epochs']\n",
    "            self.epochs = 10\n",
    "            self.bins = 20\n",
    "            self.ds_train = params['training_dataset']\n",
    "            self.ds_valid = params['validation_dataset']\n",
    "            self.ds_test = params['test_dataset']\n",
    "            self.optimizer = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "            self.output_size = params['output_size']\n",
    "            self.softmax_size = params['softmax_size']\n",
    "            self.predict_size = params['output_size'] - params['softmax_size'] + self.bins\n",
    "            self.hidden_layer_size = params['hidden_layer_size']\n",
    "            # self.loss_func = CustomLoss(params)\n",
    "            self.multiclass_loss_func = CustomLoss(performance_metric='sparse_xentropy')\n",
    "            self.binary_loss_func = CustomLoss(performance_metric='xentropy')\n",
    "\n",
    "            self.multiclass_train_loss = metrics.Mean(name='multiclass_train_loss')\n",
    "            self.binary_train_loss = metrics.Mean(name='binary_train_loss')\n",
    "            self.train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "            self.multiclass_valid_loss = metrics.Mean(name='multiclass_valid_loss')\n",
    "            self.binary_valid_loss = metrics.Mean(name='binary_valid_loss')\n",
    "            self.valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "            \n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, model, inputs, outputs, active_entries, weights):\n",
    "        if weights is None:\n",
    "            weights = tf.constant(1.0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            multiclass_outputs, binary_outputs = tf.split(outputs, [self.softmax_size, self.output_size - self.softmax_size], axis=-1)\n",
    "            batch_size = tf.shape(inputs)[0]\n",
    "            initial_state = tf.zeros([batch_size, self.hidden_layer_size], dtype=tf.float32)\n",
    "            predictions,_,_ = model([inputs,initial_state, initial_state], training=True)\n",
    "            multiclass_preds, binary_preds = tf.split(predictions, [self.bins, self.predict_size - self.bins], axis=-1)\n",
    "            active_entries1, active_entries2 = tf.split(active_entries, [self.softmax_size, self.output_size - self.softmax_size], axis=-1)\n",
    "            \n",
    "            multiclass_loss = self.multiclass_loss_func.train_call(multiclass_outputs, multiclass_preds, active_entries1, weights)\n",
    "            binary_loss = self.binary_loss_func.train_call(binary_outputs, binary_preds, active_entries2, weights)\n",
    "            total_loss = multiclass_loss + binary_loss\n",
    "\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # self.train_loss.update_state(total_loss)\n",
    "        self.multiclass_train_loss.update_state(multiclass_loss)\n",
    "        self.binary_train_loss.update_state(binary_loss)\n",
    "        self.train_metric.update_state(binary_outputs, binary_preds)\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(self, model, inputs, outputs, active_entries):\n",
    "        multiclass_outputs, binary_outputs = tf.split(outputs, [self.softmax_size, self.output_size - self.softmax_size], axis=-1) \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        initial_state = tf.zeros([batch_size, self.hidden_layer_size], dtype=tf.float32)\n",
    "        predictions,_,_ = model([inputs,initial_state, initial_state], training=False)\n",
    "        multiclass_preds, binary_preds = tf.split(predictions, [self.bins, self.predict_size - self.bins], axis=-1)\n",
    "        active_entries1, active_entries2 = tf.split(active_entries, [self.softmax_size, self.output_size - self.softmax_size], axis=-1)\n",
    "\n",
    "        multiclass_loss = self.multiclass_loss_func.valid_call(multiclass_outputs, multiclass_preds, active_entries1)\n",
    "        binary_loss = self.binary_loss_func.valid_call(binary_outputs, binary_preds, active_entries2)\n",
    "        total_loss = multiclass_loss + binary_loss\n",
    "        \n",
    "        self.multiclass_valid_loss.update_state(multiclass_loss)\n",
    "        self.binary_valid_loss.update_state(binary_loss)\n",
    "        self.valid_metric.update_state(binary_outputs, binary_preds)\n",
    "    \n",
    "    def train_model(self, model):\n",
    "        # optimisation_summary = pd.Series([])\n",
    "        for epoch in tf.range(1, self.epochs+1):\n",
    "\n",
    "            for data in self.ds_train:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "            for data in self.ds_valid:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "            # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "            # 同样的日志和状态重置操作\n",
    "            logs = 'Epoch={},Multiclass Loss:{},Binary Loss:{},Accuracy:{},Multiclass Valid Loss:{}, Binary Valid Loss:{},Valid Accuracy:{}'\n",
    "\n",
    "            if epoch%1 ==0:\n",
    "                printbar()\n",
    "                tf.print(tf.strings.format(logs,\n",
    "                (epoch,self.multiclass_train_loss.result(),self.binary_train_loss.result(),\n",
    "                 self.train_metric.result(),self.multiclass_valid_loss.result(),self.binary_valid_loss.result(),\n",
    "                 self.valid_metric.result())))\n",
    "                tf.print(\"\")\n",
    "\n",
    "            self.multiclass_train_loss.reset_states()\n",
    "            self.binary_train_loss.reset_states()\n",
    "            self.multiclass_valid_loss.reset_states()\n",
    "            self.binary_valid_loss.reset_states()\n",
    "            self.train_metric.reset_states()\n",
    "            self.valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4849e3b-b60b-47af-a82a-c493c5018995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Train = TrainModule(model_parameters)\n",
    "Train.train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9032fff3-d7d8-42f4-b6ef-5d1f2114c234",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================12:15:12\n",
      "Epoch=1,Loss:-1.04218924,Accuracy:0.315503478,Valid Loss:-1.6271013,Valid Accuracy:0.256319433\n",
      "\n",
      "================================================================================12:15:13\n",
      "Epoch=2,Loss:-1.8213017,Accuracy:0.25058189,Valid Loss:-1.92948687,Valid Accuracy:0.243127644\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=3,Loss:-1.99545646,Accuracy:0.245146066,Valid Loss:-2.00789499,Valid Accuracy:0.237991899\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=4,Loss:-2.0445447,Accuracy:0.242646903,Valid Loss:-2.07682967,Valid Accuracy:0.236074388\n",
      "\n",
      "================================================================================12:15:15\n",
      "Epoch=5,Loss:-2.07054353,Accuracy:0.241571859,Valid Loss:-2.07036352,Valid Accuracy:0.23433511\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=6,Loss:-2.08658314,Accuracy:0.24051398,Valid Loss:-2.02451658,Valid Accuracy:0.234643\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=7,Loss:-2.08822966,Accuracy:0.239788935,Valid Loss:-2.04530382,Valid Accuracy:0.233269155\n",
      "\n",
      "================================================================================12:15:17\n",
      "Epoch=8,Loss:-2.10269213,Accuracy:0.239260048,Valid Loss:-2.09449148,Valid Accuracy:0.231957987\n",
      "\n",
      "================================================================================12:15:18\n",
      "Epoch=9,Loss:-2.10704803,Accuracy:0.238550946,Valid Loss:-2.07969904,Valid Accuracy:0.231935427\n",
      "\n",
      "================================================================================12:15:19\n",
      "Epoch=10,Loss:-2.12130713,Accuracy:0.238038853,Valid Loss:-2.11661029,Valid Accuracy:0.231317282\n",
      "\n",
      "================================================================================12:15:24\n",
      "Epoch=1,Loss:-1.21397805,Accuracy:0.299676478,Valid Loss:-1.7329247,Valid Accuracy:0.245012134\n",
      "\n",
      "================================================================================12:15:25\n",
      "Epoch=2,Loss:-1.89918232,Accuracy:0.247787476,Valid Loss:-2.00681877,Valid Accuracy:0.239056319\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=3,Loss:-2.02381587,Accuracy:0.244440153,Valid Loss:-2.01860476,Valid Accuracy:0.236832559\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=4,Loss:-2.0467248,Accuracy:0.242398903,Valid Loss:-1.99007571,Valid Accuracy:0.235698864\n",
      "\n",
      "================================================================================12:15:27\n",
      "Epoch=5,Loss:-2.06792665,Accuracy:0.240782276,Valid Loss:-2.07615876,Valid Accuracy:0.23492761\n",
      "\n",
      "================================================================================12:15:28\n",
      "Epoch=6,Loss:-2.08954358,Accuracy:0.240094557,Valid Loss:-2.09414053,Valid Accuracy:0.233243883\n",
      "\n",
      "================================================================================12:15:29\n",
      "Epoch=7,Loss:-2.10589767,Accuracy:0.239126161,Valid Loss:-2.08491945,Valid Accuracy:0.233909249\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=8,Loss:-2.10808825,Accuracy:0.238345563,Valid Loss:-2.10724974,Valid Accuracy:0.23198238\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=9,Loss:-2.11758161,Accuracy:0.238061085,Valid Loss:-2.09254766,Valid Accuracy:0.231905296\n",
      "\n",
      "================================================================================12:15:31\n",
      "Epoch=10,Loss:-2.12772536,Accuracy:0.237541556,Valid Loss:-2.09236026,Valid Accuracy:0.231176525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_names = ['model1','model2']\n",
    "for name in net_names:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(model_parameters, name)\n",
    "    # train_model(model, tf_data_train, tf_data_valid, 10)\n",
    "    Train = TrainModule(model_parameters)\n",
    "    Train.train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660dcda-5d51-47ae-a57e-3570fe4c5eca",
   "metadata": {},
   "source": [
    "### 2.4 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca9b3e36-cb8b-4208-8e4d-a33d28fbc990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/treatment_rnn/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/treatment_rnn/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export saved model.\n"
     ]
    }
   ],
   "source": [
    "path = 'results/tf2_try/treatment_rnn'\n",
    "model.save(path, save_format = 'tf')\n",
    "print('export saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12fb4e51-2e73-43bb-add4-fae0b01b772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = models.load_model(path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d54a8-ba90-4260-9ffb-9b12532697b5",
   "metadata": {},
   "source": [
    "### 2.5 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f9a8e93-6b0f-440f-b46d-94de062a6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型评估函数\n",
    "def evaluate_model(model, ds_test):\n",
    "    total_loss = 0\n",
    "    total_metric = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 遍历数据集中的每个批次\n",
    "    for data in ds_test:\n",
    "        valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        total_loss += valid_loss.result().numpy()\n",
    "        total_metric += valid_metric.result().numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 重置状态\n",
    "        valid_loss.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "    # 计算整个数据集的平均损失和指标\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metric = total_metric / num_batches\n",
    "    return avg_loss, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6989a47-7ac5-4e66-9faf-ad79db5a5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -2.254194164276123, Test Accuracy: 0.22184113562107086\n",
      "5/5 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = evaluate_model(model, tf_data_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "input_data_for_prediction = tf_data_test.map(lambda x: x['inputs'])\n",
    "predictions = model_loaded.predict(input_data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee42e19-e360-49bb-9fda-3c35c119e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 160, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a3425-92cd-4780-96a5-d3ab5bc8e876",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Propensitry Generation\n",
    "def propensity_generation(dataset_map, MODEL_ROOT, b_use_predicted_confounders, b_use_all_data=False,\n",
    "                          b_use_oracle_confounders=False, b_remove_x1=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cab139-1733-4b38-ac0e-103a364f7b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 13:15:56.564629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:42:00.0, compute capability: 8.9\n",
      "2024-03-20 13:15:56.565474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:43:00.0, compute capability: 8.9\n",
      "2024-03-20 13:15:56.566205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22288 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:83:00.0, compute capability: 8.9\n",
      "2024-03-20 13:15:56.566838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22288 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:c1:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "path_actions = os.path.join(MODEL_ROOT, \"treatment_rnn_action_inputs_only/treatment_rnn_action_inputs_only_0.1_78_100_128_0.005_2.0_60_optimal\")\n",
    "model_actions = models.load_model(path_actions, compile=False)\n",
    "\n",
    "path_treats = os.path.join(MODEL_ROOT, \"treatment_rnn/treatment_rnn_0.1_145_100_64_0.005_1.0_60_optimal\")\n",
    "model_treats = models.load_model(path_treats, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0435e9e2-8e77-4b4a-8b6c-f734143bc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmsn.libs.rmsn_model as rmsn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8fb8d8-42a6-4321-b7a8-6fe4a0293f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "# action_inputs_only = rmsn_model.load_optimal_parameters(net_name='treatment_rnn_action_inputs_only', MODEL_ROOT=MODEL_ROOT)\n",
    "# action_w_trajectory_inputs = rmsn_model.load_optimal_parameters(net_name='treatment_rnn', MODEL_ROOT=MODEL_ROOT)\n",
    "\n",
    "# Generate propensity weights for validation data as well - used for MSM which is calibrated on train + valid data\n",
    "b_with_validation = False\n",
    "# Generate non-stabilised IPTWs (default false)\n",
    "b_denominator_only = False\n",
    "b_use_predicted_confounders = False\n",
    "b_use_all_data=False\n",
    "b_use_oracle_confounders=False\n",
    "b_remove_x1=False\n",
    "\n",
    "# Config + activation functions\n",
    "activation_map = {'rnn_propensity_weighted': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'rnn_model_bptt': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  }\n",
    "\n",
    "# configs = {'action_num': action_inputs_only,\n",
    "#            'action_den': action_w_trajectory_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d86ece3-4f82-4587-bd20-003ca45d3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if b_use_all_data:\n",
    "    training_data = dataset_map\n",
    "    validation_data = dataset_map\n",
    "    test_data = None\n",
    "else:\n",
    "    training_data = dataset_map['training_data']\n",
    "    validation_data = dataset_map['validation_data']\n",
    "    test_data = dataset_map['test_data']\n",
    "    \n",
    "if b_with_validation:\n",
    "    for k in training_data:\n",
    "        training_data[k] = np.concatenate([training_data[k], validation_data[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4915e78-9338-4360-a904-6fc322e6203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_dataset_predict(dataset_map, minibatch_size):\n",
    "    key_map = {'inputs': dataset_map['scaled_inputs'],\n",
    "               'outputs': dataset_map['scaled_outputs'],\n",
    "               'active_entries': dataset_map['active_entries'],\n",
    "               'sequence_lengths': dataset_map['sequence_lengths']}\n",
    "\n",
    "    if 'propensity_weights' in dataset_map:\n",
    "        key_map['propensity_weights'] = dataset_map['propensity_weights']\n",
    "\n",
    "    if 'initial_states' in dataset_map:\n",
    "        key_map['initial_states'] = dataset_map['initial_states']\n",
    "\n",
    "    #from_tensor_slices:切片; shuffle:随机打乱; batch:批次组合; prefetch:提前准备（预取）数据\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(key_map)\\\n",
    "                .batch(minibatch_size) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3467c07e-8ad0-4ef3-9e32-1452dfed8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, dataset, hidden_layer_size, mc_sampling=False):\n",
    "    # Initialize lists to store final statistics for all chunks\n",
    "    Predictions = []\n",
    "    logs = 'Predicting ' + model.name\n",
    "    if mc_sampling:\n",
    "        pred_times = 10\n",
    "    else:\n",
    "        pred_times = 1\n",
    "\n",
    "    for _ in range(pred_times):\n",
    "        predictions = []\n",
    "        for data_chunk in tqdm(dataset, desc=logs):\n",
    "            batch_size = tf.shape(data_chunk['inputs'])[0]\n",
    "            initial_state = tf.zeros([batch_size, hidden_layer_size], dtype=tf.float32)\n",
    "            # Predict the current chunk multiple times\n",
    "            chunk_prediction, _, _ = model.predict([data_chunk['inputs'], initial_state, initial_state], verbose=0)\n",
    "            # print(prediction.shape)\n",
    "                # chunk_predictions.append(prediction)\n",
    "\n",
    "            # Convert list of predictions to a numpy array for statistical computation\n",
    "            # chunk_predictions = np.array(chunk_predictions)\n",
    "            predictions.extend(chunk_prediction)\n",
    "        \n",
    "        if mc_sampling:\n",
    "            Predictions.append(predictions)\n",
    "    \n",
    "    if mc_sampling:\n",
    "        return np.stack(Predictions,axis=0)\n",
    "    else:\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef492457-dc81-4c80-a751-62931002a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_predictions(model, dataset, hidden_layer_size):\n",
    "    # Pull datasets\n",
    "    net_name = model.name\n",
    "    b_predict_actions = \"treatment_rnn\" in net_name\n",
    "    b_use_actions_only = \"rnn_action_inputs_only\" in net_name\n",
    "    b_use_predicted_confounders = True\n",
    "    softmax_size = 1\n",
    "    num_bins = 20\n",
    "    minibatch_size = 128\n",
    "\n",
    "    # Extract only relevant trajs and shift data\n",
    "    training_processed = get_processed_data(training_data, b_predict_actions, b_use_actions_only,\n",
    "                                                 b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                                   b_use_actions_only,\n",
    "                                                   b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    # rango added 23.10.24\n",
    "    # if b_with_test:\n",
    "    #     test_processed = core.get_processed_data(test_data, b_predict_actions, b_use_actions_only,\n",
    "    #                                              b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "    num_features = training_processed['scaled_inputs'].shape[-1]  # 4 if not b_use_actions_only else 3\n",
    "    num_outputs = training_processed['scaled_outputs'].shape[-1]  # 1 if not b_predict_actions else 3  # 5\n",
    "\n",
    "\n",
    "    # Unpack remaining variables\n",
    "    tf_data_train = convert_to_tf_dataset_predict(training_processed, minibatch_size)\n",
    "\n",
    "    # predictition\n",
    "    outputs = training_processed['scaled_outputs']\n",
    "    multiclass_outputs, binary_outputs = np.split(outputs, [softmax_size], axis=-1)\n",
    "    \n",
    "    # 对binary和multiclass变量预测一次就行，对continuous变量则应用mc dropout\n",
    "    # for binary and multiclass\n",
    "    results = model_predict(model, tf_data_train, hidden_layer_size)\n",
    "    multiclass_preds, binary_preds = np.split(results, [num_bins], axis=-1)\n",
    "    \n",
    "\n",
    "    return multiclass_preds, multiclass_outputs, binary_preds, binary_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "661f1d36-cb74-42fb-9fd4-e986f4cec5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 13.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predicted confounders\n",
      "Using predicted confounders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn: 100%|██████████| 19/19 [00:01<00:00, 11.53it/s]\n"
     ]
    }
   ],
   "source": [
    "multiclass_preds_actions, multiclass_outputs_actions, _, _ = get_predictions(model_actions, training_data, 78)\n",
    "multiclass_preds_treats, multiclass_outputs_treats, _, _ = get_predictions(model_treats, training_data, 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba6124dd-e3e9-43d3-b869-f3a92a162b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_outputs_actions = tf.cast(multiclass_outputs_actions, tf.int32)\n",
    "multiclass_outputs_actions = tf.squeeze(multiclass_outputs_actions, axis=-1)\n",
    "# multiclass_preds_actions = tf.convert_to_tensor(multiclass_preds_actions, dtype=tf.float32)\n",
    "# actions = tf.gather(multiclass_preds_actions, multiclass_outputs_actions, axis=1, batch_dims=1)\n",
    "\n",
    "multiclass_outputs_treats = tf.cast(multiclass_outputs_treats, tf.int32)\n",
    "multiclass_outputs_treats = tf.squeeze(multiclass_outputs_treats, axis=-1)\n",
    "# treats = tf.gather(multiclass_preds_treats, multiclass_outputs_treats, axis=1, batch_dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9ea90094-402f-432e-b533-1b58b7d19074",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = multiclass_preds_actions\n",
    "output = multiclass_outputs_actions\n",
    "# 需要为 output 生成额外的索引，因为 tf.gather 需要索引来收集数据\n",
    "batch_size, time_steps, class_indices = tf.shape(preds)\n",
    "\n",
    "# 生成每个样本和时间点的索引\n",
    "batch_indices = tf.tile(tf.reshape(tf.range(batch_size), [-1, 1, 1]), [1, time_steps, 1])\n",
    "time_indices = tf.tile(tf.reshape(tf.range(time_steps), [1, -1, 1]), [batch_size, 1, 1])\n",
    "\n",
    "# 合并索引和 output，为 tf.gather_nd 准备\n",
    "indices = tf.concat([batch_indices, time_indices, tf.expand_dims(output, axis=-1)], axis=-1)\n",
    "\n",
    "# 使用 tf.gather_nd 从 preds 中收集数据\n",
    "selected_probs = tf.gather_nd(preds, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9370904c-205c-4012-8df5-fa6fa0a1eae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = multiclass_preds_actions\n",
    "outputs = multiclass_outputs_actions\n",
    "w_list = []\n",
    "outputs = tf.squeeze(tf.cast(outputs, tf.int32), axis=-1)\n",
    "total_batches, time_steps, class_indices = tf.shape(preds)\n",
    "batch_size = 128\n",
    "# Process each batch\n",
    "for batch_start in range(0, total_batches, batch_size):\n",
    "    # Calculate batch end, ensuring it does not exceed total_batches\n",
    "    batch_end = min(batch_start + batch_size, total_batches)\n",
    "\n",
    "    # Slice the preds and outputs for the current batch\n",
    "    batch_preds = preds[batch_start:batch_end]\n",
    "    batch_outputs = outputs[batch_start:batch_end]\n",
    "\n",
    "    # Generate indices for tf.gather_nd\n",
    "    batch_indices = tf.tile(tf.reshape(tf.range(0, batch_end - batch_start), [-1, 1, 1]), [1, time_steps, 1])\n",
    "    time_indices = tf.tile(tf.reshape(tf.range(time_steps), [1, -1, 1]), [batch_end - batch_start, 1, 1])\n",
    "    indices = tf.concat([batch_indices, time_indices, tf.expand_dims(batch_outputs, axis=-1)], axis=-1)\n",
    "\n",
    "    # Gather values for the current batch\n",
    "    batch_w = tf.gather_nd(batch_preds, indices)\n",
    "\n",
    "    # Append the results for this batch\n",
    "    w_list.append(batch_w)\n",
    "\n",
    "# Concatenate the results from all batches\n",
    "w = tf.concat(w_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da10bb20-6018-4233-9208-51df56f19b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2403, 160), dtype=float32, numpy=\n",
       "array([[0.05363392, 0.05557147, 0.05658683, ..., 0.08788664, 0.05350872,\n",
       "        0.0601549 ],\n",
       "       [0.05828674, 0.02181547, 0.01673403, ..., 0.20610863, 0.20606615,\n",
       "        0.20603818],\n",
       "       [0.08382894, 0.10698327, 0.12100453, ..., 0.20903188, 0.20480792,\n",
       "        0.20494121],\n",
       "       ...,\n",
       "       [0.11872496, 0.16218522, 0.02002042, ..., 0.062911  , 0.06290466,\n",
       "        0.06289858],\n",
       "       [0.03552536, 0.05896456, 0.19522099, ..., 0.1164525 , 0.11645064,\n",
       "        0.11644923],\n",
       "       [0.08639418, 0.03577318, 0.12207589, ..., 0.20578621, 0.2058126 ,\n",
       "        0.20583475]], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f92b095-35e6-4a26-a04f-96a2aff62a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(w==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1cd1d97f-90c7-4f76-bee9-331a1e30ddff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2403, 160])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc838fb5-360d-4ba3-9de2-41df1193853e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2403, 160), dtype=float32, numpy=\n",
       "array([[0.05363392, 0.05557147, 0.05658683, ..., 0.08788664, 0.05350872,\n",
       "        0.0601549 ],\n",
       "       [0.05828674, 0.02181547, 0.01673403, ..., 0.20610863, 0.20606615,\n",
       "        0.20603818],\n",
       "       [0.08382894, 0.10698327, 0.12100453, ..., 0.20903188, 0.20480792,\n",
       "        0.20494121],\n",
       "       ...,\n",
       "       [0.11872496, 0.16218522, 0.02002042, ..., 0.062911  , 0.06290466,\n",
       "        0.06289858],\n",
       "       [0.03552536, 0.05896456, 0.19522099, ..., 0.1164525 , 0.11645064,\n",
       "        0.11644923],\n",
       "       [0.08639418, 0.03577318, 0.12207589, ..., 0.20578621, 0.2058126 ,\n",
       "        0.20583475]], dtype=float32)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e14e8dad-509b-4fba-bb62-13761bb8cd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 160, 20)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_preds_actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df00c0eb-c785-4204-aa72-586fd88725c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2403, 160), dtype=int32, numpy=\n",
       "array([[ 5,  5,  5, ..., 10,  5,  5],\n",
       "       [11, 12, 12, ...,  8,  8,  8],\n",
       "       [ 6,  6,  6, ...,  7,  7,  7],\n",
       "       ...,\n",
       "       [ 8,  8, 12, ...,  5,  5,  5],\n",
       "       [12, 11,  8, ..., 10, 10, 10],\n",
       "       [ 6,  4,  6, ...,  8,  8,  8]], dtype=int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclass_outputs_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "51801536-64ee-4238-a803-694c3569d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_predictions(config, num_continuous):\n",
    "\n",
    "    net_name = config[0]\n",
    "    serialisation_name = config[-1]\n",
    "\n",
    "    hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "    # Pull datasets\n",
    "    b_predict_actions = \"treatment_rnn\" in net_name\n",
    "    b_use_actions_only = \"rnn_action_inputs_only\" in net_name\n",
    "\n",
    "    # Extract only relevant trajs and shift data\n",
    "    training_processed = get_processed_data(training_data, b_predict_actions, b_use_actions_only,\n",
    "                                                 b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                                   b_use_actions_only,\n",
    "                                                   b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    # rango added 23.10.24\n",
    "    # if b_with_test:\n",
    "    #     test_processed = core.get_processed_data(test_data, b_predict_actions, b_use_actions_only,\n",
    "    #                                              b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "    num_features = training_processed['scaled_inputs'].shape[-1]  # 4 if not b_use_actions_only else 3\n",
    "    num_outputs = training_processed['scaled_outputs'].shape[-1]  # 1 if not b_predict_actions else 3  # 5\n",
    "\n",
    "\n",
    "    # Unpack remaining variables\n",
    "    dropout_rate = config[1]\n",
    "    hidden_layer_size = config[2]\n",
    "    memory_multiplier = config[2] / num_features\n",
    "    num_epochs = config[3]\n",
    "    minibatch_size = config[4]\n",
    "    learning_rate = config[5]\n",
    "    max_norm = config[6]\n",
    "    tf_data_train = convert_to_tf_dataset_predict(training_processed, minibatch_size)\n",
    "    tf_data_valid = convert_to_tf_dataset_predict(validation_processed, minibatch_size)\n",
    "\n",
    "    model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "    model = rmsn_model.load_model(model_folder, serialisation_name)\n",
    "\n",
    "    # predictition\n",
    "    outputs = training_processed['scaled_outputs']\n",
    "    continuous_outputs, binary_outputs = np.split(outputs, [num_continuous], axis=-1)\n",
    "    \n",
    "    # 对binary变量预测一次就行，对continuous变量则应用mc dropout\n",
    "    # for binary\n",
    "    results = model_predict(model, tf_data_train, config)\n",
    "    _, binary_preds = np.split(results, [num_continuous], axis=-1)\n",
    "    \n",
    "    # for continuous\n",
    "    mc_results = model_predict(model, tf_data_train, config, mc_sampling=True)\n",
    "    continuous_preds, _ = np.split(results, [num_continuous], axis=-1)\n",
    "\n",
    "    return continuous_preds, continuous_outputs, binary_preds, binary_outputs\n",
    "\n",
    "def get_weights(probs, targets):\n",
    "    w = probs*targets + (1-probs) * (1-targets)\n",
    "    print(f\"shape of w:{w.shape}\")\n",
    "    return w.prod(axis=2)\n",
    "\n",
    "def get_prob_density(preds, outputs):\n",
    "    mean_predictions = tf.reduce_mean(preds, axis=0)\n",
    "    std_dev_predictions = tf.math.reduce_std(preds, axis=0)\n",
    "    # use normal distribution\n",
    "    distributions = tfp.distributions.Normal(loc=mean_predictions, scale=std_dev_predictions)\n",
    "    probs = distributions.prob(outputs)\n",
    "    probs = probs.numpy()\n",
    "    return probs.prod(axis=2)\n",
    "\n",
    "def get_weights_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    continuous_preds, continuous_outputs, binary_preds, binary_outputs = get_predictions(config, 1)\n",
    "    # for binary treatments\n",
    "    binary_w = get_weights(binary_preds, binary_outputs)\n",
    "    continuous_w = get_prob_density(continuous_preds, continuous_outputs)\n",
    "    return {'binary_w':binary_w, 'continuous_w':continuous_w}\n",
    "\n",
    "def get_probabilities_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    probs, targets = get_predictions(config)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fe7c88d4-c3d3-4871-a8c3-6df5d70fb810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def model_predict(model, dataset, config, mc_sampling=False):\n",
    "    # Initialize lists to store final statistics for all chunks\n",
    "    all_means = []\n",
    "    all_upper_bounds = []\n",
    "    all_lower_bounds = []\n",
    "    logs = 'Predicting ' + model.name\n",
    "    if mc_sampling:\n",
    "        pred_times = 100\n",
    "    else:\n",
    "        pred_times = 1\n",
    "\n",
    "    for data_chunk in tqdm(dataset, desc=logs):\n",
    "        chunk_predictions = []\n",
    "        batch_size = tf.shape(data_chunk['inputs'])[0]\n",
    "        hidden_layer_size = config[2]\n",
    "        initial_state = tf.zeros([batch_size, hidden_layer_size], dtype=tf.float32)\n",
    "        # Predict the current chunk multiple times\n",
    "        for _ in range(pred_times):\n",
    "            prediction, _, _ = model.predict([data_chunk['inputs'], initial_state, initial_state], verbose=0)\n",
    "            chunk_predictions.append(prediction)\n",
    "\n",
    "        # Convert list of predictions to a numpy array for statistical computation\n",
    "        chunk_predictions = np.array(chunk_predictions)\n",
    "\n",
    "        # Calculate mean, upper bound, and lower bound for the current chunk\n",
    "        mean_estimate = np.mean(chunk_predictions, axis=0)\n",
    "        upper_bound = np.percentile(chunk_predictions, 95, axis=0)\n",
    "        lower_bound = np.percentile(chunk_predictions, 5, axis=0)\n",
    "\n",
    "        # Append the statistics of the current chunk to their respective lists\n",
    "        all_means.append(mean_estimate)\n",
    "        all_upper_bounds.append(upper_bound)\n",
    "        all_lower_bounds.append(lower_bound)\n",
    "\n",
    "    # Optional: Convert lists to numpy arrays if further processing is needed\n",
    "    all_means = np.concatenate(all_means, axis=0) if all_means else np.array([])\n",
    "    all_upper_bounds = np.concatenate(all_upper_bounds, axis=0) if all_upper_bounds else np.array([])\n",
    "    all_lower_bounds = np.concatenate(all_lower_bounds, axis=0) if all_lower_bounds else np.array([])\n",
    "\n",
    "    # At this point, you can either return the raw statistics for each chunk,\n",
    "    # or aggregate them in some way depending on your application's needs.\n",
    "    # The following returns the list of statistics for all chunks.\n",
    "    return {\n",
    "        'mean_pred': all_means,\n",
    "        'upper_bound': all_upper_bounds,\n",
    "        'lower_bound': all_lower_bounds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "719e1d4f-a171-4cf0-b8b4-564d81441371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:02<00:00,  9.31it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.37it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 13.22it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.24it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.35it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.56it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.69it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.31it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.67it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 12.41it/s]\n",
      "Predicting treatment_rnn_action_inputs_only: 100%|██████████| 19/19 [00:01<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of w:(2403, 160, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:03<00:00, 12.03it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 13.90it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 13.97it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 13.61it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 13.97it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 14.34it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:02<00:00, 13.91it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:03<00:00, 10.48it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:03<00:00, 12.06it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:03<00:00, 11.99it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 38/38 [00:03<00:00, 11.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of w:(2403, 160, 2)\n"
     ]
    }
   ],
   "source": [
    "# Action with trajs\n",
    "weights = {k: get_weights_from_config(configs[k]) for k in configs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3996880b-16fe-4cf3-a23a-5688a744799e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2403, 160)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['action_num']['continuous_w'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c77835c6-ef2f-473d-b9b4-7dd527a318db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于同时存在二元处置和连续处置的情况，将分别计算iptw后相乘\n",
    "den = weights['action_den']\n",
    "num = weights['action_num']\n",
    "\n",
    "propensity_weights = np.ones([2403, 160])\n",
    "for key in weights['action_den'].keys():\n",
    "    den = weights['action_den'][key]\n",
    "    num = weights['action_num'][key]\n",
    "    propensity_weights *= 1.0/den if b_denominator_only else num/den\n",
    "\n",
    "# 设置上界\n",
    "propensity_weights = np.clip(propensity_weights, None, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2e77491f-687b-44ca-85e9-2092c94223fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 1\n",
    "(num_patients, num_time_steps) = propensity_weights.shape\n",
    "output = np.ones((num_patients, num_time_steps, horizon))\n",
    "\n",
    "tmp = np.ones((num_patients, num_time_steps))\n",
    "tmp[:, 1:] = propensity_weights[:, :-1]\n",
    "propensity_weights = tmp\n",
    "\n",
    "for i in range(horizon):\n",
    "    output[:, :num_time_steps-i, i] = propensity_weights[:, i:]\n",
    "\n",
    "propensity_weights = output.cumprod(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "923d2686-92f6-49a1-9d3a-bb35e74989d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "den = weights['action_den']['binary_w']\n",
    "num = weights['action_num']['binary_w']\n",
    "propensity_weights *= 1.0/den if b_denominator_only else num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "77fd1172-718c-4b66-9ddc-51818f03df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999887\n",
      "1.8555174e-07\n",
      "0.9999998\n",
      "4.4851085e-08\n"
     ]
    }
   ],
   "source": [
    "print(np.max(den))\n",
    "print(np.min(den))\n",
    "print(np.max(num))\n",
    "print(np.min(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "03526280-7909-4387-b26a-52eb58cea912",
   "metadata": {},
   "outputs": [],
   "source": [
    "den = weights['action_den']['continuous_w']\n",
    "num = weights['action_num']['continuous_w']\n",
    "propensity_weights *= 1.0/den if b_denominator_only else num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "38334727-86d1-4205-bd81-5f1d8b40cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.913314699470504e-05\n",
      "147.92051141155207\n",
      "0.0027881814398668333\n"
     ]
    }
   ],
   "source": [
    "print(np.min(propensity_weights))\n",
    "print(np.max(propensity_weights))\n",
    "print(np.sum(propensity_weights>2)/(2403*160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "2fd70ae7-7aa5-425b-b588-378044134368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.29000e+02, 4.83000e+02, 2.22000e+02, 2.15000e+02, 2.17000e+02,\n",
       "        2.25000e+02, 2.00000e+02, 2.09000e+02, 2.41000e+02, 2.67000e+02,\n",
       "        3.11000e+02, 3.70000e+02, 4.29000e+02, 5.47000e+02, 6.70000e+02,\n",
       "        8.74000e+02, 1.10500e+03, 1.51700e+03, 2.13900e+03, 3.11800e+03,\n",
       "        4.66100e+03, 7.96300e+03, 1.54900e+04, 4.54710e+04, 1.14564e+05,\n",
       "        1.09046e+05, 4.05610e+04, 1.20620e+04, 5.70000e+03, 3.50500e+03,\n",
       "        2.46600e+03, 1.76900e+03, 1.28200e+03, 9.59000e+02, 7.52000e+02,\n",
       "        5.64000e+02, 4.41000e+02, 3.97000e+02, 3.04000e+02, 2.61000e+02,\n",
       "        2.56000e+02, 2.04000e+02, 1.56000e+02, 1.73000e+02, 1.37000e+02,\n",
       "        1.10000e+02, 1.12000e+02, 1.08000e+02, 7.70000e+01, 1.14100e+03]),\n",
       " array([3.91331470e-05, 4.00383505e-02, 8.00375678e-02, 1.20036785e-01,\n",
       "        1.60036002e-01, 2.00035220e-01, 2.40034437e-01, 2.80033655e-01,\n",
       "        3.20032872e-01, 3.60032089e-01, 4.00031307e-01, 4.40030524e-01,\n",
       "        4.80029741e-01, 5.20028959e-01, 5.60028176e-01, 6.00027393e-01,\n",
       "        6.40026611e-01, 6.80025828e-01, 7.20025045e-01, 7.60024263e-01,\n",
       "        8.00023480e-01, 8.40022697e-01, 8.80021915e-01, 9.20021132e-01,\n",
       "        9.60020349e-01, 1.00001957e+00, 1.04001878e+00, 1.08001800e+00,\n",
       "        1.12001722e+00, 1.16001644e+00, 1.20001565e+00, 1.24001487e+00,\n",
       "        1.28001409e+00, 1.32001331e+00, 1.36001252e+00, 1.40001174e+00,\n",
       "        1.44001096e+00, 1.48001017e+00, 1.52000939e+00, 1.56000861e+00,\n",
       "        1.60000783e+00, 1.64000704e+00, 1.68000626e+00, 1.72000548e+00,\n",
       "        1.76000470e+00, 1.80000391e+00, 1.84000313e+00, 1.88000235e+00,\n",
       "        1.92000157e+00, 1.96000078e+00, 2.00000000e+00]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(propensity_weights.flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c78f1e-16e7-46ce-9386-455f2817cd16",
   "metadata": {},
   "source": [
    "## MC dropout test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b66d21de-b880-4db5-9a64-02779432a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dbd95b6a-66b8-445b-8c30-cfc20fe2f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model, dataset, config, mc_sampling=False):\n",
    "    # Initialize lists to store final statistics for all chunks\n",
    "    Predictions = []\n",
    "    logs = 'Predicting ' + model.name\n",
    "    if mc_sampling:\n",
    "        pred_times = 10\n",
    "    else:\n",
    "        pred_times = 1\n",
    "\n",
    "    for _ in range(pred_times):\n",
    "        predictions = []\n",
    "        for data_chunk in tqdm(dataset, desc=logs):\n",
    "            batch_size = tf.shape(data_chunk['inputs'])[0]\n",
    "            hidden_layer_size = config[2]\n",
    "            initial_state = tf.zeros([batch_size, hidden_layer_size], dtype=tf.float32)\n",
    "            # Predict the current chunk multiple times\n",
    "            chunk_prediction, _, _ = model.predict([data_chunk['inputs'], initial_state, initial_state], verbose=0)\n",
    "            # print(prediction.shape)\n",
    "                # chunk_predictions.append(prediction)\n",
    "\n",
    "            # Convert list of predictions to a numpy array for statistical computation\n",
    "            # chunk_predictions = np.array(chunk_predictions)\n",
    "            predictions.extend(chunk_prediction)\n",
    "        \n",
    "        if mc_sampling:\n",
    "            Predictions.append(predictions)\n",
    "    \n",
    "    if mc_sampling:\n",
    "        return np.stack(Predictions,axis=0)\n",
    "    else:\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfae5826-44c4-43f9-9962-3817e32b0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "config_actions = configs['action_num']\n",
    "model_folder = os.path.join(MODEL_ROOT, config_actions[0])\n",
    "model_action = rmsn_model.load_model(model_folder, config_actions[-1])\n",
    "\n",
    "config_treats = configs['action_den']\n",
    "model_folder = os.path.join(MODEL_ROOT, config_treats[0])\n",
    "model_treats = rmsn_model.load_model(model_folder, config_treats[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ddd1ffb-b810-4a62-a138-9dfad5816147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.76it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.57it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.14it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.06it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.47it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.71it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.93it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.98it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.33it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.00it/s]\n"
     ]
    }
   ],
   "source": [
    "results = model_predict(model_treats, tf_data_test, config_treats, mc_sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d010713a-5fb7-42a5-a4ed-9f51dcb135cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 300, 160, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_continuous = 1\n",
    "continuous_preds, _ = np.split(results, [num_continuous], axis=-1)\n",
    "continuous_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb9a0b9a-870c-4b5d-8fa4-3f5e9821594b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No such layer: lstm. Existing layers are: ['input_2', 'initial_h', 'initial_c', 'lstm_1', 'dense_2', 'reshape_1', 'tf.split_1', 'dense_3', 'dropout_1', 'activation_1', 'concatenate_1'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m; mc_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_mc_dropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_treats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_data_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m pred\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn [47], line 4\u001b[0m, in \u001b[0;36mpredict_with_mc_dropout\u001b[0;34m(model, dataset, n_predictions)\u001b[0m\n\u001b[1;32m      2\u001b[0m Predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_predictions):\n\u001b[0;32m----> 4\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mrmsn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_data_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     preds \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m     continuous_preds,_ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msplit(preds, [mc_size, output_size \u001b[38;5;241m-\u001b[39m mc_size], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/autodl-tmp/distributed_tf2/rmsn/libs/rmsn_model.py:203\u001b[0m, in \u001b[0;36mmodel_predict\u001b[0;34m(model, dataset, mc_sampling)\u001b[0m\n\u001b[1;32m    201\u001b[0m chunk_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    202\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(data_chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 203\u001b[0m hidden_layer_size \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munits\n\u001b[1;32m    204\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros([batch_size, hidden_layer_size], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# Predict the current chunk multiple times\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/engine/training.py:3464\u001b[0m, in \u001b[0;36mModel.get_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m name:\n\u001b[1;32m   3463\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m layer\n\u001b[0;32m-> 3464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3465\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Existing layers are: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3466\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3467\u001b[0m     )\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3469\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvide either a layer name or layer index at `get_layer`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3470\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: lstm. Existing layers are: ['input_2', 'initial_h', 'initial_c', 'lstm_1', 'dense_2', 'reshape_1', 'tf.split_1', 'dense_3', 'dropout_1', 'activation_1', 'concatenate_1']."
     ]
    }
   ],
   "source": [
    "output_size=3; mc_size=1\n",
    "pred = predict_with_mc_dropout(model_treats, tf_data_test, n_predictions=1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "184117a9-aafd-4855-8415-c2933881e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_mc_dropout(model, dataset, n_predictions=10):\n",
    "    Predictions = []\n",
    "    for _ in range(n_predictions):\n",
    "        preds = model_predict(model, tf_data_test, config_treats)\n",
    "        preds = preds['mean_pred']\n",
    "        continuous_preds,_ = tf.split(preds, [mc_size, output_size - mc_size], axis=-1)\n",
    "        Predictions.append(continuous_preds)\n",
    "    \n",
    "    return tf.stack(Predictions,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bd2b88fc-5fa0-4517-ac88-c6650367a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 11.83it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.25it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.97it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.72it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.62it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.92it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.00it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 12.44it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.26it/s]\n",
      "Predicting treatment_rnn: 100%|██████████| 5/5 [00:00<00:00, 13.73it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_with_mc_dropout(model_treats, tf_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e4a94926-c461-4762-84b9-400a367e3e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 300, 160, 1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9454448e-729b-4db0-a0da-bbce269d20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个时间点建立分布\n",
    "mean_predictions = tf.reduce_mean(predictions, axis=0)\n",
    "std_dev_predictions = tf.math.reduce_std(predictions, axis=0)\n",
    "\n",
    "distributions = tfp.distributions.Normal(loc=mean_predictions, scale=std_dev_predictions)\n",
    "\n",
    "outputs = test_processed['outputs'][...,:1]\n",
    "probs = distributions.prob(outputs)\n",
    "probs = probs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "867985e4-df0e-4de9-9af0-7c717c414a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 160, 3)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4deedcd-edaf-4804-acd5-a09e81448678",
   "metadata": {
    "tags": []
   },
   "source": [
    "# dataset check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e04b6a-017c-46bd-8290-514cef9ca888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation_utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fedc838-50c1-4324-b64f-6ea24c81acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data_path = \"../fullfeature_fillmean_1000.txt\"\n",
    "origin_data = load_data(origin_data_path)\n",
    "\n",
    "process_data_path = \"results/tf2_test_dataset_with_substitute_confounders.h5\"\n",
    "process_data = load_data(process_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae1a0ed-c2e2-44ae-a2ab-1117541d99bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(3000, 160, 25)\n",
      "float64\n",
      "previous_treatments\n",
      "(3000, 160, 3)\n",
      "float64\n",
      "covariates\n",
      "(3000, 161, 25)\n",
      "float64\n",
      "treatments\n",
      "(3000, 161, 3)\n",
      "float64\n",
      "sequence_length\n",
      "(3000,)\n",
      "int64\n",
      "outcomes\n",
      "(3000, 161, 1)\n",
      "float64\n",
      "\n",
      "\n",
      "covariates\n",
      "(3000, 161, 25)\n",
      "float32\n",
      "outcomes\n",
      "(3000, 161, 1)\n",
      "float32\n",
      "predicted_confounders\n",
      "(3000, 161, 1)\n",
      "float32\n",
      "previous_covariates\n",
      "(3000, 160, 25)\n",
      "float32\n",
      "previous_treatments\n",
      "(3000, 160, 3)\n",
      "float32\n",
      "sequence_length\n",
      "(3000,)\n",
      "int64\n",
      "treatments\n",
      "(3000, 161, 3)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "for key in origin_data.keys():\n",
    "    print(key)\n",
    "    print(origin_data[key].shape)\n",
    "    print(origin_data[key].dtype)\n",
    "print(\"\\n\")\n",
    "for key in process_data.keys():\n",
    "    print(key)\n",
    "    print(process_data[key].shape)\n",
    "    print(process_data[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64994fed-9dd9-4faf-86b8-bcba9014e25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariates\n",
      "False\n",
      "outcomes\n",
      "False\n",
      "predicted_confounders\n",
      "False\n",
      "previous_covariates\n",
      "False\n",
      "previous_treatments\n",
      "False\n",
      "sequence_length\n",
      "False\n",
      "treatments\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for key in process_data.keys():\n",
    "    print(key)\n",
    "    print(np.any(np.isnan(process_data[key])))\n",
    "    # print(process_data[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30c56c0a-c7bd-4281-8c8c-46aec034864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates:False\n",
      "previous_treatments:True\n",
      "covariates:False\n",
      "treatments:True\n",
      "sequence_length:True\n",
      "outcomes:True\n"
     ]
    }
   ],
   "source": [
    "for key in origin_data.keys():\n",
    "    print(f\"{key}:{np.all(origin_data[key]==process_data[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8df3ea59-04f8-4cba-a4d0-7526e04fb8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.2 17.2  0.1 ... 29.3  6.3 29.9]\n",
      "[13.2 17.2  0.1 ... 29.3  6.3 29.9]\n"
     ]
    }
   ],
   "source": [
    "key = \"previous_covariates\"\n",
    "idx = np.where(origin_data[key]!=process_data[key])\n",
    "print(origin_data[key][idx])\n",
    "print(process_data[key][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbfeadbf-4636-44fc-b805-cbf26197b761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmsn.libs.data_process import data_generator, convert_to_tf_dataset_via_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b35437-76f6-4bba-9af7-33fd1523b7ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': array([[[ 1.        , -0.17024788, -0.02530078, ...,  0.11512377,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -1.2720935 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -1.3413386 ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -1.1035076 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ...,  1.1884362 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -1.5693371 ,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.        , -0.17024788,  1.4222689 , ...,  0.64105654,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  1.5331987 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  1.933745  ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.2674033 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.2674033 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.2674033 ,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788, -0.7490856 , ..., -1.0475737 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -1.0507629 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -1.0539656 ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.74051476,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.5120034 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.4755162 ,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.        , -0.17024788, -0.02530078, ...,  1.8655655 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788, -0.02530078, ...,  1.8655655 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788, -0.02530078, ...,  1.8655655 ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.        , -0.17024788, -0.02530078, ..., -0.13916233,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788, -0.02530078, ..., -1.1077944 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788, -0.02530078, ...,  0.10041763,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788, -0.7490856 , ..., -0.7926621 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ...,  0.00908527,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.274175  ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.17268711,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.17268711,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.17268711,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788, -0.02530078, ..., -1.0199158 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -0.9509022 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -0.8767934 ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -0.86142164,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -0.86142164,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.02530078, ..., -0.86142164,\n",
      "          0.        ,  0.        ]]], dtype=float32), 'outputs': array([[[-1.2720935 ,  0.        ,  0.        ],\n",
      "        [-1.3413386 ,  0.        ,  0.        ],\n",
      "        [-1.5379725 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.1884362 ,  0.        ,  0.        ],\n",
      "        [-1.5693371 ,  0.        ,  0.        ],\n",
      "        [-1.5521402 ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.5331987 ,  0.        ,  0.        ],\n",
      "        [ 1.933745  ,  0.        ,  0.        ],\n",
      "        [ 2.1952267 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.2674033 ,  0.        ,  0.        ],\n",
      "        [ 0.2674033 ,  0.        ,  0.        ],\n",
      "        [ 0.2674033 ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-1.0507629 ,  0.        ,  0.        ],\n",
      "        [-1.0539656 ,  0.        ,  0.        ],\n",
      "        [-1.0571814 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-0.5120034 ,  0.        ,  0.        ],\n",
      "        [-0.4755162 ,  0.        ,  0.        ],\n",
      "        [-0.44069114,  0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.8655655 ,  0.        ,  0.        ],\n",
      "        [ 1.8655655 ,  0.        ,  0.        ],\n",
      "        [ 1.8655655 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-1.1077944 ,  0.        ,  0.        ],\n",
      "        [ 0.10041763,  0.        ,  0.        ],\n",
      "        [ 0.0634762 ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.00908527,  0.        ,  0.        ],\n",
      "        [-0.274175  ,  0.        ,  0.        ],\n",
      "        [-0.22949481,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-0.17268711,  0.        ,  0.        ],\n",
      "        [-0.17268711,  0.        ,  0.        ],\n",
      "        [-0.17268711,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.9509022 ,  0.        ,  0.        ],\n",
      "        [-0.8767934 ,  0.        ,  0.        ],\n",
      "        [-0.85021305,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-0.86142164,  0.        ,  0.        ],\n",
      "        [-0.86142164,  0.        ,  0.        ],\n",
      "        [-0.86142164,  0.        ,  0.        ]]], dtype=float32), 'active_entries': array([[[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]], dtype=float32), 'sequence_lengths': array([161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161])}\n"
     ]
    }
   ],
   "source": [
    "gen = data_generator(training_processed, 256)  # 使用您的数据和批次大小\n",
    "for data in gen():\n",
    "    print(data)  # 打印看看是否产生了数据\n",
    "    break  # 只查看第一批数据\n",
    "# training_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa712101-c89b-4a28-bc1f-c585717e3986",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': <tf.Tensor: shape=(256, 160, 28), dtype=float32, numpy=\n",
      "array([[[ 0.        , -0.17024788,  1.4222689 , ..., -2.303025  ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ..., -1.2382128 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ..., -0.80969423,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.11241431,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.11241431,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  1.4222689 , ...,  0.11241431,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788,  0.698484  , ...,  1.0822569 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ...,  0.8367583 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ...,  0.17161907,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788,  0.698484  , ...,  0.6140344 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ...,  0.6140344 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ...,  0.6140344 ,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788,  0.698484  , ...,  0.77674645,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ..., -0.4568674 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ..., -0.81083226,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788,  0.698484  , ..., -0.65169066,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ..., -0.65169066,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  0.698484  , ..., -0.65169066,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.        , -0.17024788, -0.7490856 , ..., -1.203208  ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ...,  0.10573742,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.8523539 ,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -1.3175013 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -1.7713143 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788, -0.7490856 , ..., -0.04289438,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 1.        , -0.17024788,  1.4222689 , ...,  0.06825504,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  1.4222689 , ..., -0.08081053,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  1.4222689 , ...,  0.10070597,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 1.        , -0.17024788,  1.4222689 , ..., -0.7567322 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  1.4222689 , ...,  0.3622799 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 1.        , -0.17024788,  1.4222689 , ..., -0.19421305,\n",
      "          0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.        , -0.17024788,  0.698484  , ...,  0.7208016 ,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  0.698484  , ...,  0.98925287,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  0.698484  , ...,  0.16393794,\n",
      "          0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.        , -0.17024788,  0.698484  , ...,  0.03257012,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  0.698484  , ...,  0.03257012,\n",
      "          0.        ,  0.        ],\n",
      "        [ 0.        , -0.17024788,  0.698484  , ...,  0.03257012,\n",
      "          0.        ,  0.        ]]], dtype=float32)>, 'outputs': <tf.Tensor: shape=(256, 160, 3), dtype=float32, numpy=\n",
      "array([[[-1.2382128 ,  0.        ,  0.        ],\n",
      "        [-0.80969423,  0.        ,  0.        ],\n",
      "        [-0.53674585,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.11241431,  0.        ,  0.        ],\n",
      "        [ 0.11241431,  0.        ,  0.        ],\n",
      "        [ 0.11241431,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.8367583 ,  0.        ,  0.        ],\n",
      "        [ 0.17161907,  0.        ,  0.        ],\n",
      "        [ 0.40958458,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.6140344 ,  0.        ,  0.        ],\n",
      "        [ 0.6140344 ,  0.        ,  0.        ],\n",
      "        [ 0.6140344 ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.4568674 ,  0.        ,  0.        ],\n",
      "        [-0.81083226,  0.        ,  0.        ],\n",
      "        [-0.794796  ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-0.65169066,  0.        ,  0.        ],\n",
      "        [-0.65169066,  0.        ,  0.        ],\n",
      "        [-0.65169066,  0.        ,  0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.10573742,  0.        ,  0.        ],\n",
      "        [-0.8523539 ,  0.        ,  0.        ],\n",
      "        [-0.8886876 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [-1.7713143 ,  0.        ,  0.        ],\n",
      "        [-0.04289438,  0.        ,  0.        ],\n",
      "        [-1.2620735 ,  0.        ,  0.        ]],\n",
      "\n",
      "       [[-0.08081053,  0.        ,  0.        ],\n",
      "        [ 0.10070597,  0.        ,  0.        ],\n",
      "        [-0.1315756 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.3622799 ,  0.        ,  0.        ],\n",
      "        [-0.19421305,  0.        ,  0.        ],\n",
      "        [-0.65874773,  0.        ,  0.        ]],\n",
      "\n",
      "       [[ 0.98925287,  0.        ,  0.        ],\n",
      "        [ 0.16393794,  0.        ,  0.        ],\n",
      "        [ 0.7696234 ,  0.        ,  0.        ],\n",
      "        ...,\n",
      "        [ 0.03257012,  0.        ,  0.        ],\n",
      "        [ 0.03257012,  0.        ,  0.        ],\n",
      "        [ 0.03257012,  0.        ,  0.        ]]], dtype=float32)>, 'active_entries': <tf.Tensor: shape=(256, 160, 3), dtype=float32, numpy=\n",
      "array([[[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]],\n",
      "\n",
      "       [[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]], dtype=float32)>, 'sequence_lengths': <tf.Tensor: shape=(256,), dtype=int32, numpy=\n",
      "array([161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161, 161,\n",
      "       161, 161, 161, 161, 161, 161, 161, 161, 161], dtype=int32)>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 22:33:40.316081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22288 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:03:00.0, compute capability: 8.9\n",
      "2024-03-14 22:33:40.316777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22288 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "tf_data_train = convert_to_tf_dataset_via_generator(training_processed, 256)\n",
    "for data in tf_data_train.take(1):  # 只取一个批次\n",
    "    print(data)  # 检查数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea683ed4-7231-490c-a502-1a1438e081cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_predict_actions = True\n",
    "b_use_actions_only = True\n",
    "training_processed = get_processed_data(training_data, b_predict_actions, b_use_actions_only,\n",
    "                                                 b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7b0432f-8dc3-4714-836c-7f368bc60f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_quantiles(data, bins):\n",
    "   \n",
    "    if not isinstance(bins, (int, np.integer)):\n",
    "        raise ValueError(\n",
    "            \"Expected integer 'bins', but got type '{}'.\".format(type(bins))\n",
    "        )\n",
    "    quantiles = np.unique(\n",
    "        np.quantile(\n",
    "            data, np.linspace(0, 1, bins + 1), interpolation=\"lower\"\n",
    "        )\n",
    "    )\n",
    "    bins = len(quantiles) - 1\n",
    "    return quantiles, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a53bef9-05dc-4a2a-8386-bb1ca25d6d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  5.,  5., ..., 10.,  5.,  5.],\n",
       "       [11., 12., 12., ...,  8.,  8.,  8.],\n",
       "       [ 6.,  6.,  6., ...,  7.,  7.,  7.],\n",
       "       ...,\n",
       "       [ 8.,  8., 12., ...,  5.,  5.,  5.],\n",
       "       [12., 11.,  8., ..., 10., 10., 10.],\n",
       "       [ 6.,  4.,  6., ...,  8.,  8.,  8.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_processed['scaled_outputs'][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1dba9fc-dbdc-4ef9-9739-88f6c841a155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9164/2899687007.py:8: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  np.quantile(\n"
     ]
    }
   ],
   "source": [
    "quantiles, bins = _get_quantiles(training_processed['scaled_outputs'][:,:,0], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea5edfe6-8c4c-445d-aa3b-72d6e333d8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  5.,  6.,  7.,  8.,  9., 10., 11., 19.], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79207e94-a532-486a-9e31-cfb0f23d4762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_causal",
   "language": "python",
   "name": "new_causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
