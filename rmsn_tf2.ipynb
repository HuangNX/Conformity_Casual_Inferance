{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25376294-cae6-4b0e-be47-b7fc29cacc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import logging \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from data_process import get_dataset_splits\n",
    "from utils.evaluation_utils import load_data_from_file, write_results_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c6bdaf-5126-4b02-b964-fbdac392c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9dfb6dd-b0a5-49d2-994b-a5a1e0880065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *\n",
    "#打印时间分割线\n",
    "@tf.function\n",
    "def printbar():\n",
    "    ts = tf.timestamp()\n",
    "    today_ts = ts%(24*60*60)\n",
    "\n",
    "    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)\n",
    "    minite = tf.cast((today_ts%3600)//60,tf.int32)\n",
    "    second = tf.cast(tf.floor(today_ts%60),tf.int32)\n",
    "    \n",
    "    def timeformat(m):\n",
    "        if tf.strings.length(tf.strings.format(\"{}\",m))==1:\n",
    "            return(tf.strings.format(\"0{}\",m))\n",
    "        else:\n",
    "            return(tf.strings.format(\"{}\",m))\n",
    "    \n",
    "    timestring = tf.strings.join([timeformat(hour),timeformat(minite),\n",
    "                timeformat(second)],separator = \":\")\n",
    "    tf.print(\"==========\"*8,end = \"\")\n",
    "    tf.print(timestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca1508f-ddeb-47b3-8af9-c08f36e8e6d6",
   "metadata": {},
   "source": [
    "将所有代码转化为单线程，而非调用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afe277-4d5c-4645-a1ca-502132501860",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f597cc7-5a43-41a9-80d6-a7c00c5943a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_covariates\n",
      "(3000, 160, 25)\n",
      "float32\n",
      "previous_treatments\n",
      "(3000, 160, 3)\n",
      "float32\n",
      "covariates\n",
      "(3000, 161, 25)\n",
      "float32\n",
      "treatments\n",
      "(3000, 161, 3)\n",
      "float32\n",
      "sequence_length\n",
      "(3000,)\n",
      "int64\n",
      "outcomes\n",
      "(3000, 161, 1)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data_from_file(\"../fullfeature_fillmean_1000.txt\")\n",
    "# 数据类型转换\n",
    "for key in dataset.keys():\n",
    "    if key!='sequence_length':\n",
    "        dataset[key] = dataset[key].astype(np.float32)\n",
    "    print(key)\n",
    "    print(dataset[key].shape)\n",
    "    print(dataset[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65195d8c-aea7-4c80-b79b-b2449734b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.1, random_state=10)\n",
    "train_index, test_index = next(shuffle_split.split(dataset['covariates'][:, :, 0]))\n",
    "shuffle_split = ShuffleSplit(n_splits=1, test_size=0.11, random_state=10)\n",
    "train_index, val_index = next(shuffle_split.split(dataset['covariates'][train_index, :, 0]))\n",
    "dataset_map = get_dataset_splits(dataset, train_index, val_index, test_index, use_predicted_confounders=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b510c-4248-4d62-9b04-2a415e3cc6bb",
   "metadata": {},
   "source": [
    "# RMSN port"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f203b4-7487-4219-b300-ddb1da2642fd",
   "metadata": {},
   "source": [
    "train_rmsn → rnn_fit → train (RNNModel-->training:get_training_graph-->validation:get_prediction_graph) → model_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8d133-a8e8-469b-8860-b399d5c4eb0e",
   "metadata": {},
   "source": [
    "## train_rmsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb08e94e-ff18-4c87-83c5-4895c9b80125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_rmsn(dataset_map, model_name, b_use_predicted_confounders):###########################\n",
    "# model_name = 'tf2_try'\n",
    "# MODEL_ROOT = os.path.join('results', model_name)\n",
    "MODEL_ROOT = 'results/rmsn_result_test_use_confounders_False'\n",
    "\n",
    "# if not os.path.exists(MODEL_ROOT):\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "# else:\n",
    "#     # Need to delete previously saved model.\n",
    "#     shutil.rmtree(MODEL_ROOT)\n",
    "#     os.mkdir(MODEL_ROOT)\n",
    "#     print(\"Directory \", MODEL_ROOT, \" Created \")\n",
    "\n",
    "# rnn_fit参数设置\n",
    "networks_to_train='propensity_networks'\n",
    "# networks_to_train='encoder'\n",
    "b_use_predicted_confounders=False\n",
    "\n",
    "    # rnn_fit(dataset_map=dataset_map, networks_to_train='propensity_networks', MODEL_ROOT=MODEL_ROOT,\n",
    "    #         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     propensity_generation(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                           b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rnn_fit(networks_to_train='encoder', dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#             b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmsn_mse = rnn_test(dataset_map=dataset_map, MODEL_ROOT=MODEL_ROOT,\n",
    "#                         b_use_predicted_confounders=b_use_predicted_confounders)\n",
    "\n",
    "#     rmse = np.sqrt(np.mean(rmsn_mse)) * 100\n",
    "    # return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9a319-a931-4bb4-8e50-abf614034137",
   "metadata": {},
   "source": [
    "## rnn_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c529f430-9c63-40a1-9216-09d7e2e40b44",
   "metadata": {},
   "source": [
    "### 1.1 基础参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a47dc0bb-17c8-45e5-a95e-0427a826e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "specifications = {\n",
    "     'rnn_propensity_weighted': (0.1, 4, 100, 64, 0.01, 1.0),\n",
    "     'treatment_rnn_action_inputs_only': (0.1, 3, 100, 128, 0.01, 2.0),\n",
    "     'treatment_rnn': (0.1, 4, 100, 64, 0.01, 1.0),}\n",
    "# #####################################################################################\n",
    "# def rnn_fit(dataset_map, networks_to_train, MODEL_ROOT, b_use_predicted_confounders,\n",
    "#             b_use_oracle_confounders=False, b_remove_x1=False):\n",
    "    \n",
    "# Get the correct networks to train\n",
    "if networks_to_train == \"propensity_networks\":\n",
    "    logging.info(\"Training propensity networks\")\n",
    "    # net_names = ['treatment_rnn_action_inputs_only']\n",
    "    net_names = ['treatment_rnn']\n",
    "\n",
    "elif networks_to_train == \"encoder\":\n",
    "    logging.info(\"Training R-MSN encoder\")\n",
    "    net_names = [\"rnn_propensity_weighted\"]\n",
    "\n",
    "elif networks_to_train == \"user_defined\":\n",
    "    logging.info(\"Training user defined network\")\n",
    "    raise NotImplementedError(\"Specify network to use!\")\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unrecognised network type\")\n",
    "\n",
    "    logging.info(\"Running hyperparameter optimisation\")\n",
    "\n",
    "# Experiment name\n",
    "expt_name = \"treatment_effects\"\n",
    "\n",
    "# Possible networks to use along with their activation functions\n",
    "# change hidden layer of rnn_propensity_weighted to tanh\n",
    "activation_map = {'rnn_propensity_weighted': (\"tanh\", 'linear'),\n",
    "                  'rnn_propensity_weighted_logistic': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid')\n",
    "                  }\n",
    "\n",
    "    \n",
    "# Start Running hyperparam opt\n",
    "opt_params = {}\n",
    "# for net_name in net_names:\n",
    "net_name = net_names[0]\n",
    "# Re-run hyperparameter optimisation if parameters are not specified, otherwise train with defined params（如果需要超参优化则跑3次，否则跑一次就行）\n",
    "max_hyperparam_runs = 3 if net_name not in specifications else 1\n",
    "\n",
    "# Pull datasets\n",
    "b_predict_actions = \"treatment_rnn\" in net_name\n",
    "use_truncated_bptt = net_name != \"rnn_model_bptt\" # whether to train with truncated backpropagation through time\n",
    "b_propensity_weight = \"rnn_propensity_weighted\" in net_name\n",
    "b_use_actions_only = \"rnn_action_inputs_only\" in net_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ca44-6b19-4bc0-9691-44d81b5d99b7",
   "metadata": {},
   "source": [
    "### 1.2 gpu设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d30138a-0d8a-4aab-b965-746f99e26960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU with memory growth\n"
     ]
    }
   ],
   "source": [
    "# Setup tensorflow\n",
    "# 检测 GPU 设备\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # set TensorFlow to use all GPU\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        for gpu in gpus:\n",
    "            # set GPU memery growth\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Using GPU with memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        # Changing device settings after the program is running may cause errors\n",
    "        print(e)\n",
    "else:\n",
    "    # if no GPU，using CPU\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b163d5-7cac-4811-9a7b-522085a2de8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992e187-499a-4b46-9953-f70361e681d9",
   "metadata": {},
   "source": [
    "### 1.3 模型参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a9e2cbb-0c0a-409f-a3c6-fdda857fd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start hyperparamter optimisation\n",
    "hyperparam_count = 0\n",
    "# 删掉超参数优化（随机选取超参数）部分\n",
    "spec = specifications[net_name]\n",
    "logging.info(\"Using specifications for {}: {}\".format(net_name, spec))\n",
    "dropout_rate = spec[0]\n",
    "memory_multiplier = spec[1]\n",
    "num_epochs = spec[2]\n",
    "minibatch_size = spec[3]\n",
    "learning_rate = spec[4]\n",
    "max_norm = spec[5]\n",
    "hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "            \n",
    "# hyperparam_opt = train(net_name, expt_name,\n",
    "#                       training_processed, validation_processed, test_processed,\n",
    "#                       dropout_rate, memory_multiplier, num_epochs,\n",
    "#                       minibatch_size, learning_rate, max_norm,\n",
    "#                       use_truncated_bptt,\n",
    "#                       num_features, num_outputs, model_folder,\n",
    "#                       hidden_activation, output_activation,\n",
    "#                       config,\n",
    "#                       \"hyperparam opt: {} of {}\".format(hyperparam_count,\n",
    "#                                                         max_hyperparam_runs))\n",
    "\n",
    "#     hyperparam_count = len(hyperparam_opt.columns)\n",
    "#     if hyperparam_count >= max_hyperparam_runs:\n",
    "#         opt_params[net_name] = hyperparam_opt.T\n",
    "#         break\n",
    "\n",
    "# logging.info(\"Done\")\n",
    "# logging.info(hyperparam_opt.T)\n",
    "\n",
    "# # Flag optimal params\n",
    "# logging.info(opt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0ec6e-32f9-4274-9932-c2b37e0d1e71",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 数据处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fd38d16-3735-489d-ae89-261de81a91a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_processed_data(raw_sim_data,\n",
    "                       b_predict_actions,\n",
    "                       b_use_actions_only,\n",
    "                       b_use_predicted_confounders,\n",
    "                       b_use_oracle_confounders,\n",
    "                       b_remove_x1,\n",
    "                       keep_first_point=False):\n",
    "    \"\"\"\n",
    "    Create formatted data to train both propensity networks and seq2seq architecture\n",
    "\n",
    "    :param raw_sim_data: Data from simulation\n",
    "    :param scaling_params: means/standard deviations to normalise the data to\n",
    "    :param b_predict_actions: flag to package data for propensity network to forecast actions\n",
    "    :param b_use_actions_only:  flag to package data with only action inputs and not covariates\n",
    "    :param b_predict_censoring: flag to package data to predict censoring locations\n",
    "    :return: processed data to train specific network\n",
    "    \"\"\"\n",
    "    horizon = 1\n",
    "    offset = 1\n",
    "\n",
    "    # Continuous values\n",
    "\n",
    "    # Binary application\n",
    "    treatments = raw_sim_data['treatments']\n",
    "    covariates = raw_sim_data['covariates']\n",
    "    dataset_outputs = raw_sim_data['outcomes']\n",
    "    sequence_lengths = raw_sim_data['sequence_length']\n",
    "    \n",
    "    if b_use_predicted_confounders:\n",
    "        predicted_confounders = raw_sim_data['predicted_confounders']\n",
    "\n",
    "    if b_use_oracle_confounders:\n",
    "        predicted_confounders = raw_sim_data['confounders']\n",
    "\n",
    "    num_treatments = treatments.shape[-1]\n",
    "\n",
    "    # Parcelling INPUTS\n",
    "    if b_predict_actions:\n",
    "        if b_use_actions_only:\n",
    "            inputs = treatments\n",
    "            inputs = inputs[:, :-offset, :]\n",
    "\n",
    "            actions = inputs.copy()\n",
    "\n",
    "        else:\n",
    "            # Uses current covariate, to remove confounding effects between action and current value\n",
    "            if (b_use_predicted_confounders):\n",
    "                print (\"Using predicted confounders\")\n",
    "                inputs = np.concatenate([covariates[:, 1:, ], predicted_confounders[:, 1:, ], treatments[:, :-1, ]],\n",
    "                                        axis=2)\n",
    "            else:\n",
    "                inputs = np.concatenate([covariates[:, 1:,], treatments[:, :-1, ]], axis=2)\n",
    "\n",
    "            actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    else:\n",
    "        if (b_use_predicted_confounders):\n",
    "            inputs = np.concatenate([covariates, predicted_confounders, treatments], axis=2)\n",
    "        else:\n",
    "            inputs = np.concatenate([covariates, treatments], axis=2)\n",
    "        \n",
    "        if not keep_first_point:\n",
    "            inputs = inputs[:, 1:, :]\n",
    "\n",
    "        actions = inputs[:, :, -num_treatments:].copy()\n",
    "\n",
    "\n",
    "    # Parcelling OUTPUTS\n",
    "    if b_predict_actions:\n",
    "        outputs = treatments\n",
    "        outputs = outputs[:, 1:, :]\n",
    "\n",
    "    else:\n",
    "        if keep_first_point:\n",
    "            outputs = dataset_outputs\n",
    "        else:\n",
    "            outputs = dataset_outputs[:, 1:, :]\n",
    "\n",
    "\n",
    "    # Set array alignment\n",
    "    sequence_lengths = np.array([i - 1 for i in sequence_lengths]) # everything shortens by 1\n",
    "\n",
    "    # Remove any trajectories that are too short\n",
    "    inputs = inputs[sequence_lengths > 0, :, :]\n",
    "    outputs = outputs[sequence_lengths > 0, :, :]\n",
    "    sequence_lengths = sequence_lengths[sequence_lengths > 0]\n",
    "    actions = actions[sequence_lengths > 0, :, :]\n",
    "\n",
    "    # Add active entires\n",
    "    active_entries = np.zeros(outputs.shape, dtype=np.float32)\n",
    "\n",
    "    for i in range(sequence_lengths.shape[0]):\n",
    "        sequence_length = int(sequence_lengths[i])\n",
    "\n",
    "        if not b_predict_actions:\n",
    "            for k in range(horizon):\n",
    "                #include the censoring point too, but ignore future shifts that don't exist\n",
    "                active_entries[i, :sequence_length-k, k] = 1\n",
    "        else:\n",
    "            active_entries[i, :sequence_length, :] = 1\n",
    "\n",
    "    return {'outputs': outputs,  # already scaled\n",
    "            'scaled_inputs': inputs,\n",
    "            'scaled_outputs': outputs,\n",
    "            'actions': actions,\n",
    "            'sequence_lengths': sequence_lengths,\n",
    "            'active_entries': active_entries\n",
    "            }\n",
    "\n",
    "\n",
    "def convert_to_tf_dataset(dataset_map, minibatch_size):\n",
    "    key_map = {'inputs': dataset_map['scaled_inputs'],\n",
    "               'outputs': dataset_map['scaled_outputs'],\n",
    "               'active_entries': dataset_map['active_entries'],\n",
    "               'sequence_lengths': dataset_map['sequence_lengths']}\n",
    "\n",
    "    if 'propensity_weights' in dataset_map:\n",
    "        key_map['propensity_weights'] = dataset_map['propensity_weights']\n",
    "\n",
    "    if 'initial_states' in dataset_map:\n",
    "        key_map['initial_states'] = dataset_map['initial_states']\n",
    "\n",
    "    #from_tensor_slices:切片; shuffle:随机打乱; batch:批次组合; prefetch:提前准备（预取）数据\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(key_map)\\\n",
    "                .shuffle(buffer_size = 1000).batch(minibatch_size) \\\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8354ab9-1da0-44bd-8f3d-e5781b698bb3",
   "metadata": {},
   "source": [
    "### 1.4 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e895cc5-0d22-4d39-bb94-da72f4631cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = dataset_map['training_data']\n",
    "validation_data = dataset_map['validation_data']\n",
    "test_data = dataset_map['test_data']\n",
    "\n",
    " # Extract only relevant trajs and shift data\n",
    "b_use_oracle_confounders = False; b_remove_x1 = False\n",
    "training_processed = get_processed_data(training_data, b_predict_actions,\n",
    "                                             b_use_actions_only, b_use_predicted_confounders,\n",
    "                                             b_use_oracle_confounders, b_remove_x1)\n",
    "validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                               b_use_actions_only, b_use_predicted_confounders,\n",
    "                                               b_use_oracle_confounders, b_remove_x1)\n",
    "test_processed = get_processed_data(test_data, b_predict_actions,\n",
    "                                         b_use_actions_only, b_use_predicted_confounders,\n",
    "                                         b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "num_features = training_processed['scaled_inputs'].shape[-1]\n",
    "num_outputs = training_processed['scaled_outputs'].shape[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e986ae9d-9e7c-4aaf-8a21-36e28d733ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load propensity weights if they exist\n",
    "if b_propensity_weight:\n",
    "    if net_name == 'rnn_propensity_weighted_den_only':\n",
    "        # use un-stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_den_only.npy\"))\n",
    "    elif net_name == \"rnn_propensity_weighted_logistic\":\n",
    "        # Use logistic regression weights\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "        tmp = np.load(os.path.join(MODEL_ROOT, \"propensity_scores_logistic.npy\"))\n",
    "        propensity_weights = tmp[:propensity_weights.shape[0], :, :]\n",
    "    else:\n",
    "        # use stabilised IPTWs generated by propensity networks\n",
    "        propensity_weights = np.load(os.path.join(MODEL_ROOT, \"propensity_scores.npy\"))\n",
    "\n",
    "    logging.info(\"Net name = {}. Mean-adjusting!\".format(net_name))\n",
    "\n",
    "    propensity_weights /= propensity_weights.mean()\n",
    "\n",
    "    training_processed['propensity_weights'] = np.array(propensity_weights, dtype='float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffbddf8b-03ef-4bff-b872-e771df271067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to tensorflow format\n",
    "tf_data_train = convert_to_tf_dataset(training_processed, minibatch_size)\n",
    "tf_data_valid = convert_to_tf_dataset(validation_processed, minibatch_size)\n",
    "tf_data_test = convert_to_tf_dataset(test_processed, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5226159-cc7b-49e6-979a-cf5edc0cbc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec={'inputs': TensorSpec(shape=(None, 160, 28), dtype=tf.float32, name=None), 'outputs': TensorSpec(shape=(None, 160, 3), dtype=tf.float32, name=None), 'active_entries': TensorSpec(shape=(None, 160, 3), dtype=tf.float32, name=None), 'sequence_lengths': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bc09d-c919-4a2b-b5ff-96fd4fc8a4fd",
   "metadata": {},
   "source": [
    "## core routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c7346-384d-4d49-982f-f378b06198da",
   "metadata": {},
   "source": [
    "### def_train修改\n",
    "1. 去除session会话\n",
    "2. 各步骤拆分与重构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af5357-82fc-4ff8-ad1c-c754c2b57efd",
   "metadata": {},
   "source": [
    "#### 2.1 参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ec4a1-be8c-484a-aa71-7a8ba11660c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(net_name,\n",
    "#           expt_name,\n",
    "#           training_dataset, validation_dataset, test_dataset,\n",
    "#           dropout_rate,\n",
    "#           memory_multiplier,\n",
    "#           num_epochs,\n",
    "#           minibatch_size,\n",
    "#           learning_rate,\n",
    "#           max_norm,\n",
    "#           use_truncated_bptt,\n",
    "#           num_features,\n",
    "#           num_outputs,\n",
    "#           model_folder,\n",
    "#           hidden_activation,\n",
    "#           output_activation,\n",
    "#           tf_config,\n",
    "#           additonal_info=\"\",\n",
    "#           b_use_state_initialisation=False,\n",
    "#           b_use_seq2seq_feedback=False,\n",
    "#           b_use_seq2seq_training_mode=False,\n",
    "#           adapter_multiplier=0,\n",
    "#           b_use_memory_adapter=False,\n",
    "#           verbose=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a0ab33a-749c-45e7-af22-ffaeac0425de",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epochs = 1\n",
    "hidden_layer_size = int(memory_multiplier * num_features)\n",
    "\n",
    "b_use_state_initialisation = False\n",
    "if b_use_state_initialisation:\n",
    "    full_state_size = int(training_dataset['initial_states'].shape[-1])\n",
    "    adapter_size = adapter_multiplier * full_state_size\n",
    "else:\n",
    "    adapter_size = 0\n",
    "    \n",
    "model_parameters = {'net_name': net_name,\n",
    "                    'experiment_name': expt_name,\n",
    "                    'training_dataset': tf_data_train,\n",
    "                    'validation_dataset': tf_data_valid,\n",
    "                    'test_dataset': tf_data_test,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'input_size': num_features,\n",
    "                    'output_size': num_outputs,\n",
    "                    'hidden_layer_size': hidden_layer_size,\n",
    "                    'num_epochs': 10, # for test\n",
    "                    'minibatch_size': minibatch_size,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'max_norm': max_norm,\n",
    "                    'model_folder': model_folder,\n",
    "                    'hidden_activation': hidden_activation,\n",
    "                    'output_activation': output_activation,\n",
    "                    'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "                    'softmax_size': 0, #not used in this paper, but allows for categorical actions\n",
    "                    'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72be6-75cb-4797-bd27-a556bbb21222",
   "metadata": {},
   "source": [
    "#### 2.2 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8994abb-e797-4f8e-ac96-d75e9a5a67d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \n",
    "    # Data params\n",
    "    #training_data = None if 'training_dataset' not in params else params['training_dataset']\n",
    "    #validation_data = None if 'validation_dataset' not in params else params['validation_dataset']\n",
    "    #test_data = None if 'test_dataset' not in params else params['test_dataset']\n",
    "    input_size = params['input_size']\n",
    "    output_size = params['output_size']\n",
    "\n",
    "    # Network params\n",
    "    net_name = params['net_name']\n",
    "    softmax_size = params['softmax_size']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    hidden_layer_size = params['hidden_layer_size']\n",
    "    memory_activation_type = params['hidden_activation']\n",
    "    output_activation_type = params['output_activation']\n",
    "    #initial_states = None\n",
    "    # input layer\n",
    "    inputs = layers.Input(shape=(None,input_size), dtype=tf.float32)\n",
    "    # define initial states \n",
    "    initial_h =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_h')\n",
    "    initial_c =layers.Input(shape=(hidden_layer_size,), dtype=tf.float32, name='initial_c')\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm, state_h, state_c = layers.LSTM(hidden_layer_size, activation=memory_activation_type, \n",
    "                       return_sequences=True, return_state=True, dropout=dropout_rate)(inputs, initial_state=[initial_h, initial_c])\n",
    "\n",
    "    # flattened_lstm = layers.Flatten()(lstm)\n",
    "\n",
    "    # Seq2Seq(if need)\n",
    "    use_seq2seq_feedback = False\n",
    "    if use_seq2seq_feedback:\n",
    "        logits = lstm\n",
    "    else:\n",
    "        # linear output layer\n",
    "        logits = layers.Dense(output_size)(lstm)\n",
    "\n",
    "    # Softmax\n",
    "    if softmax_size != 0:\n",
    "        logits_reshaped = layers.Reshape((-1, output_size))(logits)\n",
    "        core_outputs, softmax_outputs = tf.split(logits_reshaped, [output_size - softmax_size, softmax_size], axis=-1)\n",
    "        core_activated = layers.Activation(output_activation_type)(core_outputs)\n",
    "        softmax_activated = layers.Softmax(axis=-1)(softmax_outputs)\n",
    "        outputs = layers.Concatenate(axis=-1)([core_activated, softmax_activated])\n",
    "    else:\n",
    "        outputs = layers.Activation(output_activation_type)(logits)\n",
    "\n",
    "    # construct model\n",
    "    model = models.Model(inputs=[inputs, initial_h, initial_c], outputs=[outputs, state_h, state_c], name=net_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f49d3e37-ebdf-44b6-8913-a06edcc81512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"treatment_rnn\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 28)]           0         []                            \n",
      "                                                                                                  \n",
      " initial_h (InputLayer)      [(None, 112)]                0         []                            \n",
      "                                                                                                  \n",
      " initial_c (InputLayer)      [(None, 112)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, None, 112),          63168     ['input_1[0][0]',             \n",
      "                              (None, 112),                           'initial_h[0][0]',           \n",
      "                              (None, 112)]                           'initial_c[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 3)              339       ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None, 3)              0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 63507 (248.07 KB)\n",
      "Trainable params: 63507 (248.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 创建模型\n",
    "tf.keras.backend.clear_session()\n",
    "model = create_model(model_parameters)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f8973ae-f6e6-4875-8fd7-93692b9976f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_parameters['input_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7408a-29b0-4032-852e-d79f58f5ed9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "    Common training routine to all RNN models_without_confounders - seq2seq + standard\n",
    "    \"\"\"\n",
    "\n",
    "    min_epochs = 1\n",
    "    # Setup default hidden layer size\n",
    "    hidden_layer_size = int(memory_multiplier * num_features)\n",
    "\n",
    "    if b_use_state_initialisation:\n",
    "\n",
    "        full_state_size = int(training_dataset['initial_states'].shape[-1])\n",
    "\n",
    "        adapter_size = adapter_multiplier * full_state_size\n",
    "\n",
    "    else:\n",
    "        adapter_size = 0\n",
    "\n",
    "        # Training simulation\n",
    "    model_parameters = {'net_name': net_name,\n",
    "                        'experiment_name': expt_name,\n",
    "                        'training_dataset': tf_data_train,\n",
    "                        'validation_dataset': tf_data_valid,\n",
    "                        'test_dataset': tf_data_test,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'input_size': num_features,\n",
    "                        'output_size': num_outputs,\n",
    "                        'hidden_layer_size': hidden_layer_size,\n",
    "                        'num_epochs': num_epochs,\n",
    "                        'minibatch_size': minibatch_size,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'max_norm': max_norm,\n",
    "                        'model_folder': model_folder,\n",
    "                        'hidden_activation': hidden_activation,\n",
    "                        'output_activation': output_activation,\n",
    "                        'backprop_length': 60,  # backprop over 60 timesteps for truncated backpropagation through time\n",
    "                        'softmax_size': 0, #not used in this paper, but allows for categorical actions\n",
    "                        'performance_metric': 'xentropy' if output_activation == 'sigmoid' else 'mse',\n",
    "                        'use_seq2seq_feedback': b_use_seq2seq_feedback,\n",
    "                        'use_seq2seq_training_mode': b_use_seq2seq_training_mode,\n",
    "                        'use_memory_adapter': b_use_memory_adapter,\n",
    "                        'memory_adapter_size': adapter_size}\n",
    "\n",
    "    # Get the right model\n",
    "    model = RnnModel(model_parameters)\n",
    "    serialisation_name = model.serialisation_name\n",
    "\n",
    "    if helpers.hyperparameter_result_exists(model_folder, net_name, serialisation_name):\n",
    "        logging.warning(\"Combination found: skipping {}\".format(serialisation_name))\n",
    "        return helpers.load_hyperparameter_results(model_folder, net_name)\n",
    "\n",
    "    training_handles = model.get_training_graph(use_truncated_bptt=use_truncated_bptt,\n",
    "                                                b_use_state_initialisation=b_use_state_initialisation)\n",
    "    validation_handles = model.get_prediction_graph(use_validation_set=True, with_dropout=False,\n",
    "                                                    b_use_state_initialisation=b_use_state_initialisation)\n",
    "\n",
    "    # Start optimising\n",
    "    num_minibatches = int(np.ceil(training_dataset['scaled_inputs'].shape[0] / model_parameters['minibatch_size']))\n",
    "\n",
    "    i = 1\n",
    "    epoch_count = 1\n",
    "    step_count = 1\n",
    "    min_loss = np.inf\n",
    "    with sess.as_default():\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        optimisation_summary = pd.Series([])\n",
    "\n",
    "        while True:\n",
    "            #for step_count in tqdm(range(num_minibatches), desc=f\"Epoch {epoch_count}\"):\n",
    "            try:\n",
    "                # loss, _ = sess.run([training_handles['loss'],\n",
    "                #                     training_handles['optimiser']])\n",
    "                loss, _, numerator = sess.run([training_handles['loss'],\n",
    "                                    training_handles['optimiser'],\n",
    "                                    training_handles['numerator']])\n",
    "\n",
    "                # rango added - tensorflow debugger 2023.10.20###################\n",
    "                # if sess.should_stop():\n",
    "                #     break  # NaN or Inf occurred.\n",
    "                # ###############################################################\n",
    "\n",
    "                # Flog output\n",
    "                if (verbose == True):\n",
    "                    logging.info(\"Epoch {} | iteration = {} of {}, loss = {} | loss_numerator = {} | net = {} | info = {}\".format(\n",
    "                        epoch_count,\n",
    "                        step_count,\n",
    "                        num_minibatches,\n",
    "                        loss,\n",
    "                        numerator,\n",
    "                        model.net_name,\n",
    "                        additonal_info))\n",
    "\n",
    "                if step_count == num_minibatches:\n",
    "\n",
    "                    # Reinit datasets\n",
    "                    sess.run(validation_handles['initializer'])\n",
    "\n",
    "                    means = []\n",
    "                    UBs = []\n",
    "                    LBs = []\n",
    "                    while True:\n",
    "                        try:\n",
    "                            mean, upper_bound, lower_bound = sess.run([validation_handles['mean'],\n",
    "                                                                       validation_handles['upper_bound'],\n",
    "                                                                       validation_handles['lower_bound']])\n",
    "\n",
    "                            means.append(mean)\n",
    "                            UBs.append(upper_bound)\n",
    "                            LBs.append(lower_bound)\n",
    "                        except tf.errors.OutOfRangeError:\n",
    "                            break\n",
    "\n",
    "                    means = np.concatenate(means, axis=0)\n",
    "\n",
    "                    \"\"\"\n",
    "                    means = np.concatenate(means, axis=0)*training_dataset['output_stds'] \\\n",
    "                            + training_dataset['output_means']\n",
    "                    UBs = np.concatenate(UBs, axis=0)*training_dataset['output_stds'] \\\n",
    "                          + training_dataset['output_means']\n",
    "                    LBs = np.concatenate(LBs, axis=0)*training_dataset['output_stds'] \\\n",
    "                          + training_dataset['output_means']\n",
    "                    \"\"\"\n",
    "\n",
    "\n",
    "                    active_entries = validation_dataset['active_entries']\n",
    "                    output = validation_dataset['outputs']\n",
    "\n",
    "                    if model_parameters['performance_metric'] == \"mse\":\n",
    "                        validation_loss = np.sum((means - output)**2 * active_entries) / np.sum(active_entries)\n",
    "                        #logging.info(\"Epoch {} Detection| Means= {} | Output = {}\".format(epoch_count, means, output))\n",
    "\n",
    "                    elif model_parameters['performance_metric'] == \"xentropy\":\n",
    "                        _, _,features_size = output.shape\n",
    "                        partition_idx = features_size\n",
    "\n",
    "                        # Do binary first\n",
    "                        validation_loss = np.sum((output[:, :, :partition_idx] * -np.log(means[:, :, :partition_idx] + 1e-8)\n",
    "                                                 + (1 - output[:, :, :partition_idx]) * -np.log(1 - means[:, :, :partition_idx] + 1e-8))\n",
    "                                                 * active_entries[:, :, :partition_idx]) \\\n",
    "                                          / np.sum(active_entries[:, :, :partition_idx])\n",
    "\n",
    "                    optimisation_summary[epoch_count] = validation_loss\n",
    "\n",
    "                    # Compute validation loss\n",
    "                    if (verbose == True):\n",
    "                        logging.info(\"Epoch {} Summary| Validation loss = {} | net = {} | info = {}\".format(\n",
    "                            epoch_count,\n",
    "                            validation_loss,\n",
    "                            model.net_name,\n",
    "                            additonal_info))\n",
    "\n",
    "                    if np.isnan(validation_loss):\n",
    "                        logging.warning(\"NAN Loss found, terminating routine\")\n",
    "                        break\n",
    "\n",
    "                    # Save model and loss trajectories\n",
    "                    if validation_loss < min_loss and epoch_count > min_epochs:\n",
    "                        cp_name = serialisation_name + \"_optimal\"\n",
    "                        helpers.save_network(sess, model_folder, cp_name, optimisation_summary)\n",
    "                        min_loss = validation_loss\n",
    "\n",
    "                    # Update\n",
    "                    epoch_count += 1\n",
    "                    step_count = 0\n",
    "\n",
    "                step_count += 1\n",
    "                i += 1\n",
    "\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        # Save final\n",
    "        cp_name = serialisation_name + \"_final\"\n",
    "        helpers.save_network(sess, model_folder, cp_name, optimisation_summary)\n",
    "        helpers.add_hyperparameter_results(optimisation_summary, model_folder, net_name, serialisation_name)\n",
    "\n",
    "        hyperparam_df = helpers.load_hyperparameter_results(model_folder, net_name)\n",
    "\n",
    "        logging.info(\"Terminated at iteration {}\".format(i))\n",
    "\n",
    "    return hyperparam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8715d6f0-ef02-461b-aa11-534df9297b19",
   "metadata": {},
   "source": [
    "### 2.3 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118e472-7b2b-499e-a26f-36e12ab81c60",
   "metadata": {},
   "source": [
    "#### 自定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4705a289-2e81-4828-8f9c-932f701fcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(losses.Loss):\n",
    "    def __init__(self, params, name=\"custom_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.performance_metric = params['performance_metric']\n",
    "        # self.weights = params['weights']\n",
    "        # self.active_entries = params['active_entries']\n",
    "\n",
    "    def train_call(self, y_true, y_pred, active_entries, weights):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def valid_call(self, y_true, y_pred, active_entries):\n",
    "        if self.performance_metric == \"mse\":\n",
    "            loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries ) \\\n",
    "                   / tf.reduce_sum(active_entries)\n",
    "        elif self.performance_metric == \"xentropy\":\n",
    "            loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                  (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                  * active_entries) / tf.reduce_sum(active_entries)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "        return loss\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"performance_metric\": self.performance_metric})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413a36a1-7f02-4dbb-aed7-11240388c98f",
   "metadata": {},
   "source": [
    "#### 训练步骤和验证步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f283750-1b64-426d-bc8f-ab1d25f5d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=model_parameters['learning_rate'])\n",
    "loss_func = CustomLoss(model_parameters)\n",
    "\n",
    "train_loss = metrics.Mean(name='train_loss')\n",
    "train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "valid_loss = metrics.Mean(name='valid_loss')\n",
    "valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, inputs, outputs, active_entries, weights):\n",
    "    if weights is None:\n",
    "        weights = tf.constant(1.0)\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss.update_state(loss)\n",
    "    train_metric.update_state(outputs, predictions)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(model, inputs, outputs, active_entries):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_func.valid_call(outputs, predictions, active_entries)\n",
    "    valid_loss.update_state(loss)\n",
    "    valid_metric.update_state(outputs, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed826629-9926-4112-b4a2-b3ea259f9be5",
   "metadata": {},
   "source": [
    "#### 训练模型函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b61247a1-98fd-4fd4-8f87-24f998f82176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, ds_train, ds_valid, epochs):\n",
    "    # optimisation_summary = pd.Series([])\n",
    "    for epoch in tf.range(1, epochs+1):\n",
    "        \n",
    "        for data in ds_train:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "        for data in ds_valid:\n",
    "            weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "            valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "        # 同样的日志和状态重置操作\n",
    "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            printbar()\n",
    "            tf.print(tf.strings.format(logs,\n",
    "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
    "            tf.print(\"\")\n",
    "            \n",
    "        train_loss.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        train_metric.reset_states()\n",
    "        valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "192bf221-bc75-441a-89e0-e74cde192c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModule(tf.Module):\n",
    "    def __init__(self, params, name=None):\n",
    "        super(TrainModule, self).__init__(name=name)\n",
    "        with self.name_scope:  #相当于with tf.name_scope(\"demo_module\")\n",
    "            self.epochs = params['num_epochs']\n",
    "            self.ds_train = params['training_dataset']\n",
    "            self.ds_valid = params['validation_dataset']\n",
    "            self.ds_test = params['test_dataset']\n",
    "            self.optimizer = optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "            self.loss_func = CustomLoss(params)\n",
    "\n",
    "            self.train_loss = metrics.Mean(name='train_loss')\n",
    "            self.train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "\n",
    "            self.valid_loss = metrics.Mean(name='valid_loss')\n",
    "            self.valid_metric = metrics.MeanSquaredError(name='valid_mse')\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, model, inputs, outputs, active_entries, weights):\n",
    "        if weights is None:\n",
    "            weights = tf.constant(1.0)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(inputs, training=True)\n",
    "            loss = self.loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        self.train_loss.update_state(loss)\n",
    "        self.train_metric.update_state(outputs, predictions)\n",
    "\n",
    "    @tf.function\n",
    "    def valid_step(self, model, inputs, outputs, active_entries):\n",
    "        predictions = model(inputs, training=False)\n",
    "        loss = self.loss_func.valid_call(outputs, predictions, active_entries)\n",
    "        self.valid_loss.update_state(loss)\n",
    "        self.valid_metric.update_state(outputs, predictions)\n",
    "    \n",
    "    def train_model(self, model):\n",
    "        # optimisation_summary = pd.Series([])\n",
    "        for epoch in tf.range(1, self.epochs+1):\n",
    "\n",
    "            for data in self.ds_train:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.train_step(model, data['inputs'], data['outputs'], data['active_entries'], weights)\n",
    "\n",
    "            for data in self.ds_valid:\n",
    "                weights = data['propensity_weights'] if 'propensity_weights' in data else None\n",
    "                self.valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "            # optimisation_summary[epoch] = valid_loss\n",
    "\n",
    "            # 同样的日志和状态重置操作\n",
    "            logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
    "\n",
    "            if epoch%1 ==0:\n",
    "                printbar()\n",
    "                tf.print(tf.strings.format(logs,\n",
    "                (epoch,self.train_loss.result(),self.train_metric.result(),self.valid_loss.result(),self.valid_metric.result())))\n",
    "                tf.print(\"\")\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.valid_loss.reset_states()\n",
    "            self.train_metric.reset_states()\n",
    "            self.valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4849e3b-b60b-47af-a82a-c493c5018995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================14:04:03\n",
      "Epoch=1,Loss:0.499527872,Accuracy:0.550320148,Valid Loss:0.516167939,Valid Accuracy:0.517898262\n",
      "\n",
      "================================================================================14:04:04\n",
      "Epoch=2,Loss:0.475318611,Accuracy:0.527793229,Valid Loss:0.506939232,Valid Accuracy:0.508185446\n",
      "\n",
      "================================================================================14:04:04\n",
      "Epoch=3,Loss:0.465685338,Accuracy:0.526379406,Valid Loss:0.510615706,Valid Accuracy:0.511348367\n",
      "\n",
      "================================================================================14:04:05\n",
      "Epoch=4,Loss:0.45916155,Accuracy:0.529349089,Valid Loss:0.516534448,Valid Accuracy:0.510599077\n",
      "\n",
      "================================================================================14:04:06\n",
      "Epoch=5,Loss:0.454828203,Accuracy:0.528462529,Valid Loss:0.516691566,Valid Accuracy:0.520721555\n",
      "\n",
      "================================================================================14:04:06\n",
      "Epoch=6,Loss:0.482320428,Accuracy:0.559840798,Valid Loss:0.525589466,Valid Accuracy:0.525901377\n",
      "\n",
      "================================================================================14:04:07\n",
      "Epoch=7,Loss:0.468005776,Accuracy:0.529510081,Valid Loss:0.542681396,Valid Accuracy:0.534254\n",
      "\n",
      "================================================================================14:04:08\n",
      "Epoch=8,Loss:0.457480669,Accuracy:0.533512414,Valid Loss:0.51867938,Valid Accuracy:0.512872338\n",
      "\n",
      "================================================================================14:04:09\n",
      "Epoch=9,Loss:0.450703472,Accuracy:0.534296751,Valid Loss:0.513790131,Valid Accuracy:0.515044034\n",
      "\n",
      "================================================================================14:04:09\n",
      "Epoch=10,Loss:0.443599194,Accuracy:0.533094049,Valid Loss:0.527673125,Valid Accuracy:0.528042316\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Train = TrainModule(model_parameters)\n",
    "Train.train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9032fff3-d7d8-42f4-b6ef-5d1f2114c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================12:15:12\n",
      "Epoch=1,Loss:-1.04218924,Accuracy:0.315503478,Valid Loss:-1.6271013,Valid Accuracy:0.256319433\n",
      "\n",
      "================================================================================12:15:13\n",
      "Epoch=2,Loss:-1.8213017,Accuracy:0.25058189,Valid Loss:-1.92948687,Valid Accuracy:0.243127644\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=3,Loss:-1.99545646,Accuracy:0.245146066,Valid Loss:-2.00789499,Valid Accuracy:0.237991899\n",
      "\n",
      "================================================================================12:15:14\n",
      "Epoch=4,Loss:-2.0445447,Accuracy:0.242646903,Valid Loss:-2.07682967,Valid Accuracy:0.236074388\n",
      "\n",
      "================================================================================12:15:15\n",
      "Epoch=5,Loss:-2.07054353,Accuracy:0.241571859,Valid Loss:-2.07036352,Valid Accuracy:0.23433511\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=6,Loss:-2.08658314,Accuracy:0.24051398,Valid Loss:-2.02451658,Valid Accuracy:0.234643\n",
      "\n",
      "================================================================================12:15:16\n",
      "Epoch=7,Loss:-2.08822966,Accuracy:0.239788935,Valid Loss:-2.04530382,Valid Accuracy:0.233269155\n",
      "\n",
      "================================================================================12:15:17\n",
      "Epoch=8,Loss:-2.10269213,Accuracy:0.239260048,Valid Loss:-2.09449148,Valid Accuracy:0.231957987\n",
      "\n",
      "================================================================================12:15:18\n",
      "Epoch=9,Loss:-2.10704803,Accuracy:0.238550946,Valid Loss:-2.07969904,Valid Accuracy:0.231935427\n",
      "\n",
      "================================================================================12:15:19\n",
      "Epoch=10,Loss:-2.12130713,Accuracy:0.238038853,Valid Loss:-2.11661029,Valid Accuracy:0.231317282\n",
      "\n",
      "================================================================================12:15:24\n",
      "Epoch=1,Loss:-1.21397805,Accuracy:0.299676478,Valid Loss:-1.7329247,Valid Accuracy:0.245012134\n",
      "\n",
      "================================================================================12:15:25\n",
      "Epoch=2,Loss:-1.89918232,Accuracy:0.247787476,Valid Loss:-2.00681877,Valid Accuracy:0.239056319\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=3,Loss:-2.02381587,Accuracy:0.244440153,Valid Loss:-2.01860476,Valid Accuracy:0.236832559\n",
      "\n",
      "================================================================================12:15:26\n",
      "Epoch=4,Loss:-2.0467248,Accuracy:0.242398903,Valid Loss:-1.99007571,Valid Accuracy:0.235698864\n",
      "\n",
      "================================================================================12:15:27\n",
      "Epoch=5,Loss:-2.06792665,Accuracy:0.240782276,Valid Loss:-2.07615876,Valid Accuracy:0.23492761\n",
      "\n",
      "================================================================================12:15:28\n",
      "Epoch=6,Loss:-2.08954358,Accuracy:0.240094557,Valid Loss:-2.09414053,Valid Accuracy:0.233243883\n",
      "\n",
      "================================================================================12:15:29\n",
      "Epoch=7,Loss:-2.10589767,Accuracy:0.239126161,Valid Loss:-2.08491945,Valid Accuracy:0.233909249\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=8,Loss:-2.10808825,Accuracy:0.238345563,Valid Loss:-2.10724974,Valid Accuracy:0.23198238\n",
      "\n",
      "================================================================================12:15:30\n",
      "Epoch=9,Loss:-2.11758161,Accuracy:0.238061085,Valid Loss:-2.09254766,Valid Accuracy:0.231905296\n",
      "\n",
      "================================================================================12:15:31\n",
      "Epoch=10,Loss:-2.12772536,Accuracy:0.237541556,Valid Loss:-2.09236026,Valid Accuracy:0.231176525\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_names = ['model1','model2']\n",
    "for name in net_names:\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = create_model(model_parameters, name)\n",
    "    # train_model(model, tf_data_train, tf_data_valid, 10)\n",
    "    Train = TrainModule(model_parameters)\n",
    "    Train.train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660dcda-5d51-47ae-a57e-3570fe4c5eca",
   "metadata": {},
   "source": [
    "### 2.4 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca9b3e36-cb8b-4208-8e4d-a33d28fbc990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/savedmodel_try/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: results/tf2_try/savedmodel_try/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export saved model.\n"
     ]
    }
   ],
   "source": [
    "path = 'results/tf2_try/savedmodel_try'\n",
    "model.save(path, save_format = 'tf')\n",
    "print('export saved model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12fb4e51-2e73-43bb-add4-fae0b01b772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = models.load_model(path, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d54a8-ba90-4260-9ffb-9b12532697b5",
   "metadata": {},
   "source": [
    "### 2.5 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f9a8e93-6b0f-440f-b46d-94de062a6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义模型评估函数\n",
    "def evaluate_model(model, ds_test):\n",
    "    total_loss = 0\n",
    "    total_metric = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 遍历数据集中的每个批次\n",
    "    for data in ds_test:\n",
    "        valid_step(model, data['inputs'], data['outputs'], data['active_entries'])\n",
    "        total_loss += valid_loss.result().numpy()\n",
    "        total_metric += valid_metric.result().numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 重置状态\n",
    "        valid_loss.reset_states()\n",
    "        valid_metric.reset_states()\n",
    "\n",
    "    # 计算整个数据集的平均损失和指标\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_metric = total_metric / num_batches\n",
    "    return avg_loss, avg_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6989a47-7ac5-4e66-9faf-ad79db5a5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: -2.254194164276123, Test Accuracy: 0.22184113562107086\n",
      "5/5 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = evaluate_model(model, tf_data_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "input_data_for_prediction = tf_data_test.map(lambda x: x['inputs'])\n",
    "predictions = model_loaded.predict(input_data_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9ee42e19-e360-49bb-9fda-3c35c119e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 160, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a3425-92cd-4780-96a5-d3ab5bc8e876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Propensitry Generation\n",
    "def propensity_generation(dataset_map, MODEL_ROOT, b_use_predicted_confounders, b_use_all_data=False,\n",
    "                          b_use_oracle_confounders=False, b_remove_x1=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0435e9e2-8e77-4b4a-8b6c-f734143bc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rmsn.libs.model_process as model_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8fb8d8-42a6-4321-b7a8-6fe4a0293f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/rmsn_result_test_use_confounders_True/treatment_rnn_action_inputs_only/treatment_rnn_action_inputs_only.csv\n",
      "results/rmsn_result_test_use_confounders_True/treatment_rnn/treatment_rnn.csv\n"
     ]
    }
   ],
   "source": [
    "# 参数设置\n",
    "action_inputs_only = model_process.load_optimal_parameters(net_name='treatment_rnn_action_inputs_only', MODEL_ROOT=MODEL_ROOT)\n",
    "action_w_trajectory_inputs = model_process.load_optimal_parameters(net_name='treatment_rnn', MODEL_ROOT=MODEL_ROOT)\n",
    "\n",
    "# Generate propensity weights for validation data as well - used for MSM which is calibrated on train + valid data\n",
    "b_with_validation = False\n",
    "# Generate non-stabilised IPTWs (default false)\n",
    "b_denominator_only = False\n",
    "b_use_predicted_confounders = True\n",
    "b_use_all_data=False\n",
    "b_use_oracle_confounders=False\n",
    "b_remove_x1=False\n",
    "\n",
    "# Config + activation functions\n",
    "activation_map = {'rnn_propensity_weighted': (\"elu\", 'linear'),\n",
    "                  'rnn_model': (\"elu\", 'linear'),\n",
    "                  'rnn_model_bptt': (\"elu\", 'linear'),\n",
    "                  'treatment_rnn': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  'treatment_rnn_action_inputs_only_softmax': (\"tanh\", 'sigmoid'),\n",
    "                  }\n",
    "\n",
    "configs = {'action_num': action_inputs_only,\n",
    "           'action_den': action_w_trajectory_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d86ece3-4f82-4587-bd20-003ca45d3a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if b_use_all_data:\n",
    "    training_data = dataset_map\n",
    "    validation_data = dataset_map\n",
    "    test_data = None\n",
    "else:\n",
    "    training_data = dataset_map['training_data']\n",
    "    validation_data = dataset_map['validation_data']\n",
    "    test_data = dataset_map['test_data']\n",
    "    \n",
    "if b_with_validation:\n",
    "    for k in training_data:\n",
    "        training_data[k] = np.concatenate([training_data[k], validation_data[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51801536-64ee-4238-a803-694c3569d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def get_predictions(config):\n",
    "\n",
    "    net_name = config[0]\n",
    "    serialisation_name = config[-1]\n",
    "\n",
    "    hidden_activation, output_activation = activation_map[net_name]\n",
    "\n",
    "    # Pull datasets\n",
    "    b_predict_actions = \"treatment_rnn\" in net_name\n",
    "    b_use_actions_only = \"rnn_action_inputs_only\" in net_name\n",
    "\n",
    "    # Extract only relevant trajs and shift data\n",
    "    training_processed = get_processed_data(training_data, b_predict_actions, b_use_actions_only,\n",
    "                                                 b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    validation_processed = get_processed_data(validation_data, b_predict_actions,\n",
    "                                                   b_use_actions_only,\n",
    "                                                   b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "    # rango added 23.10.24\n",
    "    # if b_with_test:\n",
    "    #     test_processed = core.get_processed_data(test_data, b_predict_actions, b_use_actions_only,\n",
    "    #                                              b_use_predicted_confounders, b_use_oracle_confounders, b_remove_x1)\n",
    "\n",
    "    num_features = training_processed['scaled_inputs'].shape[-1]  # 4 if not b_use_actions_only else 3\n",
    "    num_outputs = training_processed['scaled_outputs'].shape[-1]  # 1 if not b_predict_actions else 3  # 5\n",
    "\n",
    "\n",
    "    # Unpack remaining variables\n",
    "    dropout_rate = config[1]\n",
    "    memory_multiplier = config[2] / num_features\n",
    "    num_epochs = config[3]\n",
    "    minibatch_size = config[4]\n",
    "    learning_rate = config[5]\n",
    "    max_norm = config[6]\n",
    "    tf_data_train = convert_to_tf_dataset(training_processed, minibatch_size)\n",
    "    tf_data_valid = convert_to_tf_dataset(validation_processed, minibatch_size)\n",
    "\n",
    "    model_folder = os.path.join(MODEL_ROOT, net_name)\n",
    "    model = model_process.load_model(model_folder, serialisation_name)\n",
    "\n",
    "    # predictition\n",
    "    outputs = training_processed['scaled_outputs']\n",
    "    results = model_predict(model, tf_data_train)\n",
    "    predictions = results['mean_pred']\n",
    "    #means, outputs, _, _ = test(training_processed, validation_processed, training_processed, tf_config,\n",
    "    #                            net_name, expt_name, dropout_rate, num_features, num_outputs,\n",
    "    #                            memory_multiplier, num_epochs, minibatch_size, learning_rate, max_norm,\n",
    "    #                            hidden_activation, output_activation, model_folder)\n",
    "\n",
    "    return predictions, outputs\n",
    "\n",
    "def get_weights(probs, targets):\n",
    "    w = probs*targets + (1-probs) * (1-targets)\n",
    "    return w.prod(axis=2)\n",
    "\n",
    "\n",
    "def get_weights_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    probs, targets = get_predictions(config)\n",
    "\n",
    "    return get_weights(probs, targets)\n",
    "\n",
    "def get_probabilities_from_config(config):\n",
    "    net_name = config[0]\n",
    "\n",
    "    probs, targets = get_predictions(config)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe7c88d4-c3d3-4871-a8c3-6df5d70fb810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def model_predict(model, dataset, pred_times=100):\n",
    "    # Initialize lists to store final statistics for all chunks\n",
    "    all_means = []\n",
    "    all_upper_bounds = []\n",
    "    all_lower_bounds = []\n",
    "    logs = 'Predicting ' + model.name\n",
    "\n",
    "    for data_chunk in tqdm(dataset, desc=logs):\n",
    "        chunk_predictions = []\n",
    "\n",
    "        # Predict the current chunk multiple times\n",
    "        for _ in range(pred_times):\n",
    "            prediction = model.predict(data_chunk['inputs'], verbose=0)\n",
    "            chunk_predictions.append(prediction)\n",
    "\n",
    "        # Convert list of predictions to a numpy array for statistical computation\n",
    "        chunk_predictions = np.array(chunk_predictions)\n",
    "\n",
    "        # Calculate mean, upper bound, and lower bound for the current chunk\n",
    "        mean_estimate = np.mean(chunk_predictions, axis=0)\n",
    "        upper_bound = np.percentile(chunk_predictions, 95, axis=0)\n",
    "        lower_bound = np.percentile(chunk_predictions, 5, axis=0)\n",
    "\n",
    "        # Append the statistics of the current chunk to their respective lists\n",
    "        all_means.append(mean_estimate)\n",
    "        all_upper_bounds.append(upper_bound)\n",
    "        all_lower_bounds.append(lower_bound)\n",
    "\n",
    "    # Optional: Convert lists to numpy arrays if further processing is needed\n",
    "    all_means = np.concatenate(all_means, axis=0) if all_means else np.array([])\n",
    "    all_upper_bounds = np.concatenate(all_upper_bounds, axis=0) if all_upper_bounds else np.array([])\n",
    "    all_lower_bounds = np.concatenate(all_lower_bounds, axis=0) if all_lower_bounds else np.array([])\n",
    "\n",
    "    # At this point, you can either return the raw statistics for each chunk,\n",
    "    # or aggregate them in some way depending on your application's needs.\n",
    "    # The following returns the list of statistics for all chunks.\n",
    "    return {\n",
    "        'mean_pred': all_means,\n",
    "        'upper_bound': all_upper_bounds,\n",
    "        'lower_bound': all_lower_bounds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "719e1d4f-a171-4cf0-b8b4-564d81441371",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting treatment_rnn_action_inputs_only:  12%|█▏        | 18/151 [02:32<18:46,  8.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Action with trajs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m weights \u001b[38;5;241m=\u001b[39m {k: get_weights_from_config(configs[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m configs}\n\u001b[1;32m      4\u001b[0m den \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_den\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m num \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_num\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn [39], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Action with trajs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m weights \u001b[38;5;241m=\u001b[39m {k: \u001b[43mget_weights_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m configs}\n\u001b[1;32m      4\u001b[0m den \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_den\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m num \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_num\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn [25], line 60\u001b[0m, in \u001b[0;36mget_weights_from_config\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_weights_from_config\u001b[39m(config):\n\u001b[1;32m     58\u001b[0m     net_name \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m     probs, targets \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_weights(probs, targets)\n",
      "Cell \u001b[0;32mIn [25], line 43\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# predictition\u001b[39;00m\n\u001b[1;32m     42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m training_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_outputs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_data_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m predictions \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#means, outputs, _, _ = test(training_processed, validation_processed, training_processed, tf_config,\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#                            net_name, expt_name, dropout_rate, num_features, num_outputs,\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#                            memory_multiplier, num_epochs, minibatch_size, learning_rate, max_norm,\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#                            hidden_activation, output_activation, model_folder)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [38], line 14\u001b[0m, in \u001b[0;36mmodel_predict\u001b[0;34m(model, dataset, pred_times)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Predict the current chunk multiple times\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred_times):\n\u001b[0;32m---> 14\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_chunk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     chunk_predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert list of predictions to a numpy array for statistical computation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/engine/training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2512\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2513\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2515\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2518\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2519\u001b[0m         )\n\u001b[0;32m-> 2521\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2529\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2531\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2534\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1284\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1301\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/keras/src/engine/data_adapter.py:353\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         flat_dataset \u001b[38;5;241m=\u001b[39m flat_dataset\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(epochs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m--> 353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:2323\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2321\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2322\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flat_map_op\n\u001b[0;32m-> 2323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflat_map_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/data/ops/flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/data/ops/flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[0;32m---> 33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, dataset_ops\u001b[38;5;241m.\u001b[39mDatasetSpec):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    266\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    270\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1188\u001b[0m   \u001b[38;5;66;03m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1180\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m       \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m-> 1180\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m   1181\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1182\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1184\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:207\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(args, kwargs)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 207\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    209\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_arg_keywords \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    169\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:353\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Get runtime values of captures\u001b[39;00m\n\u001b[1;32m    351\u001b[0m captures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func_captures\u001b[38;5;241m.\u001b[39mget_by_ref_snapshot()\n\u001b[0;32m--> 353\u001b[0m current_func_context \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_function_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# cache_key_deletion_observer is useless here. It's based on all captures.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# A new cache key will be built later when saving ConcreteFunction because\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;66;03m# only active captures should be saved.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    360\u001b[0m         args, kwargs, captures))\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_context.py:45\u001b[0m, in \u001b[0;36mmake_function_context\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Don't need to open an init_scope if the tf.function call is in eager mode\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# already.\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m executing_eagerly \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuting_eagerly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m parent_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     47\u001b[0m xla_context_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/new_causal/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1012\u001b[0m, in \u001b[0;36mContext.executing_eagerly\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecuting_eagerly\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1011\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns True if current thread has eager executing enabled.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread_local_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_eager\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Action with trajs\n",
    "weights = {k: get_weights_from_config(configs[k]) for k in configs}\n",
    "\n",
    "den = weights['action_den']\n",
    "num = weights['action_num']\n",
    "\n",
    "propensity_weights = 1.0/den if b_denominator_only else num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec56d5-71c0-4c94-8bd7-aba481063e2d",
   "metadata": {},
   "source": [
    "## Mirrored Strategy Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47db271-e410-4685-8e43-a4af4a130b32",
   "metadata": {},
   "source": [
    "### 0. 初始化策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e399f00e-22e1-491c-b842-1a07273581e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 2\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: %d' % strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f702c4-d683-4082-8631-8926df81ab47",
   "metadata": {},
   "source": [
    "### 1. 设置输入流水线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "677b5246-9d52-4ba1-8284-283a0ca56af7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 15:03:47.622023: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2403\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\025TensorSliceDataset:22\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 28\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 160\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_FLOAT\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform to tensorflow format\n",
    "global_batch_size = minibatch_size * strategy.num_replicas_in_sync\n",
    "tf_data_train = convert_to_tf_dataset(training_processed, global_batch_size)\n",
    "dist_tf_data_train = strategy.experimental_distribute_dataset(tf_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b33a80f-b359-4701-bcfa-15a31c7c529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (64, 160, 28)\n",
      "outputs: (64, 160, 3)\n",
      "active_entries: (64, 160, 3)\n",
      "sequence_lengths: (64,)\n",
      "inputs: (50, 160, 28)\n",
      "outputs: (50, 160, 3)\n",
      "active_entries: (50, 160, 3)\n",
      "sequence_lengths: (50,)\n",
      "inputs: (49, 160, 28)\n",
      "outputs: (49, 160, 3)\n",
      "active_entries: (49, 160, 3)\n",
      "sequence_lengths: (49,)\n"
     ]
    }
   ],
   "source": [
    "def inspect_dataset(batch):\n",
    "    # 这里的内容根据你的数据集结构调整\n",
    "    # 例如，打印出批次的形状或一些关键数据\n",
    "    for key, value in batch.items():\n",
    "        # 打印出每个键对应的值的形状\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "\n",
    "for dist_batch in dist_tf_data_train:\n",
    "    strategy.run(inspect_dataset, args=(dist_batch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac46964f-37ef-4fb9-a5d1-5493f4601d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'treatment_rnn'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b456d1-9a1f-4110-b77e-5a54460877b0",
   "metadata": {},
   "source": [
    "### 2. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d4a4dc-a7d9-4255-bfa1-ba4a910ffcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # test loss function ###################################\n",
    "    mse_loss_object = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "    \n",
    "    def compute_mse_loss(labels, predictions):\n",
    "        # 计算每个样本的MSE损失\n",
    "        per_example_loss = mse_loss_object(labels, predictions)\n",
    "        # 计算所有样本的平均MSE损失，并根据全局批量大小进行调整\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=global_batch_size)\n",
    "    \n",
    "    # custom loss function ##################################\n",
    "    class CustomLoss(losses.Loss):\n",
    "        def __init__(self, performance_metric, num_gpus, global_batch_size, name=\"custom_loss\"):\n",
    "            super().__init__(name=name) #reduction=losses.Reduction.NONE\n",
    "            self.performance_metric = performance_metric\n",
    "            self.num_gpus = num_gpus\n",
    "            self.global_batch_size = global_batch_size\n",
    "            # self.weights = params['weights']\n",
    "            # self.active_entries = params['active_entries']\n",
    "\n",
    "        def train_call(self, y_true, y_pred, active_entries, weights):\n",
    "            if self.performance_metric == \"mse\":\n",
    "                loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                       / tf.reduce_sum(active_entries)\n",
    "                # per_example_loss = (tf.square(y_true - y_pred) * active_entries * weights) \\\n",
    "                #                     / tf.reduce_sum(active_entries)\n",
    "            elif self.performance_metric == \"xentropy\":\n",
    "                loss = tf.reduce_sum((y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                                       (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "                                       * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "                # per_example_loss = ((y_true * -tf.math.log(y_pred + 1e-8) + \\\n",
    "                #                    (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8)) * active_entries * weights) / tf.reduce_sum(active_entries)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "\n",
    "            # 将总和除以gpu数，获得全局平均损失\n",
    "            return loss * (1./self.num_gpus)\n",
    "            # return tf.nn.compute_average_loss(per_example_loss, global_batch_size=self.global_batch_size)\n",
    "\n",
    "        def valid_call(self, y_true, y_pred):\n",
    "            if self.performance_metric == \"mse\":\n",
    "               #loss = tf.reduce_sum(tf.square(y_true - y_pred) * active_entries ) \\\n",
    "               #        / tf.reduce_sum(active_entries)\n",
    "                loss = tf.square(y_true - y_pred)\n",
    "\n",
    "            elif self.performance_metric == \"xentropy\":\n",
    "                loss = (y_true * -tf.math.log(y_pred + 1e-8) +\n",
    "                       (1 - y_true) * -tf.math.log(1 - y_pred + 1e-8))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unknown performance metric {}\".format(self.performance_metric))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\"performance_metric\": self.performance_metric, \"global_batch_size\": self.global_batch_size})\n",
    "            return config\n",
    "    \n",
    "    # loss_func = CustomLoss(model_parameters['performance_metric'], strategy.num_replicas_in_sync, global_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4ea7e-50d1-4481-bc83-6999c5ada186",
   "metadata": {},
   "source": [
    "### 3. 定义衡量指标以跟踪损失和准确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114a6623-ff78-40bd-a1fd-52252248a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_metric = metrics.MeanSquaredError(name='train_mse')\n",
    "    valid_loss = metrics.Mean(name='valid_loss')\n",
    "    valid_metric = metrics.MeanSquaredError(name='valid_mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef422af-a949-47c6-88b1-1cb17b3a649f",
   "metadata": {},
   "source": [
    "### 4. 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b781d4b6-26be-4fd2-9cfb-e6b23410e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"treatment_rnn\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None, 28)]           0         []                            \n",
      "                                                                                                  \n",
      " initial_h (InputLayer)      [(None, 112)]                0         []                            \n",
      "                                                                                                  \n",
      " initial_c (InputLayer)      [(None, 112)]                0         []                            \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, None, 112),          63168     ['input_1[0][0]',             \n",
      "                              (None, 112),                           'initial_h[0][0]',           \n",
      "                              (None, 112)]                           'initial_c[0][0]']           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 3)              339       ['lstm[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, None, 3)              0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 63507 (248.07 KB)\n",
      "Trainable params: 63507 (248.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# A model, an optimizer, and a checkpoint must be created under `strategy.scope`.\n",
    "with strategy.scope():\n",
    "    model = create_model(model_parameters)\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1750e4fb-94fe-417b-9ce6-fc8348b2b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data): #, chunk_sizes\n",
    "    inputs = data['inputs']\n",
    "    outputs = data['outputs']\n",
    "    active_entries = data['active_entries']\n",
    "    weights = data['propensity_weights'] if 'propensity_weights' in data else tf.constant(1.0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "       \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        initial_state = tf.zeros([batch_size, hidden_layer_size], dtype=tf.float32)\n",
    "        predictions,_,_ = model([inputs,initial_state, initial_state], training=True)\n",
    "        # Compute loss\n",
    "        # loss = loss_func.train_call(outputs, predictions, active_entries, weights)\n",
    "        loss = compute_mse_loss(outputs, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # Clip gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, clip_norm = max_norm)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    #self.train_loss.update_state(loss)\n",
    "    train_metric.update_state(outputs, predictions)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4ddac-f1a4-497c-baad-ff9914f16bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Collective all_reduce tensors: 5 all_reduces, num_devices = 2, group_size = 2, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2024-03-05 14:28:27.584663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-03-05 14:28:27.610959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-03-05 14:28:27.621997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def distributed_train_step(data): #, chunk_sizes\n",
    "    per_replica_losses = strategy.run(train_step, args=(data,)) #, chunk_sizes\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in tf_data_train:\n",
    "        total_loss += distributed_train_step(x)\n",
    "        num_batches += 1\n",
    "        train_loss = total_loss / num_batches\n",
    "\n",
    "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
    "              \"Test Accuracy: {}\")\n",
    "    print(template.format(epoch + 1, train_loss,\n",
    "                         train_metric.result() * 100, valid_loss.result(),\n",
    "                         valid_metric.result() * 100))\n",
    "\n",
    "    valid_loss.reset_states()\n",
    "    train_metric.reset_states()\n",
    "    valid_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6831b-0af3-4a6a-819b-f9f55a2843af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_causal",
   "language": "python",
   "name": "new_causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
